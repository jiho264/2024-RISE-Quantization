
Case: [ resnet18_AdaRound_AbsMaxQuantizer_CH_W4A8_AdaRoundLR0.001 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT8
    - per_channel: False

Replace to QuantModule
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2140.9060 (MSE:0.0008, Reg:2140.9053) beta=20.00
Iter  5000 | Total loss: 16.4978 (MSE:0.0025, Reg:16.4953) beta=18.88
Iter  6000 | Total loss: 3.0031 (MSE:0.0031, Reg:3.0000) beta=17.75
Iter  7000 | Total loss: 3.0034 (MSE:0.0034, Reg:3.0000) beta=16.62
Iter  8000 | Total loss: 2.9972 (MSE:0.0025, Reg:2.9947) beta=15.50
Iter  9000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9781.6709 (MSE:0.0004, Reg:9781.6709) beta=20.00
Iter  5000 | Total loss: 553.7947 (MSE:0.0011, Reg:553.7936) beta=18.88
Iter  6000 | Total loss: 234.7917 (MSE:0.0012, Reg:234.7905) beta=17.75
Iter  7000 | Total loss: 164.6793 (MSE:0.0012, Reg:164.6781) beta=16.62
Iter  8000 | Total loss: 112.7999 (MSE:0.0010, Reg:112.7989) beta=15.50
Iter  9000 | Total loss: 83.0078 (MSE:0.0010, Reg:83.0068) beta=14.38
Iter 10000 | Total loss: 63.1880 (MSE:0.0011, Reg:63.1869) beta=13.25
Iter 11000 | Total loss: 43.6392 (MSE:0.0014, Reg:43.6378) beta=12.12
Iter 12000 | Total loss: 31.1535 (MSE:0.0010, Reg:31.1525) beta=11.00
Iter 13000 | Total loss: 24.5369 (MSE:0.0011, Reg:24.5358) beta=9.88
Iter 14000 | Total loss: 17.9649 (MSE:0.0014, Reg:17.9634) beta=8.75
Iter 15000 | Total loss: 9.3776 (MSE:0.0015, Reg:9.3761) beta=7.62
Iter 16000 | Total loss: 5.0012 (MSE:0.0012, Reg:5.0000) beta=6.50
Iter 17000 | Total loss: 1.7471 (MSE:0.0010, Reg:1.7461) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 13499.6914 (MSE:0.0020, Reg:13499.6895) beta=20.00
Iter  5000 | Total loss: 2799.9971 (MSE:0.0033, Reg:2799.9937) beta=18.88
Iter  6000 | Total loss: 1707.1071 (MSE:0.0025, Reg:1707.1045) beta=17.75
Iter  7000 | Total loss: 1261.9131 (MSE:0.0025, Reg:1261.9106) beta=16.62
Iter  8000 | Total loss: 918.4911 (MSE:0.0024, Reg:918.4888) beta=15.50
Iter  9000 | Total loss: 665.6804 (MSE:0.0024, Reg:665.6779) beta=14.38
Iter 10000 | Total loss: 503.1579 (MSE:0.0031, Reg:503.1547) beta=13.25
Iter 11000 | Total loss: 365.9976 (MSE:0.0027, Reg:365.9949) beta=12.12
Iter 12000 | Total loss: 268.9317 (MSE:0.0023, Reg:268.9293) beta=11.00
Iter 13000 | Total loss: 175.7141 (MSE:0.0024, Reg:175.7117) beta=9.88
Iter 14000 | Total loss: 107.8295 (MSE:0.0025, Reg:107.8270) beta=8.75
Iter 15000 | Total loss: 57.0749 (MSE:0.0032, Reg:57.0717) beta=7.62
Iter 16000 | Total loss: 29.1354 (MSE:0.0037, Reg:29.1317) beta=6.50
Iter 17000 | Total loss: 7.7858 (MSE:0.0028, Reg:7.7830) beta=5.38
Iter 18000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 14454.8232 (MSE:0.0010, Reg:14454.8223) beta=20.00
Iter  5000 | Total loss: 1566.0667 (MSE:0.0013, Reg:1566.0654) beta=18.88
Iter  6000 | Total loss: 740.2151 (MSE:0.0011, Reg:740.2140) beta=17.75
Iter  7000 | Total loss: 487.9621 (MSE:0.0011, Reg:487.9609) beta=16.62
Iter  8000 | Total loss: 361.3727 (MSE:0.0012, Reg:361.3715) beta=15.50
Iter  9000 | Total loss: 280.2000 (MSE:0.0011, Reg:280.1990) beta=14.38
Iter 10000 | Total loss: 209.5785 (MSE:0.0010, Reg:209.5775) beta=13.25
Iter 11000 | Total loss: 159.2451 (MSE:0.0011, Reg:159.2440) beta=12.12
Iter 12000 | Total loss: 110.2737 (MSE:0.0012, Reg:110.2725) beta=11.00
Iter 13000 | Total loss: 77.8622 (MSE:0.0011, Reg:77.8611) beta=9.88
Iter 14000 | Total loss: 58.7004 (MSE:0.0011, Reg:58.6993) beta=8.75
Iter 15000 | Total loss: 28.9503 (MSE:0.0010, Reg:28.9493) beta=7.62
Iter 16000 | Total loss: 10.5880 (MSE:0.0010, Reg:10.5870) beta=6.50
Iter 17000 | Total loss: 2.2743 (MSE:0.0011, Reg:2.2733) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0107 (MSE:0.0107, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0101 (MSE:0.0101, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 17337.3027 (MSE:0.0064, Reg:17337.2969) beta=20.00
Iter  5000 | Total loss: 4361.7344 (MSE:0.0080, Reg:4361.7266) beta=18.88
Iter  6000 | Total loss: 2816.3501 (MSE:0.0076, Reg:2816.3425) beta=17.75
Iter  7000 | Total loss: 2059.7661 (MSE:0.0079, Reg:2059.7583) beta=16.62
Iter  8000 | Total loss: 1585.5964 (MSE:0.0075, Reg:1585.5890) beta=15.50
Iter  9000 | Total loss: 1228.0093 (MSE:0.0070, Reg:1228.0022) beta=14.38
Iter 10000 | Total loss: 942.1780 (MSE:0.0073, Reg:942.1708) beta=13.25
Iter 11000 | Total loss: 700.2589 (MSE:0.0076, Reg:700.2513) beta=12.12
Iter 12000 | Total loss: 499.7174 (MSE:0.0077, Reg:499.7097) beta=11.00
Iter 13000 | Total loss: 318.5825 (MSE:0.0073, Reg:318.5752) beta=9.88
Iter 14000 | Total loss: 174.4904 (MSE:0.0085, Reg:174.4819) beta=8.75
Iter 15000 | Total loss: 81.8796 (MSE:0.0077, Reg:81.8719) beta=7.62
Iter 16000 | Total loss: 26.9319 (MSE:0.0076, Reg:26.9243) beta=6.50
Iter 17000 | Total loss: 4.7693 (MSE:0.0077, Reg:4.7617) beta=5.38
Iter 18000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 34711.0703 (MSE:0.0013, Reg:34711.0703) beta=20.00
Iter  5000 | Total loss: 2998.1594 (MSE:0.0014, Reg:2998.1580) beta=18.88
Iter  6000 | Total loss: 1433.9675 (MSE:0.0014, Reg:1433.9661) beta=17.75
Iter  7000 | Total loss: 918.5999 (MSE:0.0014, Reg:918.5985) beta=16.62
Iter  8000 | Total loss: 702.5859 (MSE:0.0014, Reg:702.5845) beta=15.50
Iter  9000 | Total loss: 544.9467 (MSE:0.0014, Reg:544.9453) beta=14.38
Iter 10000 | Total loss: 420.6158 (MSE:0.0014, Reg:420.6144) beta=13.25
Iter 11000 | Total loss: 315.1130 (MSE:0.0014, Reg:315.1116) beta=12.12
Iter 12000 | Total loss: 231.5190 (MSE:0.0015, Reg:231.5175) beta=11.00
Iter 13000 | Total loss: 162.6284 (MSE:0.0016, Reg:162.6269) beta=9.88
Iter 14000 | Total loss: 100.1391 (MSE:0.0014, Reg:100.1377) beta=8.75
Iter 15000 | Total loss: 59.9923 (MSE:0.0015, Reg:59.9909) beta=7.62
Iter 16000 | Total loss: 21.1385 (MSE:0.0014, Reg:21.1371) beta=6.50
Iter 17000 | Total loss: 4.0241 (MSE:0.0014, Reg:4.0227) beta=5.38
Iter 18000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 78529.4609 (MSE:0.0044, Reg:78529.4531) beta=20.00
Iter  5000 | Total loss: 10069.5322 (MSE:0.0054, Reg:10069.5273) beta=18.88
Iter  6000 | Total loss: 5517.7876 (MSE:0.0061, Reg:5517.7812) beta=17.75
Iter  7000 | Total loss: 3927.5381 (MSE:0.0064, Reg:3927.5317) beta=16.62
Iter  8000 | Total loss: 3044.2219 (MSE:0.0059, Reg:3044.2161) beta=15.50
Iter  9000 | Total loss: 2410.8174 (MSE:0.0069, Reg:2410.8105) beta=14.38
Iter 10000 | Total loss: 1892.1757 (MSE:0.0069, Reg:1892.1687) beta=13.25
Iter 11000 | Total loss: 1460.3304 (MSE:0.0057, Reg:1460.3248) beta=12.12
Iter 12000 | Total loss: 1075.0752 (MSE:0.0056, Reg:1075.0696) beta=11.00
Iter 13000 | Total loss: 755.9265 (MSE:0.0060, Reg:755.9205) beta=9.88
Iter 14000 | Total loss: 473.1077 (MSE:0.0057, Reg:473.1021) beta=8.75
Iter 15000 | Total loss: 228.3741 (MSE:0.0053, Reg:228.3688) beta=7.62
Iter 16000 | Total loss: 84.7019 (MSE:0.0055, Reg:84.6964) beta=6.50
Iter 17000 | Total loss: 22.5469 (MSE:0.0062, Reg:22.5407) beta=5.38
Iter 18000 | Total loss: 1.4396 (MSE:0.0064, Reg:1.4332) beta=4.25
Iter 19000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5277.1514 (MSE:0.0017, Reg:5277.1499) beta=20.00
Iter  5000 | Total loss: 1492.3341 (MSE:0.0021, Reg:1492.3320) beta=18.88
Iter  6000 | Total loss: 1098.2787 (MSE:0.0024, Reg:1098.2764) beta=17.75
Iter  7000 | Total loss: 852.2422 (MSE:0.0020, Reg:852.2402) beta=16.62
Iter  8000 | Total loss: 708.5709 (MSE:0.0021, Reg:708.5687) beta=15.50
Iter  9000 | Total loss: 611.8472 (MSE:0.0021, Reg:611.8451) beta=14.38
Iter 10000 | Total loss: 523.9344 (MSE:0.0031, Reg:523.9313) beta=13.25
Iter 11000 | Total loss: 430.5892 (MSE:0.0030, Reg:430.5862) beta=12.12
Iter 12000 | Total loss: 344.2717 (MSE:0.0022, Reg:344.2695) beta=11.00
Iter 13000 | Total loss: 231.3986 (MSE:0.0022, Reg:231.3964) beta=9.88
Iter 14000 | Total loss: 130.5347 (MSE:0.0023, Reg:130.5324) beta=8.75
Iter 15000 | Total loss: 62.8859 (MSE:0.0023, Reg:62.8836) beta=7.62
Iter 16000 | Total loss: 29.8509 (MSE:0.0024, Reg:29.8486) beta=6.50
Iter 17000 | Total loss: 7.6338 (MSE:0.0024, Reg:7.6314) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 60535.7734 (MSE:0.0009, Reg:60535.7734) beta=20.00
Iter  5000 | Total loss: 1565.5254 (MSE:0.0013, Reg:1565.5242) beta=18.88
Iter  6000 | Total loss: 618.7590 (MSE:0.0012, Reg:618.7578) beta=17.75
Iter  7000 | Total loss: 350.4678 (MSE:0.0013, Reg:350.4666) beta=16.62
Iter  8000 | Total loss: 249.8712 (MSE:0.0013, Reg:249.8700) beta=15.50
Iter  9000 | Total loss: 196.2862 (MSE:0.0012, Reg:196.2850) beta=14.38
Iter 10000 | Total loss: 153.0093 (MSE:0.0012, Reg:153.0081) beta=13.25
Iter 11000 | Total loss: 122.4969 (MSE:0.0012, Reg:122.4957) beta=12.12
Iter 12000 | Total loss: 97.5768 (MSE:0.0012, Reg:97.5756) beta=11.00
Iter 13000 | Total loss: 60.6285 (MSE:0.0012, Reg:60.6273) beta=9.88
Iter 14000 | Total loss: 39.3538 (MSE:0.0012, Reg:39.3526) beta=8.75
Iter 15000 | Total loss: 22.1437 (MSE:0.0012, Reg:22.1425) beta=7.62
Iter 16000 | Total loss: 10.8238 (MSE:0.0013, Reg:10.8226) beta=6.50
Iter 17000 | Total loss: 0.8927 (MSE:0.0012, Reg:0.8916) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 71563.6250 (MSE:0.0051, Reg:71563.6172) beta=20.00
Iter  5000 | Total loss: 8829.3740 (MSE:0.0054, Reg:8829.3682) beta=18.88
Iter  6000 | Total loss: 5213.0942 (MSE:0.0056, Reg:5213.0889) beta=17.75
Iter  7000 | Total loss: 3645.7627 (MSE:0.0057, Reg:3645.7571) beta=16.62
Iter  8000 | Total loss: 2823.5115 (MSE:0.0055, Reg:2823.5061) beta=15.50
Iter  9000 | Total loss: 2287.9165 (MSE:0.0058, Reg:2287.9106) beta=14.38
Iter 10000 | Total loss: 1794.5537 (MSE:0.0056, Reg:1794.5481) beta=13.25
Iter 11000 | Total loss: 1357.9995 (MSE:0.0059, Reg:1357.9937) beta=12.12
Iter 12000 | Total loss: 1010.8768 (MSE:0.0057, Reg:1010.8711) beta=11.00
Iter 13000 | Total loss: 680.6433 (MSE:0.0057, Reg:680.6376) beta=9.88
Iter 14000 | Total loss: 401.4929 (MSE:0.0058, Reg:401.4871) beta=8.75
Iter 15000 | Total loss: 196.7466 (MSE:0.0059, Reg:196.7406) beta=7.62
Iter 16000 | Total loss: 72.5515 (MSE:0.0061, Reg:72.5454) beta=6.50
Iter 17000 | Total loss: 15.8483 (MSE:0.0059, Reg:15.8424) beta=5.38
Iter 18000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 112015.7969 (MSE:0.0016, Reg:112015.7969) beta=20.00
Iter  5000 | Total loss: 859.0279 (MSE:0.0018, Reg:859.0261) beta=18.88
Iter  6000 | Total loss: 293.3669 (MSE:0.0019, Reg:293.3651) beta=17.75
Iter  7000 | Total loss: 131.2491 (MSE:0.0020, Reg:131.2471) beta=16.62
Iter  8000 | Total loss: 95.3520 (MSE:0.0018, Reg:95.3503) beta=15.50
Iter  9000 | Total loss: 72.5509 (MSE:0.0019, Reg:72.5490) beta=14.38
Iter 10000 | Total loss: 53.0015 (MSE:0.0018, Reg:52.9998) beta=13.25
Iter 11000 | Total loss: 44.5818 (MSE:0.0018, Reg:44.5801) beta=12.12
Iter 12000 | Total loss: 33.9234 (MSE:0.0019, Reg:33.9215) beta=11.00
Iter 13000 | Total loss: 25.9116 (MSE:0.0019, Reg:25.9097) beta=9.88
Iter 14000 | Total loss: 20.2238 (MSE:0.0017, Reg:20.2221) beta=8.75
Iter 15000 | Total loss: 12.0017 (MSE:0.0018, Reg:12.0000) beta=7.62
Iter 16000 | Total loss: 4.5090 (MSE:0.0018, Reg:4.5072) beta=6.50
Iter 17000 | Total loss: 1.0795 (MSE:0.0018, Reg:1.0777) beta=5.38
Iter 18000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 231185.9062 (MSE:0.0051, Reg:231185.9062) beta=20.00
Iter  5000 | Total loss: 8448.5381 (MSE:0.0061, Reg:8448.5322) beta=18.88
Iter  6000 | Total loss: 3862.0759 (MSE:0.0062, Reg:3862.0698) beta=17.75
Iter  7000 | Total loss: 2401.2761 (MSE:0.0064, Reg:2401.2698) beta=16.62
Iter  8000 | Total loss: 1776.0930 (MSE:0.0062, Reg:1776.0868) beta=15.50
Iter  9000 | Total loss: 1378.4210 (MSE:0.0068, Reg:1378.4143) beta=14.38
Iter 10000 | Total loss: 1067.7786 (MSE:0.0067, Reg:1067.7720) beta=13.25
Iter 11000 | Total loss: 826.4182 (MSE:0.0064, Reg:826.4117) beta=12.12
Iter 12000 | Total loss: 624.8519 (MSE:0.0064, Reg:624.8455) beta=11.00
Iter 13000 | Total loss: 449.9613 (MSE:0.0062, Reg:449.9551) beta=9.88
Iter 14000 | Total loss: 278.3076 (MSE:0.0061, Reg:278.3015) beta=8.75
Iter 15000 | Total loss: 150.9421 (MSE:0.0064, Reg:150.9357) beta=7.62
Iter 16000 | Total loss: 59.5622 (MSE:0.0066, Reg:59.5556) beta=6.50
Iter 17000 | Total loss: 11.6635 (MSE:0.0072, Reg:11.6563) beta=5.38
Iter 18000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 18362.8086 (MSE:0.0005, Reg:18362.8086) beta=20.00
Iter  5000 | Total loss: 1010.2648 (MSE:0.0005, Reg:1010.2643) beta=18.88
Iter  6000 | Total loss: 732.2418 (MSE:0.0005, Reg:732.2412) beta=17.75
Iter  7000 | Total loss: 517.0310 (MSE:0.0005, Reg:517.0305) beta=16.62
Iter  8000 | Total loss: 404.6221 (MSE:0.0005, Reg:404.6216) beta=15.50
Iter  9000 | Total loss: 314.1122 (MSE:0.0005, Reg:314.1116) beta=14.38
Iter 10000 | Total loss: 259.2762 (MSE:0.0006, Reg:259.2757) beta=13.25
Iter 11000 | Total loss: 212.9537 (MSE:0.0006, Reg:212.9531) beta=12.12
Iter 12000 | Total loss: 175.9468 (MSE:0.0005, Reg:175.9463) beta=11.00
Iter 13000 | Total loss: 134.6106 (MSE:0.0006, Reg:134.6099) beta=9.88
Iter 14000 | Total loss: 94.6835 (MSE:0.0006, Reg:94.6829) beta=8.75
Iter 15000 | Total loss: 43.3774 (MSE:0.0006, Reg:43.3768) beta=7.62
Iter 16000 | Total loss: 18.9159 (MSE:0.0005, Reg:18.9153) beta=6.50
Iter 17000 | Total loss: 3.3860 (MSE:0.0005, Reg:3.3855) beta=5.38
Iter 18000 | Total loss: 0.6544 (MSE:0.0006, Reg:0.6539) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 127829.1875 (MSE:0.0006, Reg:127829.1875) beta=20.00
Iter  5000 | Total loss: 83.2177 (MSE:0.0007, Reg:83.2170) beta=18.88
Iter  6000 | Total loss: 21.8108 (MSE:0.0008, Reg:21.8100) beta=17.75
Iter  7000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=16.62
Iter  8000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 192686.0938 (MSE:0.0049, Reg:192686.0938) beta=20.00
Iter  5000 | Total loss: 5511.0381 (MSE:0.0055, Reg:5511.0327) beta=18.88
Iter  6000 | Total loss: 2515.5425 (MSE:0.0056, Reg:2515.5369) beta=17.75
Iter  7000 | Total loss: 1474.0718 (MSE:0.0055, Reg:1474.0663) beta=16.62
Iter  8000 | Total loss: 1012.5073 (MSE:0.0056, Reg:1012.5016) beta=15.50
Iter  9000 | Total loss: 778.1724 (MSE:0.0053, Reg:778.1671) beta=14.38
Iter 10000 | Total loss: 602.7843 (MSE:0.0056, Reg:602.7787) beta=13.25
Iter 11000 | Total loss: 434.0017 (MSE:0.0056, Reg:433.9961) beta=12.12
Iter 12000 | Total loss: 308.5280 (MSE:0.0054, Reg:308.5227) beta=11.00
Iter 13000 | Total loss: 224.2650 (MSE:0.0054, Reg:224.2596) beta=9.88
Iter 14000 | Total loss: 159.2880 (MSE:0.0059, Reg:159.2822) beta=8.75
Iter 15000 | Total loss: 83.7641 (MSE:0.0058, Reg:83.7582) beta=7.62
Iter 16000 | Total loss: 35.1919 (MSE:0.0055, Reg:35.1864) beta=6.50
Iter 17000 | Total loss: 8.8798 (MSE:0.0057, Reg:8.8742) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 140081.0156 (MSE:0.0008, Reg:140081.0156) beta=20.00
Iter  5000 | Total loss: 33.9994 (MSE:0.0008, Reg:33.9986) beta=18.88
Iter  6000 | Total loss: 0.4203 (MSE:0.0008, Reg:0.4194) beta=17.75
Iter  7000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0167 (MSE:0.0167, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0146 (MSE:0.0146, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0157 (MSE:0.0157, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0139 (MSE:0.0139, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 686431.0000 (MSE:0.0131, Reg:686431.0000) beta=20.00
Iter  5000 | Total loss: 7881.6509 (MSE:0.0163, Reg:7881.6348) beta=18.88
Iter  6000 | Total loss: 3251.7639 (MSE:0.0153, Reg:3251.7488) beta=17.75
Iter  7000 | Total loss: 1502.5367 (MSE:0.0162, Reg:1502.5205) beta=16.62
Iter  8000 | Total loss: 967.4977 (MSE:0.0176, Reg:967.4801) beta=15.50
Iter  9000 | Total loss: 712.6714 (MSE:0.0175, Reg:712.6539) beta=14.38
Iter 10000 | Total loss: 530.9586 (MSE:0.0163, Reg:530.9423) beta=13.25
Iter 11000 | Total loss: 407.4385 (MSE:0.0168, Reg:407.4217) beta=12.12
Iter 12000 | Total loss: 307.9855 (MSE:0.0155, Reg:307.9699) beta=11.00
Iter 13000 | Total loss: 217.8364 (MSE:0.0158, Reg:217.8205) beta=9.88
Iter 14000 | Total loss: 136.6473 (MSE:0.0160, Reg:136.6313) beta=8.75
Iter 15000 | Total loss: 82.0367 (MSE:0.0159, Reg:82.0207) beta=7.62
Iter 16000 | Total loss: 41.4510 (MSE:0.0168, Reg:41.4342) beta=6.50
Iter 17000 | Total loss: 9.8984 (MSE:0.0151, Reg:9.8833) beta=5.38
Iter 18000 | Total loss: 0.4469 (MSE:0.0153, Reg:0.4316) beta=4.25
Iter 19000 | Total loss: 0.0165 (MSE:0.0165, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0150 (MSE:0.0150, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 64011.7031 (MSE:0.0040, Reg:64011.6992) beta=20.00
Iter  5000 | Total loss: 7981.8506 (MSE:0.0049, Reg:7981.8457) beta=18.88
Iter  6000 | Total loss: 5664.3252 (MSE:0.0046, Reg:5664.3208) beta=17.75
Iter  7000 | Total loss: 4024.0808 (MSE:0.0055, Reg:4024.0752) beta=16.62
Iter  8000 | Total loss: 3144.4780 (MSE:0.0048, Reg:3144.4731) beta=15.50
Iter  9000 | Total loss: 2528.7996 (MSE:0.0049, Reg:2528.7947) beta=14.38
Iter 10000 | Total loss: 2065.8008 (MSE:0.0054, Reg:2065.7954) beta=13.25
Iter 11000 | Total loss: 1695.2892 (MSE:0.0047, Reg:1695.2844) beta=12.12
Iter 12000 | Total loss: 1335.3771 (MSE:0.0049, Reg:1335.3721) beta=11.00
Iter 13000 | Total loss: 945.7716 (MSE:0.0049, Reg:945.7667) beta=9.88
Iter 14000 | Total loss: 637.8812 (MSE:0.0048, Reg:637.8763) beta=8.75
Iter 15000 | Total loss: 365.8964 (MSE:0.0050, Reg:365.8914) beta=7.62
Iter 16000 | Total loss: 156.7615 (MSE:0.0050, Reg:156.7565) beta=6.50
Iter 17000 | Total loss: 35.6811 (MSE:0.0050, Reg:35.6761) beta=5.38
Iter 18000 | Total loss: 1.1219 (MSE:0.0050, Reg:1.1169) beta=4.25
Iter 19000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 173922.7188 (MSE:0.0012, Reg:173922.7188) beta=20.00
Iter  5000 | Total loss: 1.0011 (MSE:0.0011, Reg:1.0000) beta=18.88
Iter  6000 | Total loss: 1.0012 (MSE:0.0012, Reg:1.0000) beta=17.75
Iter  7000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.5369 (MSE:0.5369, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.5344 (MSE:0.5344, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.5027 (MSE:0.5027, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4806 (MSE:0.4806, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 564686.3125 (MSE:0.4844, Reg:564685.8125) beta=20.00
Iter  5000 | Total loss: 58748.5898 (MSE:0.5245, Reg:58748.0664) beta=18.88
Iter  6000 | Total loss: 44461.3086 (MSE:0.5299, Reg:44460.7773) beta=17.75
Iter  7000 | Total loss: 26752.6621 (MSE:0.5333, Reg:26752.1289) beta=16.62
Iter  8000 | Total loss: 17136.4102 (MSE:0.5307, Reg:17135.8789) beta=15.50
Iter  9000 | Total loss: 12181.8838 (MSE:0.5049, Reg:12181.3789) beta=14.38
Iter 10000 | Total loss: 9214.8154 (MSE:0.5016, Reg:9214.3135) beta=13.25
Iter 11000 | Total loss: 7118.2412 (MSE:0.4960, Reg:7117.7451) beta=12.12
Iter 12000 | Total loss: 5477.5576 (MSE:0.5000, Reg:5477.0576) beta=11.00
Iter 13000 | Total loss: 4036.7415 (MSE:0.5069, Reg:4036.2346) beta=9.88
Iter 14000 | Total loss: 2801.3618 (MSE:0.4860, Reg:2800.8760) beta=8.75
Iter 15000 | Total loss: 1727.4493 (MSE:0.5096, Reg:1726.9398) beta=7.62
Iter 16000 | Total loss: 883.1384 (MSE:0.5451, Reg:882.5933) beta=6.50
Iter 17000 | Total loss: 278.8342 (MSE:0.5005, Reg:278.3337) beta=5.38
Iter 18000 | Total loss: 14.5857 (MSE:0.4821, Reg:14.1036) beta=4.25
Iter 19000 | Total loss: 0.4816 (MSE:0.4816, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4959 (MSE:0.4959, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.4788 (MSE:0.4788, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3056 (MSE:0.3056, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2998 (MSE:0.2998, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2733 (MSE:0.2733, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 97015.1172 (MSE:0.3011, Reg:97014.8125) beta=20.00
Iter  5000 | Total loss: 1562.8121 (MSE:0.4088, Reg:1562.4033) beta=18.88
Iter  6000 | Total loss: 602.3243 (MSE:0.4031, Reg:601.9211) beta=17.75
Iter  7000 | Total loss: 416.0701 (MSE:0.4184, Reg:415.6517) beta=16.62
Iter  8000 | Total loss: 318.3551 (MSE:0.4018, Reg:317.9533) beta=15.50
Iter  9000 | Total loss: 240.1255 (MSE:0.4113, Reg:239.7142) beta=14.38
Iter 10000 | Total loss: 201.0637 (MSE:0.4367, Reg:200.6270) beta=13.25
Iter 11000 | Total loss: 167.5101 (MSE:0.3770, Reg:167.1331) beta=12.12
Iter 12000 | Total loss: 135.2253 (MSE:0.4211, Reg:134.8041) beta=11.00
Iter 13000 | Total loss: 106.9580 (MSE:0.4138, Reg:106.5442) beta=9.88
Iter 14000 | Total loss: 74.1231 (MSE:0.3823, Reg:73.7408) beta=8.75
Iter 15000 | Total loss: 49.9751 (MSE:0.4113, Reg:49.5638) beta=7.62
Iter 16000 | Total loss: 26.1192 (MSE:0.4133, Reg:25.7059) beta=6.50
Iter 17000 | Total loss: 7.3621 (MSE:0.4098, Reg:6.9523) beta=5.38
Iter 18000 | Total loss: 1.1794 (MSE:0.4136, Reg:0.7658) beta=4.25
Iter 19000 | Total loss: 0.4098 (MSE:0.4098, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4323 (MSE:0.4323, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 67.966%
Total time: 1300.80 sec

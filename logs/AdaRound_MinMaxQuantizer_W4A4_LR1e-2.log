
Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A4_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: False

Replace to QuantModule
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0220 (MSE:0.0220, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 988.5448 (MSE:0.0018, Reg:988.5430) beta=20.00
Iter  5000 | Total loss: 48.9959 (MSE:0.0014, Reg:48.9945) beta=18.88
Iter  6000 | Total loss: 31.6756 (MSE:0.0023, Reg:31.6733) beta=17.75
Iter  7000 | Total loss: 24.0024 (MSE:0.0024, Reg:24.0000) beta=16.62
Iter  8000 | Total loss: 16.0017 (MSE:0.0017, Reg:16.0000) beta=15.50
Iter  9000 | Total loss: 11.0018 (MSE:0.0018, Reg:11.0000) beta=14.38
Iter 10000 | Total loss: 6.0017 (MSE:0.0017, Reg:6.0000) beta=13.25
Iter 11000 | Total loss: 6.0019 (MSE:0.0019, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 3.0018 (MSE:0.0018, Reg:3.0000) beta=11.00
Iter 13000 | Total loss: 2.0021 (MSE:0.0021, Reg:2.0000) beta=9.88
Iter 14000 | Total loss: 2.0017 (MSE:0.0017, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4441.9941 (MSE:0.0012, Reg:4441.9932) beta=20.00
Iter  5000 | Total loss: 414.0790 (MSE:0.0012, Reg:414.0779) beta=18.88
Iter  6000 | Total loss: 241.9457 (MSE:0.0014, Reg:241.9442) beta=17.75
Iter  7000 | Total loss: 156.1846 (MSE:0.0014, Reg:156.1832) beta=16.62
Iter  8000 | Total loss: 111.0011 (MSE:0.0011, Reg:111.0000) beta=15.50
Iter  9000 | Total loss: 81.0011 (MSE:0.0011, Reg:81.0000) beta=14.38
Iter 10000 | Total loss: 45.3825 (MSE:0.0013, Reg:45.3812) beta=13.25
Iter 11000 | Total loss: 23.7829 (MSE:0.0018, Reg:23.7811) beta=12.12
Iter 12000 | Total loss: 14.9991 (MSE:0.0012, Reg:14.9979) beta=11.00
Iter 13000 | Total loss: 6.0011 (MSE:0.0011, Reg:6.0000) beta=9.88
Iter 14000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0330 (MSE:0.0330, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0089 (MSE:0.0089, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6141.1982 (MSE:0.0082, Reg:6141.1899) beta=20.00
Iter  5000 | Total loss: 791.4341 (MSE:0.0097, Reg:791.4243) beta=18.88
Iter  6000 | Total loss: 548.0974 (MSE:0.0084, Reg:548.0890) beta=17.75
Iter  7000 | Total loss: 399.9395 (MSE:0.0093, Reg:399.9302) beta=16.62
Iter  8000 | Total loss: 301.0084 (MSE:0.0084, Reg:301.0000) beta=15.50
Iter  9000 | Total loss: 238.8986 (MSE:0.0095, Reg:238.8891) beta=14.38
Iter 10000 | Total loss: 162.4632 (MSE:0.0093, Reg:162.4539) beta=13.25
Iter 11000 | Total loss: 99.0088 (MSE:0.0089, Reg:98.9999) beta=12.12
Iter 12000 | Total loss: 70.0078 (MSE:0.0086, Reg:69.9992) beta=11.00
Iter 13000 | Total loss: 38.0088 (MSE:0.0088, Reg:38.0000) beta=9.88
Iter 14000 | Total loss: 16.0089 (MSE:0.0089, Reg:16.0000) beta=8.75
Iter 15000 | Total loss: 1.0092 (MSE:0.0092, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0102 (MSE:0.0102, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0117 (MSE:0.0117, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0101 (MSE:0.0101, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8928.1670 (MSE:0.0026, Reg:8928.1641) beta=20.00
Iter  5000 | Total loss: 1000.5153 (MSE:0.0030, Reg:1000.5123) beta=18.88
Iter  6000 | Total loss: 619.9803 (MSE:0.0027, Reg:619.9776) beta=17.75
Iter  7000 | Total loss: 445.5081 (MSE:0.0027, Reg:445.5054) beta=16.62
Iter  8000 | Total loss: 317.9214 (MSE:0.0029, Reg:317.9186) beta=15.50
Iter  9000 | Total loss: 227.3692 (MSE:0.0027, Reg:227.3665) beta=14.38
Iter 10000 | Total loss: 148.9658 (MSE:0.0027, Reg:148.9630) beta=13.25
Iter 11000 | Total loss: 90.0013 (MSE:0.0027, Reg:89.9986) beta=12.12
Iter 12000 | Total loss: 40.0026 (MSE:0.0027, Reg:39.9999) beta=11.00
Iter 13000 | Total loss: 17.0027 (MSE:0.0027, Reg:17.0000) beta=9.88
Iter 14000 | Total loss: 3.9839 (MSE:0.0027, Reg:3.9812) beta=8.75
Iter 15000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0651 (MSE:0.0651, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0290 (MSE:0.0290, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0266 (MSE:0.0266, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0256 (MSE:0.0256, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10604.9355 (MSE:0.0269, Reg:10604.9082) beta=20.00
Iter  5000 | Total loss: 1662.4489 (MSE:0.0275, Reg:1662.4214) beta=18.88
Iter  6000 | Total loss: 1221.8795 (MSE:0.0280, Reg:1221.8514) beta=17.75
Iter  7000 | Total loss: 965.4131 (MSE:0.0276, Reg:965.3854) beta=16.62
Iter  8000 | Total loss: 734.6895 (MSE:0.0288, Reg:734.6606) beta=15.50
Iter  9000 | Total loss: 503.5795 (MSE:0.0268, Reg:503.5527) beta=14.38
Iter 10000 | Total loss: 335.3125 (MSE:0.0277, Reg:335.2849) beta=13.25
Iter 11000 | Total loss: 229.9579 (MSE:0.0273, Reg:229.9306) beta=12.12
Iter 12000 | Total loss: 125.1325 (MSE:0.0272, Reg:125.1053) beta=11.00
Iter 13000 | Total loss: 66.0111 (MSE:0.0270, Reg:65.9841) beta=9.88
Iter 14000 | Total loss: 20.0297 (MSE:0.0297, Reg:20.0000) beta=8.75
Iter 15000 | Total loss: 2.0289 (MSE:0.0289, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0279 (MSE:0.0279, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0272 (MSE:0.0272, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0293 (MSE:0.0293, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0268 (MSE:0.0268, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0277 (MSE:0.0277, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 23251.1523 (MSE:0.0047, Reg:23251.1484) beta=20.00
Iter  5000 | Total loss: 2829.0393 (MSE:0.0041, Reg:2829.0352) beta=18.88
Iter  6000 | Total loss: 1766.9442 (MSE:0.0042, Reg:1766.9399) beta=17.75
Iter  7000 | Total loss: 1241.1868 (MSE:0.0044, Reg:1241.1824) beta=16.62
Iter  8000 | Total loss: 907.9102 (MSE:0.0039, Reg:907.9062) beta=15.50
Iter  9000 | Total loss: 650.4334 (MSE:0.0043, Reg:650.4291) beta=14.38
Iter 10000 | Total loss: 456.4568 (MSE:0.0044, Reg:456.4524) beta=13.25
Iter 11000 | Total loss: 295.8319 (MSE:0.0039, Reg:295.8281) beta=12.12
Iter 12000 | Total loss: 151.8142 (MSE:0.0042, Reg:151.8100) beta=11.00
Iter 13000 | Total loss: 72.0032 (MSE:0.0041, Reg:71.9991) beta=9.88
Iter 14000 | Total loss: 19.0037 (MSE:0.0038, Reg:18.9999) beta=8.75
Iter 15000 | Total loss: 1.0041 (MSE:0.0041, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0368 (MSE:0.0368, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0154 (MSE:0.0154, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0154 (MSE:0.0154, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 54442.1562 (MSE:0.0157, Reg:54442.1406) beta=20.00
Iter  5000 | Total loss: 6808.9180 (MSE:0.0152, Reg:6808.9028) beta=18.88
Iter  6000 | Total loss: 4964.8896 (MSE:0.0158, Reg:4964.8740) beta=17.75
Iter  7000 | Total loss: 3960.8904 (MSE:0.0164, Reg:3960.8740) beta=16.62
Iter  8000 | Total loss: 3091.3599 (MSE:0.0164, Reg:3091.3435) beta=15.50
Iter  9000 | Total loss: 2305.6865 (MSE:0.0171, Reg:2305.6694) beta=14.38
Iter 10000 | Total loss: 1540.8149 (MSE:0.0170, Reg:1540.7979) beta=13.25
Iter 11000 | Total loss: 973.2400 (MSE:0.0160, Reg:973.2239) beta=12.12
Iter 12000 | Total loss: 537.6500 (MSE:0.0164, Reg:537.6335) beta=11.00
Iter 13000 | Total loss: 261.7432 (MSE:0.0171, Reg:261.7260) beta=9.88
Iter 14000 | Total loss: 85.9771 (MSE:0.0153, Reg:85.9618) beta=8.75
Iter 15000 | Total loss: 9.0148 (MSE:0.0148, Reg:9.0000) beta=7.62
Iter 16000 | Total loss: 0.0161 (MSE:0.0161, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0153 (MSE:0.0153, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0163 (MSE:0.0163, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0160 (MSE:0.0160, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0152 (MSE:0.0152, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0213 (MSE:0.0213, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3969.7517 (MSE:0.0075, Reg:3969.7441) beta=20.00
Iter  5000 | Total loss: 866.9232 (MSE:0.0074, Reg:866.9158) beta=18.88
Iter  6000 | Total loss: 763.0080 (MSE:0.0080, Reg:763.0000) beta=17.75
Iter  7000 | Total loss: 647.9942 (MSE:0.0075, Reg:647.9867) beta=16.62
Iter  8000 | Total loss: 581.7972 (MSE:0.0076, Reg:581.7897) beta=15.50
Iter  9000 | Total loss: 472.9019 (MSE:0.0077, Reg:472.8942) beta=14.38
Iter 10000 | Total loss: 357.0088 (MSE:0.0088, Reg:357.0000) beta=13.25
Iter 11000 | Total loss: 263.9980 (MSE:0.0080, Reg:263.9900) beta=12.12
Iter 12000 | Total loss: 164.0005 (MSE:0.0079, Reg:163.9926) beta=11.00
Iter 13000 | Total loss: 100.0004 (MSE:0.0076, Reg:99.9928) beta=9.88
Iter 14000 | Total loss: 53.9856 (MSE:0.0080, Reg:53.9776) beta=8.75
Iter 15000 | Total loss: 14.2887 (MSE:0.0076, Reg:14.2810) beta=7.62
Iter 16000 | Total loss: 0.7760 (MSE:0.0076, Reg:0.7684) beta=6.50
Iter 17000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 43746.2578 (MSE:0.0026, Reg:43746.2539) beta=20.00
Iter  5000 | Total loss: 5651.9272 (MSE:0.0027, Reg:5651.9248) beta=18.88
Iter  6000 | Total loss: 3697.7615 (MSE:0.0027, Reg:3697.7588) beta=17.75
Iter  7000 | Total loss: 2660.4558 (MSE:0.0025, Reg:2660.4534) beta=16.62
Iter  8000 | Total loss: 1891.0997 (MSE:0.0025, Reg:1891.0972) beta=15.50
Iter  9000 | Total loss: 1336.0021 (MSE:0.0026, Reg:1335.9995) beta=14.38
Iter 10000 | Total loss: 888.0052 (MSE:0.0027, Reg:888.0025) beta=13.25
Iter 11000 | Total loss: 549.5850 (MSE:0.0027, Reg:549.5823) beta=12.12
Iter 12000 | Total loss: 308.2672 (MSE:0.0028, Reg:308.2645) beta=11.00
Iter 13000 | Total loss: 134.3770 (MSE:0.0027, Reg:134.3743) beta=9.88
Iter 14000 | Total loss: 33.0026 (MSE:0.0026, Reg:33.0000) beta=8.75
Iter 15000 | Total loss: 3.0025 (MSE:0.0025, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0389 (MSE:0.0389, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0161 (MSE:0.0161, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0154 (MSE:0.0154, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0155 (MSE:0.0155, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 45588.9805 (MSE:0.0155, Reg:45588.9648) beta=20.00
Iter  5000 | Total loss: 7010.1226 (MSE:0.0153, Reg:7010.1074) beta=18.88
Iter  6000 | Total loss: 5256.3340 (MSE:0.0164, Reg:5256.3174) beta=17.75
Iter  7000 | Total loss: 4216.2866 (MSE:0.0162, Reg:4216.2705) beta=16.62
Iter  8000 | Total loss: 3321.9656 (MSE:0.0158, Reg:3321.9497) beta=15.50
Iter  9000 | Total loss: 2461.4802 (MSE:0.0151, Reg:2461.4651) beta=14.38
Iter 10000 | Total loss: 1642.9674 (MSE:0.0157, Reg:1642.9517) beta=13.25
Iter 11000 | Total loss: 968.0609 (MSE:0.0161, Reg:968.0447) beta=12.12
Iter 12000 | Total loss: 506.5870 (MSE:0.0160, Reg:506.5710) beta=11.00
Iter 13000 | Total loss: 226.0251 (MSE:0.0161, Reg:226.0090) beta=9.88
Iter 14000 | Total loss: 56.0154 (MSE:0.0154, Reg:56.0000) beta=8.75
Iter 15000 | Total loss: 8.0157 (MSE:0.0157, Reg:8.0000) beta=7.62
Iter 16000 | Total loss: 0.0151 (MSE:0.0151, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0168 (MSE:0.0168, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0163 (MSE:0.0163, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0170 (MSE:0.0170, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 83113.5312 (MSE:0.0037, Reg:83113.5312) beta=20.00
Iter  5000 | Total loss: 11974.3135 (MSE:0.0037, Reg:11974.3096) beta=18.88
Iter  6000 | Total loss: 8148.2886 (MSE:0.0037, Reg:8148.2847) beta=17.75
Iter  7000 | Total loss: 5830.5557 (MSE:0.0038, Reg:5830.5518) beta=16.62
Iter  8000 | Total loss: 4207.6846 (MSE:0.0036, Reg:4207.6812) beta=15.50
Iter  9000 | Total loss: 2997.6184 (MSE:0.0038, Reg:2997.6147) beta=14.38
Iter 10000 | Total loss: 1999.1996 (MSE:0.0036, Reg:1999.1960) beta=13.25
Iter 11000 | Total loss: 1240.8015 (MSE:0.0037, Reg:1240.7979) beta=12.12
Iter 12000 | Total loss: 669.3943 (MSE:0.0037, Reg:669.3906) beta=11.00
Iter 13000 | Total loss: 297.6697 (MSE:0.0038, Reg:297.6660) beta=9.88
Iter 14000 | Total loss: 85.9731 (MSE:0.0038, Reg:85.9693) beta=8.75
Iter 15000 | Total loss: 6.0035 (MSE:0.0036, Reg:6.0000) beta=7.62
Iter 16000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0333 (MSE:0.0333, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0132 (MSE:0.0132, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 142888.4844 (MSE:0.0130, Reg:142888.4688) beta=20.00
Iter  5000 | Total loss: 18678.3008 (MSE:0.0132, Reg:18678.2871) beta=18.88
Iter  6000 | Total loss: 12672.1982 (MSE:0.0138, Reg:12672.1846) beta=17.75
Iter  7000 | Total loss: 9235.3359 (MSE:0.0137, Reg:9235.3223) beta=16.62
Iter  8000 | Total loss: 6695.4404 (MSE:0.0136, Reg:6695.4268) beta=15.50
Iter  9000 | Total loss: 4617.1270 (MSE:0.0139, Reg:4617.1133) beta=14.38
Iter 10000 | Total loss: 3018.4473 (MSE:0.0132, Reg:3018.4341) beta=13.25
Iter 11000 | Total loss: 1761.1742 (MSE:0.0128, Reg:1761.1614) beta=12.12
Iter 12000 | Total loss: 896.6216 (MSE:0.0134, Reg:896.6083) beta=11.00
Iter 13000 | Total loss: 312.7798 (MSE:0.0137, Reg:312.7662) beta=9.88
Iter 14000 | Total loss: 59.9020 (MSE:0.0135, Reg:59.8886) beta=8.75
Iter 15000 | Total loss: 4.0133 (MSE:0.0133, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 1.0137 (MSE:0.0137, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0141 (MSE:0.0141, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0130 (MSE:0.0130, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0140 (MSE:0.0140, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 13627.6523 (MSE:0.0016, Reg:13627.6504) beta=20.00
Iter  5000 | Total loss: 2700.6887 (MSE:0.0017, Reg:2700.6870) beta=18.88
Iter  6000 | Total loss: 2151.1804 (MSE:0.0017, Reg:2151.1787) beta=17.75
Iter  7000 | Total loss: 1814.6357 (MSE:0.0017, Reg:1814.6340) beta=16.62
Iter  8000 | Total loss: 1506.6648 (MSE:0.0016, Reg:1506.6632) beta=15.50
Iter  9000 | Total loss: 1170.3339 (MSE:0.0016, Reg:1170.3323) beta=14.38
Iter 10000 | Total loss: 892.8087 (MSE:0.0017, Reg:892.8070) beta=13.25
Iter 11000 | Total loss: 654.3533 (MSE:0.0016, Reg:654.3517) beta=12.12
Iter 12000 | Total loss: 421.9957 (MSE:0.0017, Reg:421.9941) beta=11.00
Iter 13000 | Total loss: 232.2816 (MSE:0.0018, Reg:232.2799) beta=9.88
Iter 14000 | Total loss: 100.9565 (MSE:0.0017, Reg:100.9548) beta=8.75
Iter 15000 | Total loss: 27.4647 (MSE:0.0017, Reg:27.4631) beta=7.62
Iter 16000 | Total loss: 3.5926 (MSE:0.0016, Reg:3.5910) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 129351.4062 (MSE:0.0013, Reg:129351.4062) beta=20.00
Iter  5000 | Total loss: 8647.4131 (MSE:0.0013, Reg:8647.4121) beta=18.88
Iter  6000 | Total loss: 4528.2910 (MSE:0.0014, Reg:4528.2896) beta=17.75
Iter  7000 | Total loss: 2909.7910 (MSE:0.0013, Reg:2909.7898) beta=16.62
Iter  8000 | Total loss: 1991.0677 (MSE:0.0013, Reg:1991.0665) beta=15.50
Iter  9000 | Total loss: 1415.6428 (MSE:0.0013, Reg:1415.6415) beta=14.38
Iter 10000 | Total loss: 974.3988 (MSE:0.0013, Reg:974.3975) beta=13.25
Iter 11000 | Total loss: 636.5884 (MSE:0.0013, Reg:636.5872) beta=12.12
Iter 12000 | Total loss: 382.3580 (MSE:0.0013, Reg:382.3567) beta=11.00
Iter 13000 | Total loss: 213.6241 (MSE:0.0014, Reg:213.6228) beta=9.88
Iter 14000 | Total loss: 69.9610 (MSE:0.0013, Reg:69.9597) beta=8.75
Iter 15000 | Total loss: 13.3849 (MSE:0.0012, Reg:13.3837) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0280 (MSE:0.0280, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0113 (MSE:0.0113, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0111 (MSE:0.0111, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 133514.2031 (MSE:0.0112, Reg:133514.1875) beta=20.00
Iter  5000 | Total loss: 18020.4805 (MSE:0.0112, Reg:18020.4688) beta=18.88
Iter  6000 | Total loss: 12041.9756 (MSE:0.0112, Reg:12041.9648) beta=17.75
Iter  7000 | Total loss: 8686.6748 (MSE:0.0112, Reg:8686.6641) beta=16.62
Iter  8000 | Total loss: 6249.8765 (MSE:0.0117, Reg:6249.8647) beta=15.50
Iter  9000 | Total loss: 4414.9443 (MSE:0.0114, Reg:4414.9331) beta=14.38
Iter 10000 | Total loss: 2949.3367 (MSE:0.0115, Reg:2949.3252) beta=13.25
Iter 11000 | Total loss: 1765.7627 (MSE:0.0112, Reg:1765.7515) beta=12.12
Iter 12000 | Total loss: 937.6518 (MSE:0.0121, Reg:937.6397) beta=11.00
Iter 13000 | Total loss: 370.8788 (MSE:0.0116, Reg:370.8672) beta=9.88
Iter 14000 | Total loss: 92.7580 (MSE:0.0117, Reg:92.7463) beta=8.75
Iter 15000 | Total loss: 13.9180 (MSE:0.0118, Reg:13.9062) beta=7.62
Iter 16000 | Total loss: 0.0108 (MSE:0.0108, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0120 (MSE:0.0120, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0110 (MSE:0.0110, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0124 (MSE:0.0124, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 231459.2188 (MSE:0.0014, Reg:231459.2188) beta=20.00
Iter  5000 | Total loss: 9168.2969 (MSE:0.0015, Reg:9168.2949) beta=18.88
Iter  6000 | Total loss: 2993.6750 (MSE:0.0014, Reg:2993.6736) beta=17.75
Iter  7000 | Total loss: 1613.0229 (MSE:0.0015, Reg:1613.0215) beta=16.62
Iter  8000 | Total loss: 1045.7180 (MSE:0.0014, Reg:1045.7166) beta=15.50
Iter  9000 | Total loss: 742.5131 (MSE:0.0015, Reg:742.5117) beta=14.38
Iter 10000 | Total loss: 516.5183 (MSE:0.0014, Reg:516.5168) beta=13.25
Iter 11000 | Total loss: 348.5735 (MSE:0.0015, Reg:348.5721) beta=12.12
Iter 12000 | Total loss: 210.5566 (MSE:0.0015, Reg:210.5551) beta=11.00
Iter 13000 | Total loss: 110.8620 (MSE:0.0014, Reg:110.8606) beta=9.88
Iter 14000 | Total loss: 42.0014 (MSE:0.0014, Reg:42.0000) beta=8.75
Iter 15000 | Total loss: 8.0015 (MSE:0.0015, Reg:8.0000) beta=7.62
Iter 16000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0562 (MSE:0.0562, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0252 (MSE:0.0252, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0268 (MSE:0.0268, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0256 (MSE:0.0256, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 450765.0938 (MSE:0.0275, Reg:450765.0625) beta=20.00
Iter  5000 | Total loss: 72648.8516 (MSE:0.0252, Reg:72648.8281) beta=18.88
Iter  6000 | Total loss: 49481.6953 (MSE:0.0252, Reg:49481.6719) beta=17.75
Iter  7000 | Total loss: 35079.6719 (MSE:0.0270, Reg:35079.6445) beta=16.62
Iter  8000 | Total loss: 24451.8594 (MSE:0.0265, Reg:24451.8320) beta=15.50
Iter  9000 | Total loss: 16230.4756 (MSE:0.0280, Reg:16230.4473) beta=14.38
Iter 10000 | Total loss: 9900.9307 (MSE:0.0263, Reg:9900.9043) beta=13.25
Iter 11000 | Total loss: 5242.0181 (MSE:0.0260, Reg:5241.9922) beta=12.12
Iter 12000 | Total loss: 2131.9893 (MSE:0.0258, Reg:2131.9634) beta=11.00
Iter 13000 | Total loss: 571.8710 (MSE:0.0252, Reg:571.8458) beta=9.88
Iter 14000 | Total loss: 67.1230 (MSE:0.0261, Reg:67.0969) beta=8.75
Iter 15000 | Total loss: 1.0249 (MSE:0.0249, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0261 (MSE:0.0261, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0251 (MSE:0.0251, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0247 (MSE:0.0247, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0270 (MSE:0.0270, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0240 (MSE:0.0240, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0199 (MSE:0.0199, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0111 (MSE:0.0111, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0112 (MSE:0.0112, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0112 (MSE:0.0112, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 46356.9336 (MSE:0.0109, Reg:46356.9219) beta=20.00
Iter  5000 | Total loss: 10222.8594 (MSE:0.0117, Reg:10222.8477) beta=18.88
Iter  6000 | Total loss: 7799.8042 (MSE:0.0119, Reg:7799.7925) beta=17.75
Iter  7000 | Total loss: 6199.7456 (MSE:0.0122, Reg:6199.7334) beta=16.62
Iter  8000 | Total loss: 4915.2891 (MSE:0.0119, Reg:4915.2773) beta=15.50
Iter  9000 | Total loss: 3661.1362 (MSE:0.0117, Reg:3661.1245) beta=14.38
Iter 10000 | Total loss: 2562.3450 (MSE:0.0117, Reg:2562.3333) beta=13.25
Iter 11000 | Total loss: 1562.7904 (MSE:0.0119, Reg:1562.7786) beta=12.12
Iter 12000 | Total loss: 724.1720 (MSE:0.0116, Reg:724.1603) beta=11.00
Iter 13000 | Total loss: 253.4411 (MSE:0.0111, Reg:253.4300) beta=9.88
Iter 14000 | Total loss: 47.7552 (MSE:0.0119, Reg:47.7433) beta=8.75
Iter 15000 | Total loss: 2.9925 (MSE:0.0116, Reg:2.9808) beta=7.62
Iter 16000 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0126 (MSE:0.0126, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0116 (MSE:0.0116, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0120 (MSE:0.0120, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 431363.1250 (MSE:0.0017, Reg:431363.1250) beta=20.00
Iter  5000 | Total loss: 12979.6826 (MSE:0.0016, Reg:12979.6807) beta=18.88
Iter  6000 | Total loss: 2573.1306 (MSE:0.0018, Reg:2573.1289) beta=17.75
Iter  7000 | Total loss: 1197.6271 (MSE:0.0016, Reg:1197.6255) beta=16.62
Iter  8000 | Total loss: 721.0499 (MSE:0.0018, Reg:721.0481) beta=15.50
Iter  9000 | Total loss: 462.5034 (MSE:0.0017, Reg:462.5017) beta=14.38
Iter 10000 | Total loss: 310.4435 (MSE:0.0017, Reg:310.4418) beta=13.25
Iter 11000 | Total loss: 228.0016 (MSE:0.0016, Reg:228.0000) beta=12.12
Iter 12000 | Total loss: 144.9920 (MSE:0.0017, Reg:144.9904) beta=11.00
Iter 13000 | Total loss: 79.9925 (MSE:0.0017, Reg:79.9908) beta=9.88
Iter 14000 | Total loss: 24.9026 (MSE:0.0017, Reg:24.9009) beta=8.75
Iter 15000 | Total loss: 2.0017 (MSE:0.0017, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 1.5617 (MSE:1.5617, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 1.2252 (MSE:1.2252, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.9087 (MSE:0.9087, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.7419 (MSE:0.7419, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 291549.4688 (MSE:0.7268, Reg:291548.7500) beta=20.00
Iter  5000 | Total loss: 56631.5508 (MSE:0.7214, Reg:56630.8281) beta=18.88
Iter  6000 | Total loss: 36680.0430 (MSE:0.7800, Reg:36679.2617) beta=17.75
Iter  7000 | Total loss: 23851.9297 (MSE:0.7760, Reg:23851.1543) beta=16.62
Iter  8000 | Total loss: 14880.7891 (MSE:0.7283, Reg:14880.0605) beta=15.50
Iter  9000 | Total loss: 8633.8662 (MSE:0.7333, Reg:8633.1328) beta=14.38
Iter 10000 | Total loss: 4318.8740 (MSE:0.7102, Reg:4318.1641) beta=13.25
Iter 11000 | Total loss: 1809.7765 (MSE:0.6863, Reg:1809.0902) beta=12.12
Iter 12000 | Total loss: 544.2672 (MSE:0.7050, Reg:543.5621) beta=11.00
Iter 13000 | Total loss: 96.6554 (MSE:0.7251, Reg:95.9304) beta=9.88
Iter 14000 | Total loss: 6.6819 (MSE:0.6819, Reg:6.0000) beta=8.75
Iter 15000 | Total loss: 1.7176 (MSE:0.7176, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.7520 (MSE:0.7520, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.7037 (MSE:0.7037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.6830 (MSE:0.6830, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.6984 (MSE:0.6984, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.7057 (MSE:0.7057, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 1.0152 (MSE:1.0152, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.5765 (MSE:0.5765, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.5318 (MSE:0.5318, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.5401 (MSE:0.5401, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 35521.0547 (MSE:0.5393, Reg:35520.5156) beta=20.00
Iter  5000 | Total loss: 6691.0410 (MSE:0.5408, Reg:6690.5000) beta=18.88
Iter  6000 | Total loss: 4497.6113 (MSE:0.4550, Reg:4497.1562) beta=17.75
Iter  7000 | Total loss: 3068.9089 (MSE:0.4879, Reg:3068.4209) beta=16.62
Iter  8000 | Total loss: 2029.1044 (MSE:0.5666, Reg:2028.5378) beta=15.50
Iter  9000 | Total loss: 1261.3999 (MSE:0.5231, Reg:1260.8768) beta=14.38
Iter 10000 | Total loss: 692.6599 (MSE:0.4841, Reg:692.1758) beta=13.25
Iter 11000 | Total loss: 315.2181 (MSE:0.4437, Reg:314.7744) beta=12.12
Iter 12000 | Total loss: 120.8433 (MSE:0.5289, Reg:120.3144) beta=11.00
Iter 13000 | Total loss: 29.1936 (MSE:0.4773, Reg:28.7163) beta=9.88
Iter 14000 | Total loss: 4.4873 (MSE:0.4873, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.5570 (MSE:0.5570, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.4855 (MSE:0.4855, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.5096 (MSE:0.5096, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.4809 (MSE:0.4809, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.4885 (MSE:0.4885, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4859 (MSE:0.4859, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 48.010%
Total time: 1297.60 sec


Case: [ resnet18_AdaRound_OrgNormQuantizerCode_CH_W4A4_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: OrgNormQuantizerCode
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: OrgNormQuantizerCode
    - dstDtype: INT4
    - per_channel: False

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Parent class is OrgNormQuantizerCode
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0226 (MSE:0.0226, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0212 (MSE:0.0212, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0221 (MSE:0.0221, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0234 (MSE:0.0234, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0247 (MSE:0.0247, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0229 (MSE:0.0229, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0215 (MSE:0.0215, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0221 (MSE:0.0221, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0222 (MSE:0.0222, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0294 (MSE:0.0294, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0253 (MSE:0.0253, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0229 (MSE:0.0229, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0229 (MSE:0.0229, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0218 (MSE:0.0218, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0246 (MSE:0.0246, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0430 (MSE:0.0430, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0479 (MSE:0.0479, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0476 (MSE:0.0476, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0479 (MSE:0.0479, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0468 (MSE:0.0468, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0488 (MSE:0.0488, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0453 (MSE:0.0453, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0491 (MSE:0.0491, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0455 (MSE:0.0455, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0506 (MSE:0.0506, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0455 (MSE:0.0455, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0467 (MSE:0.0467, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0468 (MSE:0.0468, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0468 (MSE:0.0468, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0479 (MSE:0.0479, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0477 (MSE:0.0477, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0449 (MSE:0.0449, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0520 (MSE:0.0520, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0467 (MSE:0.0467, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0483 (MSE:0.0483, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0320 (MSE:0.0320, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0288 (MSE:0.0288, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0312 (MSE:0.0312, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0320 (MSE:0.0320, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0304 (MSE:0.0304, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0345 (MSE:0.0345, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0344 (MSE:0.0344, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0316 (MSE:0.0316, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0338 (MSE:0.0338, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0307 (MSE:0.0307, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0331 (MSE:0.0331, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0324 (MSE:0.0324, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0305 (MSE:0.0305, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0320 (MSE:0.0320, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0312 (MSE:0.0312, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0325 (MSE:0.0325, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0316 (MSE:0.0316, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0310 (MSE:0.0310, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0330 (MSE:0.0330, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0286 (MSE:0.0286, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0333 (MSE:0.0333, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0961 (MSE:0.0961, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1180 (MSE:0.1180, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0939 (MSE:0.0939, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0938 (MSE:0.0938, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1025 (MSE:0.1025, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1079 (MSE:0.1079, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1065 (MSE:0.1065, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0933 (MSE:0.0933, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0975 (MSE:0.0975, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0999 (MSE:0.0999, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1042 (MSE:0.1042, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1035 (MSE:0.1035, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1005 (MSE:0.1005, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1008 (MSE:0.1008, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1145 (MSE:0.1145, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0974 (MSE:0.0974, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0977 (MSE:0.0977, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1030 (MSE:0.1030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1073 (MSE:0.1073, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1049 (MSE:0.1049, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1080 (MSE:0.1080, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0413 (MSE:0.0413, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0410 (MSE:0.0410, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0419 (MSE:0.0419, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0419 (MSE:0.0419, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0419 (MSE:0.0419, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0425 (MSE:0.0425, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0439 (MSE:0.0439, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0417 (MSE:0.0417, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0413 (MSE:0.0413, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0430 (MSE:0.0430, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0429 (MSE:0.0429, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0412 (MSE:0.0412, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0438 (MSE:0.0438, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0453 (MSE:0.0453, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0414 (MSE:0.0414, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0436 (MSE:0.0436, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0414 (MSE:0.0414, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0438 (MSE:0.0438, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0449 (MSE:0.0449, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0400 (MSE:0.0400, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0438 (MSE:0.0438, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0674 (MSE:0.0674, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0626 (MSE:0.0626, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0659 (MSE:0.0659, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0668 (MSE:0.0668, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0711 (MSE:0.0711, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0677 (MSE:0.0677, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0712 (MSE:0.0712, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0731 (MSE:0.0731, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0679 (MSE:0.0679, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0665 (MSE:0.0665, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0762 (MSE:0.0762, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0634 (MSE:0.0634, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0644 (MSE:0.0644, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0720 (MSE:0.0720, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0675 (MSE:0.0675, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0662 (MSE:0.0662, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0671 (MSE:0.0671, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0702 (MSE:0.0702, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0714 (MSE:0.0714, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0691 (MSE:0.0691, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0684 (MSE:0.0684, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0218 (MSE:0.0218, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0213 (MSE:0.0213, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0221 (MSE:0.0221, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0214 (MSE:0.0214, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0253 (MSE:0.0253, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0222 (MSE:0.0222, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0240 (MSE:0.0240, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0270 (MSE:0.0270, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0242 (MSE:0.0242, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0226 (MSE:0.0226, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0228 (MSE:0.0228, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0250 (MSE:0.0250, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0241 (MSE:0.0241, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0233 (MSE:0.0233, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0238 (MSE:0.0238, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0228 (MSE:0.0228, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0468 (MSE:0.0468, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0571 (MSE:0.0571, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0553 (MSE:0.0553, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0561 (MSE:0.0561, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0595 (MSE:0.0595, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0532 (MSE:0.0532, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0565 (MSE:0.0565, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0581 (MSE:0.0581, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0569 (MSE:0.0569, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0569 (MSE:0.0569, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0584 (MSE:0.0584, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0531 (MSE:0.0531, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0585 (MSE:0.0585, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0536 (MSE:0.0536, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0564 (MSE:0.0564, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0576 (MSE:0.0576, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0578 (MSE:0.0578, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0566 (MSE:0.0566, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0567 (MSE:0.0567, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0575 (MSE:0.0575, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0608 (MSE:0.0608, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0682 (MSE:0.0682, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0646 (MSE:0.0646, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0729 (MSE:0.0729, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0680 (MSE:0.0680, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0716 (MSE:0.0716, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0695 (MSE:0.0695, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0753 (MSE:0.0753, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0699 (MSE:0.0699, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0689 (MSE:0.0689, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0690 (MSE:0.0690, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0727 (MSE:0.0727, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0726 (MSE:0.0726, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0743 (MSE:0.0743, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0750 (MSE:0.0750, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0679 (MSE:0.0679, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0662 (MSE:0.0662, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0653 (MSE:0.0653, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0680 (MSE:0.0680, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0703 (MSE:0.0703, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0699 (MSE:0.0699, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0806 (MSE:0.0806, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0635 (MSE:0.0635, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0640 (MSE:0.0640, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0612 (MSE:0.0612, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0604 (MSE:0.0604, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0609 (MSE:0.0609, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0634 (MSE:0.0634, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0637 (MSE:0.0637, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0627 (MSE:0.0627, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0639 (MSE:0.0639, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0614 (MSE:0.0614, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0610 (MSE:0.0610, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0623 (MSE:0.0623, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0627 (MSE:0.0627, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0621 (MSE:0.0621, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0606 (MSE:0.0606, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0609 (MSE:0.0609, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0634 (MSE:0.0634, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0583 (MSE:0.0583, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0657 (MSE:0.0657, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0623 (MSE:0.0623, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0584 (MSE:0.0584, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0802 (MSE:0.0802, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0780 (MSE:0.0780, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0792 (MSE:0.0792, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0830 (MSE:0.0830, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0818 (MSE:0.0818, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0772 (MSE:0.0772, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0780 (MSE:0.0780, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0811 (MSE:0.0811, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0794 (MSE:0.0794, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0775 (MSE:0.0775, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0798 (MSE:0.0798, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0797 (MSE:0.0797, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0810 (MSE:0.0810, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0774 (MSE:0.0774, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0779 (MSE:0.0779, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0801 (MSE:0.0801, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0832 (MSE:0.0832, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0772 (MSE:0.0772, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0799 (MSE:0.0799, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0862 (MSE:0.0862, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0868 (MSE:0.0868, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0624 (MSE:0.0624, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0753 (MSE:0.0753, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0860 (MSE:0.0860, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0840 (MSE:0.0840, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0835 (MSE:0.0835, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0903 (MSE:0.0903, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0860 (MSE:0.0860, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0899 (MSE:0.0899, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0801 (MSE:0.0801, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0813 (MSE:0.0813, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0944 (MSE:0.0944, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0789 (MSE:0.0789, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0909 (MSE:0.0909, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0841 (MSE:0.0841, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0812 (MSE:0.0812, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0850 (MSE:0.0850, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0832 (MSE:0.0832, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0884 (MSE:0.0884, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0853 (MSE:0.0853, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0864 (MSE:0.0864, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0888 (MSE:0.0888, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1324 (MSE:0.1324, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1155 (MSE:0.1155, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1178 (MSE:0.1178, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1254 (MSE:0.1254, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1218 (MSE:0.1218, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1122 (MSE:0.1122, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1077 (MSE:0.1077, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1108 (MSE:0.1108, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.1176 (MSE:0.1176, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.1099 (MSE:0.1099, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1140 (MSE:0.1140, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1081 (MSE:0.1081, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1187 (MSE:0.1187, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1179 (MSE:0.1179, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1114 (MSE:0.1114, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1240 (MSE:0.1240, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1056 (MSE:0.1056, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1132 (MSE:0.1132, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1076 (MSE:0.1076, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1191 (MSE:0.1191, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1199 (MSE:0.1199, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0618 (MSE:0.0618, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0650 (MSE:0.0650, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0661 (MSE:0.0661, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0624 (MSE:0.0624, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0636 (MSE:0.0636, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0677 (MSE:0.0677, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0701 (MSE:0.0701, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0605 (MSE:0.0605, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0643 (MSE:0.0643, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0606 (MSE:0.0606, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0609 (MSE:0.0609, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0681 (MSE:0.0681, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0637 (MSE:0.0637, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0620 (MSE:0.0620, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0643 (MSE:0.0643, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0651 (MSE:0.0651, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0671 (MSE:0.0671, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0672 (MSE:0.0672, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0638 (MSE:0.0638, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0641 (MSE:0.0641, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0714 (MSE:0.0714, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2539 (MSE:0.2539, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2183 (MSE:0.2183, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2530 (MSE:0.2530, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2263 (MSE:0.2263, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.2148 (MSE:0.2148, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.2072 (MSE:0.2072, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.2300 (MSE:0.2300, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.2060 (MSE:0.2060, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.2558 (MSE:0.2558, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.2552 (MSE:0.2552, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.2377 (MSE:0.2377, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.2018 (MSE:0.2018, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.2336 (MSE:0.2336, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.2295 (MSE:0.2295, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.2369 (MSE:0.2369, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.2312 (MSE:0.2312, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2033 (MSE:0.2033, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2247 (MSE:0.2247, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2219 (MSE:0.2219, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2558 (MSE:0.2558, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2269 (MSE:0.2269, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0572 (MSE:0.0572, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0559 (MSE:0.0559, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0601 (MSE:0.0601, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0592 (MSE:0.0592, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0580 (MSE:0.0580, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0571 (MSE:0.0571, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0587 (MSE:0.0587, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0550 (MSE:0.0550, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0652 (MSE:0.0652, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0563 (MSE:0.0563, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0543 (MSE:0.0543, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0574 (MSE:0.0574, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0575 (MSE:0.0575, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0535 (MSE:0.0535, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0565 (MSE:0.0565, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0525 (MSE:0.0525, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0545 (MSE:0.0545, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0642 (MSE:0.0642, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0633 (MSE:0.0633, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0583 (MSE:0.0583, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0539 (MSE:0.0539, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2401 (MSE:0.2401, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1550 (MSE:0.1550, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1732 (MSE:0.1732, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1384 (MSE:0.1384, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1575 (MSE:0.1575, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1617 (MSE:0.1617, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1665 (MSE:0.1665, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1572 (MSE:0.1572, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.1447 (MSE:0.1447, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.1515 (MSE:0.1515, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1503 (MSE:0.1503, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1693 (MSE:0.1693, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1624 (MSE:0.1624, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1645 (MSE:0.1645, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1887 (MSE:0.1887, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1686 (MSE:0.1686, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1568 (MSE:0.1568, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1482 (MSE:0.1482, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1494 (MSE:0.1494, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1588 (MSE:0.1588, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1538 (MSE:0.1538, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 3.9699 (MSE:3.9699, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 3.9649 (MSE:3.9649, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 3.6465 (MSE:3.6465, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 3.6143 (MSE:3.6143, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4.0493 (MSE:4.0493, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 4.0210 (MSE:4.0210, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 3.7610 (MSE:3.7610, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 4.0149 (MSE:4.0149, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 3.9725 (MSE:3.9725, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 3.8994 (MSE:3.8994, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 3.6979 (MSE:3.6979, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 3.7287 (MSE:3.7287, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 3.7722 (MSE:3.7722, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 3.7698 (MSE:3.7698, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 3.4548 (MSE:3.4548, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 3.7073 (MSE:3.7073, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 3.9739 (MSE:3.9739, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 3.5895 (MSE:3.5895, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 3.7287 (MSE:3.7287, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 3.6221 (MSE:3.6221, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 3.5306 (MSE:3.5306, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 8.0375 (MSE:8.0375, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 6.9421 (MSE:6.9421, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 7.2125 (MSE:7.2125, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 7.3623 (MSE:7.3623, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7.5735 (MSE:7.5735, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 7.9095 (MSE:7.9095, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 6.6261 (MSE:6.6261, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 7.5884 (MSE:7.5884, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 7.3238 (MSE:7.3238, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 6.8520 (MSE:6.8520, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 7.0330 (MSE:7.0330, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 6.9169 (MSE:6.9169, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 6.8453 (MSE:6.8453, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 7.7532 (MSE:7.7532, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 7.6066 (MSE:7.6066, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 7.5141 (MSE:7.5141, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 6.6084 (MSE:6.6084, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 7.1703 (MSE:7.1703, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 7.1067 (MSE:7.1067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 6.9664 (MSE:6.9664, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 8.0428 (MSE:8.0428, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 0.100%
Total time: 1223.49 sec


Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A32_BNFold_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 895.3534 (MSE:0.0002, Reg:895.3531) beta=20.00
Iter  5000 | Total loss: 28.0003 (MSE:0.0003, Reg:28.0000) beta=18.88
Iter  6000 | Total loss: 24.0003 (MSE:0.0003, Reg:24.0000) beta=17.75
Iter  7000 | Total loss: 17.0003 (MSE:0.0003, Reg:17.0000) beta=16.62
Iter  8000 | Total loss: 14.0003 (MSE:0.0003, Reg:14.0000) beta=15.50
Iter  9000 | Total loss: 11.0003 (MSE:0.0003, Reg:11.0000) beta=14.38
Iter 10000 | Total loss: 9.0003 (MSE:0.0003, Reg:9.0000) beta=13.25
Iter 11000 | Total loss: 9.0003 (MSE:0.0003, Reg:9.0000) beta=12.12
Iter 12000 | Total loss: 9.0003 (MSE:0.0003, Reg:9.0000) beta=11.00
Iter 13000 | Total loss: 7.0003 (MSE:0.0003, Reg:7.0000) beta=9.88
Iter 14000 | Total loss: 3.0003 (MSE:0.0003, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2162.7415 (MSE:0.0003, Reg:2162.7412) beta=20.00
Iter  5000 | Total loss: 89.6068 (MSE:0.0003, Reg:89.6065) beta=18.88
Iter  6000 | Total loss: 51.9991 (MSE:0.0003, Reg:51.9988) beta=17.75
Iter  7000 | Total loss: 35.9655 (MSE:0.0003, Reg:35.9652) beta=16.62
Iter  8000 | Total loss: 30.0003 (MSE:0.0003, Reg:30.0000) beta=15.50
Iter  9000 | Total loss: 22.9966 (MSE:0.0003, Reg:22.9963) beta=14.38
Iter 10000 | Total loss: 14.0003 (MSE:0.0003, Reg:14.0000) beta=13.25
Iter 11000 | Total loss: 10.0003 (MSE:0.0003, Reg:10.0000) beta=12.12
Iter 12000 | Total loss: 7.0003 (MSE:0.0003, Reg:7.0000) beta=11.00
Iter 13000 | Total loss: 3.0003 (MSE:0.0003, Reg:3.0000) beta=9.88
Iter 14000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3329.6716 (MSE:0.0013, Reg:3329.6704) beta=20.00
Iter  5000 | Total loss: 318.7009 (MSE:0.0011, Reg:318.6998) beta=18.88
Iter  6000 | Total loss: 230.9611 (MSE:0.0011, Reg:230.9600) beta=17.75
Iter  7000 | Total loss: 195.0012 (MSE:0.0012, Reg:195.0000) beta=16.62
Iter  8000 | Total loss: 144.4430 (MSE:0.0012, Reg:144.4418) beta=15.50
Iter  9000 | Total loss: 105.0013 (MSE:0.0013, Reg:105.0000) beta=14.38
Iter 10000 | Total loss: 60.8449 (MSE:0.0012, Reg:60.8436) beta=13.25
Iter 11000 | Total loss: 36.0011 (MSE:0.0011, Reg:36.0000) beta=12.12
Iter 12000 | Total loss: 16.0012 (MSE:0.0012, Reg:16.0000) beta=11.00
Iter 13000 | Total loss: 5.0010 (MSE:0.0010, Reg:5.0000) beta=9.88
Iter 14000 | Total loss: 3.0012 (MSE:0.0012, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 1.0011 (MSE:0.0011, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2955.6108 (MSE:0.0005, Reg:2955.6104) beta=20.00
Iter  5000 | Total loss: 146.8299 (MSE:0.0006, Reg:146.8293) beta=18.88
Iter  6000 | Total loss: 87.0006 (MSE:0.0006, Reg:87.0000) beta=17.75
Iter  7000 | Total loss: 60.0005 (MSE:0.0005, Reg:60.0000) beta=16.62
Iter  8000 | Total loss: 48.0006 (MSE:0.0006, Reg:48.0000) beta=15.50
Iter  9000 | Total loss: 35.0005 (MSE:0.0005, Reg:35.0000) beta=14.38
Iter 10000 | Total loss: 24.0006 (MSE:0.0006, Reg:24.0000) beta=13.25
Iter 11000 | Total loss: 18.0006 (MSE:0.0006, Reg:18.0000) beta=12.12
Iter 12000 | Total loss: 9.0006 (MSE:0.0006, Reg:9.0000) beta=11.00
Iter 13000 | Total loss: 4.0006 (MSE:0.0006, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 2.0005 (MSE:0.0005, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5255.7085 (MSE:0.0045, Reg:5255.7041) beta=20.00
Iter  5000 | Total loss: 659.0019 (MSE:0.0047, Reg:658.9972) beta=18.88
Iter  6000 | Total loss: 526.5303 (MSE:0.0046, Reg:526.5258) beta=17.75
Iter  7000 | Total loss: 445.9869 (MSE:0.0051, Reg:445.9818) beta=16.62
Iter  8000 | Total loss: 372.8596 (MSE:0.0045, Reg:372.8551) beta=15.50
Iter  9000 | Total loss: 292.9883 (MSE:0.0044, Reg:292.9839) beta=14.38
Iter 10000 | Total loss: 210.9871 (MSE:0.0050, Reg:210.9821) beta=13.25
Iter 11000 | Total loss: 122.9909 (MSE:0.0045, Reg:122.9864) beta=12.12
Iter 12000 | Total loss: 48.3429 (MSE:0.0048, Reg:48.3381) beta=11.00
Iter 13000 | Total loss: 15.7741 (MSE:0.0050, Reg:15.7690) beta=9.88
Iter 14000 | Total loss: 6.0051 (MSE:0.0051, Reg:6.0000) beta=8.75
Iter 15000 | Total loss: 2.0050 (MSE:0.0050, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6129.2656 (MSE:0.0008, Reg:6129.2646) beta=20.00
Iter  5000 | Total loss: 472.1660 (MSE:0.0008, Reg:472.1652) beta=18.88
Iter  6000 | Total loss: 333.9077 (MSE:0.0007, Reg:333.9070) beta=17.75
Iter  7000 | Total loss: 247.8942 (MSE:0.0007, Reg:247.8935) beta=16.62
Iter  8000 | Total loss: 196.7168 (MSE:0.0008, Reg:196.7160) beta=15.50
Iter  9000 | Total loss: 152.5295 (MSE:0.0007, Reg:152.5288) beta=14.38
Iter 10000 | Total loss: 124.9974 (MSE:0.0008, Reg:124.9966) beta=13.25
Iter 11000 | Total loss: 77.0008 (MSE:0.0008, Reg:77.0000) beta=12.12
Iter 12000 | Total loss: 25.0008 (MSE:0.0008, Reg:25.0000) beta=11.00
Iter 13000 | Total loss: 11.9993 (MSE:0.0008, Reg:11.9986) beta=9.88
Iter 14000 | Total loss: 3.0008 (MSE:0.0008, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 2.0008 (MSE:0.0008, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 18093.3086 (MSE:0.0041, Reg:18093.3047) beta=20.00
Iter  5000 | Total loss: 1279.1774 (MSE:0.0040, Reg:1279.1733) beta=18.88
Iter  6000 | Total loss: 1036.7188 (MSE:0.0038, Reg:1036.7150) beta=17.75
Iter  7000 | Total loss: 877.4274 (MSE:0.0036, Reg:877.4237) beta=16.62
Iter  8000 | Total loss: 728.7404 (MSE:0.0040, Reg:728.7365) beta=15.50
Iter  9000 | Total loss: 561.9684 (MSE:0.0038, Reg:561.9646) beta=14.38
Iter 10000 | Total loss: 397.4559 (MSE:0.0042, Reg:397.4517) beta=13.25
Iter 11000 | Total loss: 261.5419 (MSE:0.0036, Reg:261.5383) beta=12.12
Iter 12000 | Total loss: 97.0034 (MSE:0.0038, Reg:96.9996) beta=11.00
Iter 13000 | Total loss: 30.9980 (MSE:0.0037, Reg:30.9944) beta=9.88
Iter 14000 | Total loss: 9.0005 (MSE:0.0039, Reg:8.9966) beta=8.75
Iter 15000 | Total loss: 4.0037 (MSE:0.0039, Reg:3.9997) beta=7.62
Iter 16000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2023.9666 (MSE:0.0012, Reg:2023.9653) beta=20.00
Iter  5000 | Total loss: 142.0013 (MSE:0.0013, Reg:142.0000) beta=18.88
Iter  6000 | Total loss: 121.0004 (MSE:0.0014, Reg:120.9990) beta=17.75
Iter  7000 | Total loss: 91.0013 (MSE:0.0013, Reg:91.0000) beta=16.62
Iter  8000 | Total loss: 79.9794 (MSE:0.0015, Reg:79.9779) beta=15.50
Iter  9000 | Total loss: 68.0013 (MSE:0.0013, Reg:68.0000) beta=14.38
Iter 10000 | Total loss: 54.5232 (MSE:0.0014, Reg:54.5218) beta=13.25
Iter 11000 | Total loss: 39.0014 (MSE:0.0014, Reg:39.0000) beta=12.12
Iter 12000 | Total loss: 31.0014 (MSE:0.0014, Reg:31.0000) beta=11.00
Iter 13000 | Total loss: 14.9996 (MSE:0.0014, Reg:14.9982) beta=9.88
Iter 14000 | Total loss: 6.0013 (MSE:0.0013, Reg:6.0000) beta=8.75
Iter 15000 | Total loss: 2.0014 (MSE:0.0014, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 16176.8506 (MSE:0.0007, Reg:16176.8496) beta=20.00
Iter  5000 | Total loss: 1091.5703 (MSE:0.0007, Reg:1091.5696) beta=18.88
Iter  6000 | Total loss: 774.3397 (MSE:0.0006, Reg:774.3391) beta=17.75
Iter  7000 | Total loss: 611.1634 (MSE:0.0007, Reg:611.1627) beta=16.62
Iter  8000 | Total loss: 492.3541 (MSE:0.0007, Reg:492.3535) beta=15.50
Iter  9000 | Total loss: 377.1216 (MSE:0.0007, Reg:377.1209) beta=14.38
Iter 10000 | Total loss: 231.0706 (MSE:0.0007, Reg:231.0699) beta=13.25
Iter 11000 | Total loss: 160.3413 (MSE:0.0008, Reg:160.3405) beta=12.12
Iter 12000 | Total loss: 93.9994 (MSE:0.0007, Reg:93.9988) beta=11.00
Iter 13000 | Total loss: 36.0007 (MSE:0.0007, Reg:36.0000) beta=9.88
Iter 14000 | Total loss: 14.0007 (MSE:0.0007, Reg:14.0000) beta=8.75
Iter 15000 | Total loss: 2.0007 (MSE:0.0007, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 1.0007 (MSE:0.0007, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 32800.7305 (MSE:0.0035, Reg:32800.7266) beta=20.00
Iter  5000 | Total loss: 2624.8086 (MSE:0.0038, Reg:2624.8047) beta=18.88
Iter  6000 | Total loss: 2040.6649 (MSE:0.0033, Reg:2040.6616) beta=17.75
Iter  7000 | Total loss: 1729.4406 (MSE:0.0035, Reg:1729.4370) beta=16.62
Iter  8000 | Total loss: 1376.7152 (MSE:0.0035, Reg:1376.7118) beta=15.50
Iter  9000 | Total loss: 1049.1406 (MSE:0.0040, Reg:1049.1366) beta=14.38
Iter 10000 | Total loss: 731.9667 (MSE:0.0037, Reg:731.9630) beta=13.25
Iter 11000 | Total loss: 472.6464 (MSE:0.0035, Reg:472.6429) beta=12.12
Iter 12000 | Total loss: 250.0519 (MSE:0.0037, Reg:250.0482) beta=11.00
Iter 13000 | Total loss: 98.9907 (MSE:0.0037, Reg:98.9870) beta=9.88
Iter 14000 | Total loss: 28.0035 (MSE:0.0035, Reg:28.0000) beta=8.75
Iter 15000 | Total loss: 5.0037 (MSE:0.0037, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 36805.2891 (MSE:0.0009, Reg:36805.2891) beta=20.00
Iter  5000 | Total loss: 2400.3147 (MSE:0.0009, Reg:2400.3137) beta=18.88
Iter  6000 | Total loss: 1614.7513 (MSE:0.0008, Reg:1614.7505) beta=17.75
Iter  7000 | Total loss: 1240.1051 (MSE:0.0009, Reg:1240.1042) beta=16.62
Iter  8000 | Total loss: 983.0939 (MSE:0.0009, Reg:983.0930) beta=15.50
Iter  9000 | Total loss: 767.7826 (MSE:0.0009, Reg:767.7817) beta=14.38
Iter 10000 | Total loss: 538.1180 (MSE:0.0009, Reg:538.1171) beta=13.25
Iter 11000 | Total loss: 349.0948 (MSE:0.0010, Reg:349.0938) beta=12.12
Iter 12000 | Total loss: 192.0005 (MSE:0.0009, Reg:191.9996) beta=11.00
Iter 13000 | Total loss: 76.4904 (MSE:0.0009, Reg:76.4894) beta=9.88
Iter 14000 | Total loss: 19.6829 (MSE:0.0009, Reg:19.6820) beta=8.75
Iter 15000 | Total loss: 7.0009 (MSE:0.0009, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 70547.0625 (MSE:0.0032, Reg:70547.0625) beta=20.00
Iter  5000 | Total loss: 4678.1948 (MSE:0.0036, Reg:4678.1914) beta=18.88
Iter  6000 | Total loss: 3240.4824 (MSE:0.0031, Reg:3240.4792) beta=17.75
Iter  7000 | Total loss: 2525.8179 (MSE:0.0034, Reg:2525.8145) beta=16.62
Iter  8000 | Total loss: 1935.0188 (MSE:0.0035, Reg:1935.0154) beta=15.50
Iter  9000 | Total loss: 1451.4606 (MSE:0.0035, Reg:1451.4570) beta=14.38
Iter 10000 | Total loss: 996.7302 (MSE:0.0035, Reg:996.7266) beta=13.25
Iter 11000 | Total loss: 600.4728 (MSE:0.0034, Reg:600.4694) beta=12.12
Iter 12000 | Total loss: 284.9495 (MSE:0.0035, Reg:284.9460) beta=11.00
Iter 13000 | Total loss: 102.6513 (MSE:0.0033, Reg:102.6480) beta=9.88
Iter 14000 | Total loss: 20.9961 (MSE:0.0034, Reg:20.9927) beta=8.75
Iter 15000 | Total loss: 6.0026 (MSE:0.0030, Reg:5.9995) beta=7.62
Iter 16000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9907.4609 (MSE:0.0003, Reg:9907.4609) beta=20.00
Iter  5000 | Total loss: 636.8366 (MSE:0.0003, Reg:636.8363) beta=18.88
Iter  6000 | Total loss: 482.9918 (MSE:0.0004, Reg:482.9914) beta=17.75
Iter  7000 | Total loss: 407.9713 (MSE:0.0003, Reg:407.9709) beta=16.62
Iter  8000 | Total loss: 348.7085 (MSE:0.0003, Reg:348.7082) beta=15.50
Iter  9000 | Total loss: 290.4668 (MSE:0.0003, Reg:290.4664) beta=14.38
Iter 10000 | Total loss: 213.0003 (MSE:0.0003, Reg:213.0000) beta=13.25
Iter 11000 | Total loss: 135.4547 (MSE:0.0003, Reg:135.4544) beta=12.12
Iter 12000 | Total loss: 74.5877 (MSE:0.0004, Reg:74.5873) beta=11.00
Iter 13000 | Total loss: 41.0003 (MSE:0.0004, Reg:40.9999) beta=9.88
Iter 14000 | Total loss: 15.8938 (MSE:0.0003, Reg:15.8935) beta=8.75
Iter 15000 | Total loss: 7.1592 (MSE:0.0004, Reg:7.1588) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 70724.7188 (MSE:0.0003, Reg:70724.7188) beta=20.00
Iter  5000 | Total loss: 1049.1271 (MSE:0.0004, Reg:1049.1267) beta=18.88
Iter  6000 | Total loss: 555.9085 (MSE:0.0004, Reg:555.9081) beta=17.75
Iter  7000 | Total loss: 379.8890 (MSE:0.0004, Reg:379.8886) beta=16.62
Iter  8000 | Total loss: 281.2429 (MSE:0.0004, Reg:281.2425) beta=15.50
Iter  9000 | Total loss: 196.6960 (MSE:0.0004, Reg:196.6957) beta=14.38
Iter 10000 | Total loss: 146.7364 (MSE:0.0003, Reg:146.7361) beta=13.25
Iter 11000 | Total loss: 113.0003 (MSE:0.0003, Reg:113.0000) beta=12.12
Iter 12000 | Total loss: 72.5119 (MSE:0.0003, Reg:72.5116) beta=11.00
Iter 13000 | Total loss: 34.0004 (MSE:0.0004, Reg:34.0000) beta=9.88
Iter 14000 | Total loss: 18.9994 (MSE:0.0003, Reg:18.9991) beta=8.75
Iter 15000 | Total loss: 6.0004 (MSE:0.0004, Reg:6.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 120087.4766 (MSE:0.0024, Reg:120087.4766) beta=20.00
Iter  5000 | Total loss: 6356.7754 (MSE:0.0026, Reg:6356.7729) beta=18.88
Iter  6000 | Total loss: 4225.1260 (MSE:0.0026, Reg:4225.1235) beta=17.75
Iter  7000 | Total loss: 3136.4719 (MSE:0.0026, Reg:3136.4692) beta=16.62
Iter  8000 | Total loss: 2475.9705 (MSE:0.0026, Reg:2475.9678) beta=15.50
Iter  9000 | Total loss: 1896.8392 (MSE:0.0026, Reg:1896.8367) beta=14.38
Iter 10000 | Total loss: 1359.8120 (MSE:0.0025, Reg:1359.8094) beta=13.25
Iter 11000 | Total loss: 864.4458 (MSE:0.0027, Reg:864.4431) beta=12.12
Iter 12000 | Total loss: 480.2055 (MSE:0.0028, Reg:480.2027) beta=11.00
Iter 13000 | Total loss: 220.7481 (MSE:0.0027, Reg:220.7455) beta=9.88
Iter 14000 | Total loss: 68.6163 (MSE:0.0025, Reg:68.6139) beta=8.75
Iter 15000 | Total loss: 11.9921 (MSE:0.0027, Reg:11.9894) beta=7.62
Iter 16000 | Total loss: 3.0027 (MSE:0.0027, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 163251.5938 (MSE:0.0003, Reg:163251.5938) beta=20.00
Iter  5000 | Total loss: 935.0139 (MSE:0.0004, Reg:935.0134) beta=18.88
Iter  6000 | Total loss: 499.2164 (MSE:0.0004, Reg:499.2159) beta=17.75
Iter  7000 | Total loss: 329.5239 (MSE:0.0004, Reg:329.5235) beta=16.62
Iter  8000 | Total loss: 232.0002 (MSE:0.0004, Reg:231.9998) beta=15.50
Iter  9000 | Total loss: 173.9935 (MSE:0.0004, Reg:173.9931) beta=14.38
Iter 10000 | Total loss: 125.4184 (MSE:0.0004, Reg:125.4179) beta=13.25
Iter 11000 | Total loss: 83.7158 (MSE:0.0004, Reg:83.7154) beta=12.12
Iter 12000 | Total loss: 56.8157 (MSE:0.0004, Reg:56.8153) beta=11.00
Iter 13000 | Total loss: 32.8202 (MSE:0.0004, Reg:32.8198) beta=9.88
Iter 14000 | Total loss: 20.9607 (MSE:0.0004, Reg:20.9603) beta=8.75
Iter 15000 | Total loss: 7.0004 (MSE:0.0004, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0087 (MSE:0.0087, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 308049.4375 (MSE:0.0070, Reg:308049.4375) beta=20.00
Iter  5000 | Total loss: 24475.9062 (MSE:0.0076, Reg:24475.8984) beta=18.88
Iter  6000 | Total loss: 16696.3164 (MSE:0.0072, Reg:16696.3086) beta=17.75
Iter  7000 | Total loss: 12444.5918 (MSE:0.0077, Reg:12444.5840) beta=16.62
Iter  8000 | Total loss: 9403.7754 (MSE:0.0074, Reg:9403.7676) beta=15.50
Iter  9000 | Total loss: 6907.2480 (MSE:0.0072, Reg:6907.2407) beta=14.38
Iter 10000 | Total loss: 4746.1235 (MSE:0.0074, Reg:4746.1162) beta=13.25
Iter 11000 | Total loss: 2868.3345 (MSE:0.0076, Reg:2868.3269) beta=12.12
Iter 12000 | Total loss: 1415.8123 (MSE:0.0074, Reg:1415.8049) beta=11.00
Iter 13000 | Total loss: 495.3214 (MSE:0.0077, Reg:495.3137) beta=9.88
Iter 14000 | Total loss: 75.9924 (MSE:0.0071, Reg:75.9853) beta=8.75
Iter 15000 | Total loss: 4.7835 (MSE:0.0075, Reg:4.7760) beta=7.62
Iter 16000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30283.1914 (MSE:0.0022, Reg:30283.1895) beta=20.00
Iter  5000 | Total loss: 2094.9019 (MSE:0.0025, Reg:2094.8994) beta=18.88
Iter  6000 | Total loss: 1695.0306 (MSE:0.0024, Reg:1695.0283) beta=17.75
Iter  7000 | Total loss: 1447.8389 (MSE:0.0024, Reg:1447.8364) beta=16.62
Iter  8000 | Total loss: 1197.7749 (MSE:0.0025, Reg:1197.7725) beta=15.50
Iter  9000 | Total loss: 954.2853 (MSE:0.0023, Reg:954.2830) beta=14.38
Iter 10000 | Total loss: 681.9203 (MSE:0.0026, Reg:681.9177) beta=13.25
Iter 11000 | Total loss: 397.3510 (MSE:0.0025, Reg:397.3484) beta=12.12
Iter 12000 | Total loss: 201.0104 (MSE:0.0024, Reg:201.0079) beta=11.00
Iter 13000 | Total loss: 106.4342 (MSE:0.0024, Reg:106.4318) beta=9.88
Iter 14000 | Total loss: 30.0024 (MSE:0.0024, Reg:30.0000) beta=8.75
Iter 15000 | Total loss: 5.0027 (MSE:0.0027, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 339516.7188 (MSE:0.0005, Reg:339516.7188) beta=20.00
Iter  5000 | Total loss: 2176.2012 (MSE:0.0005, Reg:2176.2007) beta=18.88
Iter  6000 | Total loss: 1118.9062 (MSE:0.0006, Reg:1118.9056) beta=17.75
Iter  7000 | Total loss: 721.2886 (MSE:0.0006, Reg:721.2881) beta=16.62
Iter  8000 | Total loss: 483.0103 (MSE:0.0005, Reg:483.0098) beta=15.50
Iter  9000 | Total loss: 349.9981 (MSE:0.0006, Reg:349.9975) beta=14.38
Iter 10000 | Total loss: 256.3550 (MSE:0.0006, Reg:256.3544) beta=13.25
Iter 11000 | Total loss: 182.9381 (MSE:0.0006, Reg:182.9376) beta=12.12
Iter 12000 | Total loss: 109.0006 (MSE:0.0006, Reg:109.0000) beta=11.00
Iter 13000 | Total loss: 63.9990 (MSE:0.0006, Reg:63.9984) beta=9.88
Iter 14000 | Total loss: 32.0005 (MSE:0.0005, Reg:32.0000) beta=8.75
Iter 15000 | Total loss: 7.0005 (MSE:0.0005, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2491 (MSE:0.2491, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2241 (MSE:0.2241, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2170 (MSE:0.2170, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2035 (MSE:0.2035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 230311.3594 (MSE:0.2051, Reg:230311.1562) beta=20.00
Iter  5000 | Total loss: 43589.2500 (MSE:0.2009, Reg:43589.0508) beta=18.88
Iter  6000 | Total loss: 30646.4629 (MSE:0.2009, Reg:30646.2617) beta=17.75
Iter  7000 | Total loss: 21594.4863 (MSE:0.2087, Reg:21594.2773) beta=16.62
Iter  8000 | Total loss: 14778.1357 (MSE:0.2061, Reg:14777.9297) beta=15.50
Iter  9000 | Total loss: 9357.7900 (MSE:0.2093, Reg:9357.5811) beta=14.38
Iter 10000 | Total loss: 4998.2822 (MSE:0.1979, Reg:4998.0845) beta=13.25
Iter 11000 | Total loss: 2185.8508 (MSE:0.2064, Reg:2185.6443) beta=12.12
Iter 12000 | Total loss: 629.7406 (MSE:0.2133, Reg:629.5273) beta=11.00
Iter 13000 | Total loss: 101.1184 (MSE:0.1976, Reg:100.9209) beta=9.88
Iter 14000 | Total loss: 4.2186 (MSE:0.2186, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.2040 (MSE:0.2040, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2062 (MSE:0.2062, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2167 (MSE:0.2167, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2145 (MSE:0.2145, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2110 (MSE:0.2110, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2069 (MSE:0.2069, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1720 (MSE:0.1720, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1194 (MSE:0.1194, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1228 (MSE:0.1228, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1243 (MSE:0.1243, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 37539.6328 (MSE:0.1420, Reg:37539.4922) beta=20.00
Iter  5000 | Total loss: 7244.6597 (MSE:0.1309, Reg:7244.5288) beta=18.88
Iter  6000 | Total loss: 5413.0854 (MSE:0.1371, Reg:5412.9482) beta=17.75
Iter  7000 | Total loss: 4082.2964 (MSE:0.1397, Reg:4082.1567) beta=16.62
Iter  8000 | Total loss: 2956.0471 (MSE:0.1325, Reg:2955.9146) beta=15.50
Iter  9000 | Total loss: 1985.5065 (MSE:0.1322, Reg:1985.3743) beta=14.38
Iter 10000 | Total loss: 1136.1206 (MSE:0.1270, Reg:1135.9937) beta=13.25
Iter 11000 | Total loss: 531.2598 (MSE:0.1326, Reg:531.1271) beta=12.12
Iter 12000 | Total loss: 174.5075 (MSE:0.1247, Reg:174.3829) beta=11.00
Iter 13000 | Total loss: 38.0361 (MSE:0.1253, Reg:37.9108) beta=9.88
Iter 14000 | Total loss: 8.5146 (MSE:0.1184, Reg:8.3963) beta=8.75
Iter 15000 | Total loss: 3.1299 (MSE:0.1299, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.1264 (MSE:0.1264, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1290 (MSE:0.1290, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1221 (MSE:0.1221, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1201 (MSE:0.1201, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1353 (MSE:0.1353, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 69.046%
Total time: 861.07 sec


Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1894.5814 (MSE:0.0003, Reg:1894.5811) beta=20.00
Iter  5000 | Total loss: 18.0050 (MSE:0.0050, Reg:18.0000) beta=18.88
Iter  6000 | Total loss: 10.0036 (MSE:0.0036, Reg:10.0000) beta=17.75
Iter  7000 | Total loss: 7.0035 (MSE:0.0035, Reg:7.0000) beta=16.62
Iter  8000 | Total loss: 6.9583 (MSE:0.0037, Reg:6.9546) beta=15.50
Iter  9000 | Total loss: 2.8613 (MSE:0.0037, Reg:2.8575) beta=14.38
Iter 10000 | Total loss: 2.0039 (MSE:0.0039, Reg:2.0000) beta=13.25
Iter 11000 | Total loss: 0.6678 (MSE:0.0031, Reg:0.6647) beta=12.12
Iter 12000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5919.2241 (MSE:0.0009, Reg:5919.2231) beta=20.00
Iter  5000 | Total loss: 346.3573 (MSE:0.0031, Reg:346.3542) beta=18.88
Iter  6000 | Total loss: 157.8774 (MSE:0.0034, Reg:157.8740) beta=17.75
Iter  7000 | Total loss: 115.9707 (MSE:0.0036, Reg:115.9671) beta=16.62
Iter  8000 | Total loss: 92.1225 (MSE:0.0035, Reg:92.1190) beta=15.50
Iter  9000 | Total loss: 68.1714 (MSE:0.0034, Reg:68.1680) beta=14.38
Iter 10000 | Total loss: 40.2076 (MSE:0.0033, Reg:40.2044) beta=13.25
Iter 11000 | Total loss: 26.9036 (MSE:0.0034, Reg:26.9002) beta=12.12
Iter 12000 | Total loss: 10.7470 (MSE:0.0034, Reg:10.7435) beta=11.00
Iter 13000 | Total loss: 6.0034 (MSE:0.0034, Reg:6.0000) beta=9.88
Iter 14000 | Total loss: 4.0032 (MSE:0.0032, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 1.9279 (MSE:0.0034, Reg:1.9245) beta=7.62
Iter 16000 | Total loss: 0.0708 (MSE:0.0033, Reg:0.0676) beta=6.50
Iter 17000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7854.6206 (MSE:0.0026, Reg:7854.6182) beta=20.00
Iter  5000 | Total loss: 830.8742 (MSE:0.0032, Reg:830.8710) beta=18.88
Iter  6000 | Total loss: 519.7620 (MSE:0.0032, Reg:519.7588) beta=17.75
Iter  7000 | Total loss: 368.9632 (MSE:0.0033, Reg:368.9598) beta=16.62
Iter  8000 | Total loss: 292.9145 (MSE:0.0034, Reg:292.9111) beta=15.50
Iter  9000 | Total loss: 209.8838 (MSE:0.0033, Reg:209.8805) beta=14.38
Iter 10000 | Total loss: 145.3424 (MSE:0.0033, Reg:145.3391) beta=13.25
Iter 11000 | Total loss: 101.6704 (MSE:0.0031, Reg:101.6673) beta=12.12
Iter 12000 | Total loss: 63.4443 (MSE:0.0032, Reg:63.4410) beta=11.00
Iter 13000 | Total loss: 34.8839 (MSE:0.0030, Reg:34.8809) beta=9.88
Iter 14000 | Total loss: 16.9994 (MSE:0.0032, Reg:16.9962) beta=8.75
Iter 15000 | Total loss: 6.6132 (MSE:0.0032, Reg:6.6100) beta=7.62
Iter 16000 | Total loss: 2.0034 (MSE:0.0034, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.2349 (MSE:0.0037, Reg:0.2312) beta=5.38
Iter 18000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5123.5366 (MSE:0.0026, Reg:5123.5342) beta=20.00
Iter  5000 | Total loss: 596.6014 (MSE:0.0045, Reg:596.5969) beta=18.88
Iter  6000 | Total loss: 360.7272 (MSE:0.0051, Reg:360.7221) beta=17.75
Iter  7000 | Total loss: 272.4144 (MSE:0.0041, Reg:272.4103) beta=16.62
Iter  8000 | Total loss: 200.8479 (MSE:0.0044, Reg:200.8435) beta=15.50
Iter  9000 | Total loss: 141.0659 (MSE:0.0039, Reg:141.0620) beta=14.38
Iter 10000 | Total loss: 102.6204 (MSE:0.0047, Reg:102.6158) beta=13.25
Iter 11000 | Total loss: 70.4588 (MSE:0.0045, Reg:70.4543) beta=12.12
Iter 12000 | Total loss: 36.1187 (MSE:0.0043, Reg:36.1144) beta=11.00
Iter 13000 | Total loss: 17.6897 (MSE:0.0043, Reg:17.6854) beta=9.88
Iter 14000 | Total loss: 13.0327 (MSE:0.0043, Reg:13.0284) beta=8.75
Iter 15000 | Total loss: 5.3220 (MSE:0.0046, Reg:5.3173) beta=7.62
Iter 16000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0129 (MSE:0.0129, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0100 (MSE:0.0100, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8732.3711 (MSE:0.0097, Reg:8732.3613) beta=20.00
Iter  5000 | Total loss: 1604.4406 (MSE:0.0099, Reg:1604.4307) beta=18.88
Iter  6000 | Total loss: 1109.5648 (MSE:0.0096, Reg:1109.5552) beta=17.75
Iter  7000 | Total loss: 832.1910 (MSE:0.0104, Reg:832.1805) beta=16.62
Iter  8000 | Total loss: 627.8387 (MSE:0.0095, Reg:627.8292) beta=15.50
Iter  9000 | Total loss: 481.6007 (MSE:0.0093, Reg:481.5914) beta=14.38
Iter 10000 | Total loss: 352.0260 (MSE:0.0102, Reg:352.0159) beta=13.25
Iter 11000 | Total loss: 244.8033 (MSE:0.0094, Reg:244.7940) beta=12.12
Iter 12000 | Total loss: 163.4970 (MSE:0.0097, Reg:163.4873) beta=11.00
Iter 13000 | Total loss: 105.5437 (MSE:0.0104, Reg:105.5334) beta=9.88
Iter 14000 | Total loss: 59.1948 (MSE:0.0106, Reg:59.1842) beta=8.75
Iter 15000 | Total loss: 30.7960 (MSE:0.0103, Reg:30.7857) beta=7.62
Iter 16000 | Total loss: 13.4699 (MSE:0.0107, Reg:13.4592) beta=6.50
Iter 17000 | Total loss: 4.0096 (MSE:0.0097, Reg:3.9999) beta=5.38
Iter 18000 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0097 (MSE:0.0097, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10377.5137 (MSE:0.0044, Reg:10377.5098) beta=20.00
Iter  5000 | Total loss: 886.6810 (MSE:0.0045, Reg:886.6765) beta=18.88
Iter  6000 | Total loss: 524.8576 (MSE:0.0046, Reg:524.8530) beta=17.75
Iter  7000 | Total loss: 352.9565 (MSE:0.0045, Reg:352.9520) beta=16.62
Iter  8000 | Total loss: 251.5590 (MSE:0.0046, Reg:251.5544) beta=15.50
Iter  9000 | Total loss: 181.3835 (MSE:0.0044, Reg:181.3791) beta=14.38
Iter 10000 | Total loss: 125.4851 (MSE:0.0048, Reg:125.4802) beta=13.25
Iter 11000 | Total loss: 78.2492 (MSE:0.0049, Reg:78.2443) beta=12.12
Iter 12000 | Total loss: 54.6988 (MSE:0.0050, Reg:54.6938) beta=11.00
Iter 13000 | Total loss: 34.3432 (MSE:0.0047, Reg:34.3385) beta=9.88
Iter 14000 | Total loss: 10.9394 (MSE:0.0048, Reg:10.9346) beta=8.75
Iter 15000 | Total loss: 4.6911 (MSE:0.0048, Reg:4.6863) beta=7.62
Iter 16000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 22679.0957 (MSE:0.0065, Reg:22679.0898) beta=20.00
Iter  5000 | Total loss: 1487.2200 (MSE:0.0068, Reg:1487.2131) beta=18.88
Iter  6000 | Total loss: 904.6095 (MSE:0.0065, Reg:904.6030) beta=17.75
Iter  7000 | Total loss: 600.3893 (MSE:0.0061, Reg:600.3832) beta=16.62
Iter  8000 | Total loss: 438.1412 (MSE:0.0064, Reg:438.1348) beta=15.50
Iter  9000 | Total loss: 336.3550 (MSE:0.0063, Reg:336.3488) beta=14.38
Iter 10000 | Total loss: 266.5274 (MSE:0.0068, Reg:266.5206) beta=13.25
Iter 11000 | Total loss: 202.2555 (MSE:0.0061, Reg:202.2494) beta=12.12
Iter 12000 | Total loss: 146.1024 (MSE:0.0063, Reg:146.0961) beta=11.00
Iter 13000 | Total loss: 98.0533 (MSE:0.0062, Reg:98.0470) beta=9.88
Iter 14000 | Total loss: 56.6158 (MSE:0.0064, Reg:56.6095) beta=8.75
Iter 15000 | Total loss: 26.1891 (MSE:0.0065, Reg:26.1826) beta=7.62
Iter 16000 | Total loss: 5.3662 (MSE:0.0063, Reg:5.3599) beta=6.50
Iter 17000 | Total loss: 1.0061 (MSE:0.0061, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.1871 (MSE:0.0069, Reg:0.1803) beta=4.25
Iter 19000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2405.2815 (MSE:0.0027, Reg:2405.2788) beta=20.00
Iter  5000 | Total loss: 233.0263 (MSE:0.0028, Reg:233.0235) beta=18.88
Iter  6000 | Total loss: 147.5879 (MSE:0.0028, Reg:147.5851) beta=17.75
Iter  7000 | Total loss: 118.7620 (MSE:0.0028, Reg:118.7592) beta=16.62
Iter  8000 | Total loss: 96.9384 (MSE:0.0029, Reg:96.9354) beta=15.50
Iter  9000 | Total loss: 77.6186 (MSE:0.0028, Reg:77.6158) beta=14.38
Iter 10000 | Total loss: 60.7579 (MSE:0.0029, Reg:60.7550) beta=13.25
Iter 11000 | Total loss: 49.1646 (MSE:0.0028, Reg:49.1617) beta=12.12
Iter 12000 | Total loss: 36.8993 (MSE:0.0030, Reg:36.8963) beta=11.00
Iter 13000 | Total loss: 18.4752 (MSE:0.0027, Reg:18.4725) beta=9.88
Iter 14000 | Total loss: 14.0012 (MSE:0.0027, Reg:13.9985) beta=8.75
Iter 15000 | Total loss: 8.3406 (MSE:0.0029, Reg:8.3377) beta=7.62
Iter 16000 | Total loss: 2.8864 (MSE:0.0027, Reg:2.8837) beta=6.50
Iter 17000 | Total loss: 1.2723 (MSE:0.0028, Reg:1.2696) beta=5.38
Iter 18000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 22890.3887 (MSE:0.0050, Reg:22890.3828) beta=20.00
Iter  5000 | Total loss: 1768.8705 (MSE:0.0055, Reg:1768.8650) beta=18.88
Iter  6000 | Total loss: 944.4957 (MSE:0.0051, Reg:944.4905) beta=17.75
Iter  7000 | Total loss: 619.2271 (MSE:0.0054, Reg:619.2217) beta=16.62
Iter  8000 | Total loss: 458.9341 (MSE:0.0052, Reg:458.9288) beta=15.50
Iter  9000 | Total loss: 364.3842 (MSE:0.0053, Reg:364.3789) beta=14.38
Iter 10000 | Total loss: 270.4099 (MSE:0.0053, Reg:270.4047) beta=13.25
Iter 11000 | Total loss: 187.6703 (MSE:0.0060, Reg:187.6644) beta=12.12
Iter 12000 | Total loss: 128.1203 (MSE:0.0052, Reg:128.1151) beta=11.00
Iter 13000 | Total loss: 77.7086 (MSE:0.0055, Reg:77.7032) beta=9.88
Iter 14000 | Total loss: 51.3255 (MSE:0.0055, Reg:51.3200) beta=8.75
Iter 15000 | Total loss: 22.8659 (MSE:0.0056, Reg:22.8603) beta=7.62
Iter 16000 | Total loss: 5.9442 (MSE:0.0054, Reg:5.9387) beta=6.50
Iter 17000 | Total loss: 2.0025 (MSE:0.0053, Reg:1.9971) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38041.4180 (MSE:0.0058, Reg:38041.4141) beta=20.00
Iter  5000 | Total loss: 2864.6621 (MSE:0.0062, Reg:2864.6560) beta=18.88
Iter  6000 | Total loss: 1636.5909 (MSE:0.0055, Reg:1636.5854) beta=17.75
Iter  7000 | Total loss: 1111.4446 (MSE:0.0058, Reg:1111.4387) beta=16.62
Iter  8000 | Total loss: 866.4323 (MSE:0.0059, Reg:866.4263) beta=15.50
Iter  9000 | Total loss: 686.5249 (MSE:0.0064, Reg:686.5186) beta=14.38
Iter 10000 | Total loss: 548.3574 (MSE:0.0061, Reg:548.3514) beta=13.25
Iter 11000 | Total loss: 417.5692 (MSE:0.0057, Reg:417.5635) beta=12.12
Iter 12000 | Total loss: 293.8271 (MSE:0.0061, Reg:293.8211) beta=11.00
Iter 13000 | Total loss: 199.7953 (MSE:0.0060, Reg:199.7893) beta=9.88
Iter 14000 | Total loss: 121.8941 (MSE:0.0057, Reg:121.8884) beta=8.75
Iter 15000 | Total loss: 55.7413 (MSE:0.0060, Reg:55.7353) beta=7.62
Iter 16000 | Total loss: 16.5809 (MSE:0.0059, Reg:16.5750) beta=6.50
Iter 17000 | Total loss: 1.2613 (MSE:0.0056, Reg:1.2557) beta=5.38
Iter 18000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 54842.0742 (MSE:0.0047, Reg:54842.0703) beta=20.00
Iter  5000 | Total loss: 2026.2443 (MSE:0.0054, Reg:2026.2389) beta=18.88
Iter  6000 | Total loss: 987.5607 (MSE:0.0049, Reg:987.5558) beta=17.75
Iter  7000 | Total loss: 540.2808 (MSE:0.0052, Reg:540.2755) beta=16.62
Iter  8000 | Total loss: 388.0893 (MSE:0.0054, Reg:388.0840) beta=15.50
Iter  9000 | Total loss: 296.6269 (MSE:0.0056, Reg:296.6213) beta=14.38
Iter 10000 | Total loss: 222.1335 (MSE:0.0052, Reg:222.1283) beta=13.25
Iter 11000 | Total loss: 166.1494 (MSE:0.0054, Reg:166.1440) beta=12.12
Iter 12000 | Total loss: 123.9180 (MSE:0.0053, Reg:123.9127) beta=11.00
Iter 13000 | Total loss: 96.4436 (MSE:0.0055, Reg:96.4381) beta=9.88
Iter 14000 | Total loss: 59.8097 (MSE:0.0053, Reg:59.8044) beta=8.75
Iter 15000 | Total loss: 25.2067 (MSE:0.0052, Reg:25.2015) beta=7.62
Iter 16000 | Total loss: 9.9273 (MSE:0.0054, Reg:9.9218) beta=6.50
Iter 17000 | Total loss: 0.5137 (MSE:0.0051, Reg:0.5085) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 100905.3672 (MSE:0.0054, Reg:100905.3594) beta=20.00
Iter  5000 | Total loss: 2529.8950 (MSE:0.0064, Reg:2529.8887) beta=18.88
Iter  6000 | Total loss: 1268.3129 (MSE:0.0058, Reg:1268.3070) beta=17.75
Iter  7000 | Total loss: 789.3531 (MSE:0.0065, Reg:789.3467) beta=16.62
Iter  8000 | Total loss: 555.5748 (MSE:0.0063, Reg:555.5685) beta=15.50
Iter  9000 | Total loss: 412.4935 (MSE:0.0064, Reg:412.4871) beta=14.38
Iter 10000 | Total loss: 301.5674 (MSE:0.0065, Reg:301.5610) beta=13.25
Iter 11000 | Total loss: 231.9845 (MSE:0.0061, Reg:231.9783) beta=12.12
Iter 12000 | Total loss: 176.0001 (MSE:0.0064, Reg:175.9937) beta=11.00
Iter 13000 | Total loss: 124.7443 (MSE:0.0060, Reg:124.7383) beta=9.88
Iter 14000 | Total loss: 79.1744 (MSE:0.0062, Reg:79.1682) beta=8.75
Iter 15000 | Total loss: 38.5738 (MSE:0.0059, Reg:38.5679) beta=7.62
Iter 16000 | Total loss: 11.0213 (MSE:0.0062, Reg:11.0151) beta=6.50
Iter 17000 | Total loss: 1.4831 (MSE:0.0063, Reg:1.4767) beta=5.38
Iter 18000 | Total loss: 0.2646 (MSE:0.0059, Reg:0.2587) beta=4.25
Iter 19000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 12298.6104 (MSE:0.0005, Reg:12298.6094) beta=20.00
Iter  5000 | Total loss: 288.0953 (MSE:0.0006, Reg:288.0947) beta=18.88
Iter  6000 | Total loss: 158.3954 (MSE:0.0006, Reg:158.3948) beta=17.75
Iter  7000 | Total loss: 100.6804 (MSE:0.0006, Reg:100.6799) beta=16.62
Iter  8000 | Total loss: 80.7919 (MSE:0.0006, Reg:80.7913) beta=15.50
Iter  9000 | Total loss: 63.9000 (MSE:0.0006, Reg:63.8994) beta=14.38
Iter 10000 | Total loss: 56.6806 (MSE:0.0006, Reg:56.6801) beta=13.25
Iter 11000 | Total loss: 47.8024 (MSE:0.0006, Reg:47.8018) beta=12.12
Iter 12000 | Total loss: 38.5950 (MSE:0.0006, Reg:38.5944) beta=11.00
Iter 13000 | Total loss: 20.3805 (MSE:0.0006, Reg:20.3799) beta=9.88
Iter 14000 | Total loss: 11.0006 (MSE:0.0006, Reg:11.0000) beta=8.75
Iter 15000 | Total loss: 7.2809 (MSE:0.0006, Reg:7.2803) beta=7.62
Iter 16000 | Total loss: 3.0006 (MSE:0.0006, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 1.4289 (MSE:0.0006, Reg:1.4283) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 96512.2422 (MSE:0.0034, Reg:96512.2422) beta=20.00
Iter  5000 | Total loss: 996.8775 (MSE:0.0044, Reg:996.8730) beta=18.88
Iter  6000 | Total loss: 461.7430 (MSE:0.0043, Reg:461.7387) beta=17.75
Iter  7000 | Total loss: 283.4308 (MSE:0.0042, Reg:283.4266) beta=16.62
Iter  8000 | Total loss: 205.8248 (MSE:0.0043, Reg:205.8205) beta=15.50
Iter  9000 | Total loss: 163.4412 (MSE:0.0042, Reg:163.4369) beta=14.38
Iter 10000 | Total loss: 110.8370 (MSE:0.0041, Reg:110.8330) beta=13.25
Iter 11000 | Total loss: 86.0335 (MSE:0.0040, Reg:86.0296) beta=12.12
Iter 12000 | Total loss: 63.7795 (MSE:0.0042, Reg:63.7753) beta=11.00
Iter 13000 | Total loss: 38.6421 (MSE:0.0043, Reg:38.6378) beta=9.88
Iter 14000 | Total loss: 22.0788 (MSE:0.0041, Reg:22.0746) beta=8.75
Iter 15000 | Total loss: 12.0268 (MSE:0.0043, Reg:12.0225) beta=7.62
Iter 16000 | Total loss: 7.2177 (MSE:0.0044, Reg:7.2134) beta=6.50
Iter 17000 | Total loss: 3.1159 (MSE:0.0044, Reg:3.1115) beta=5.38
Iter 18000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 138446.5000 (MSE:0.0045, Reg:138446.5000) beta=20.00
Iter  5000 | Total loss: 2512.1221 (MSE:0.0053, Reg:2512.1167) beta=18.88
Iter  6000 | Total loss: 1270.3126 (MSE:0.0051, Reg:1270.3076) beta=17.75
Iter  7000 | Total loss: 838.6663 (MSE:0.0049, Reg:838.6614) beta=16.62
Iter  8000 | Total loss: 600.2285 (MSE:0.0051, Reg:600.2234) beta=15.50
Iter  9000 | Total loss: 453.8498 (MSE:0.0052, Reg:453.8445) beta=14.38
Iter 10000 | Total loss: 348.8427 (MSE:0.0050, Reg:348.8378) beta=13.25
Iter 11000 | Total loss: 262.9081 (MSE:0.0051, Reg:262.9030) beta=12.12
Iter 12000 | Total loss: 190.7788 (MSE:0.0053, Reg:190.7735) beta=11.00
Iter 13000 | Total loss: 136.4290 (MSE:0.0052, Reg:136.4238) beta=9.88
Iter 14000 | Total loss: 84.8403 (MSE:0.0048, Reg:84.8355) beta=8.75
Iter 15000 | Total loss: 41.4857 (MSE:0.0052, Reg:41.4805) beta=7.62
Iter 16000 | Total loss: 20.2067 (MSE:0.0051, Reg:20.2016) beta=6.50
Iter 17000 | Total loss: 2.9447 (MSE:0.0054, Reg:2.9393) beta=5.38
Iter 18000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 220281.5938 (MSE:0.0040, Reg:220281.5938) beta=20.00
Iter  5000 | Total loss: 285.2413 (MSE:0.0044, Reg:285.2369) beta=18.88
Iter  6000 | Total loss: 161.7646 (MSE:0.0046, Reg:161.7600) beta=17.75
Iter  7000 | Total loss: 111.5958 (MSE:0.0046, Reg:111.5913) beta=16.62
Iter  8000 | Total loss: 74.2801 (MSE:0.0044, Reg:74.2757) beta=15.50
Iter  9000 | Total loss: 50.0379 (MSE:0.0043, Reg:50.0336) beta=14.38
Iter 10000 | Total loss: 36.5374 (MSE:0.0046, Reg:36.5329) beta=13.25
Iter 11000 | Total loss: 27.0044 (MSE:0.0046, Reg:26.9998) beta=12.12
Iter 12000 | Total loss: 18.3827 (MSE:0.0045, Reg:18.3782) beta=11.00
Iter 13000 | Total loss: 8.7073 (MSE:0.0045, Reg:8.7028) beta=9.88
Iter 14000 | Total loss: 5.0046 (MSE:0.0046, Reg:5.0000) beta=8.75
Iter 15000 | Total loss: 2.0044 (MSE:0.0044, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0140 (MSE:0.0140, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0138 (MSE:0.0138, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0139 (MSE:0.0139, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 447132.3750 (MSE:0.0133, Reg:447132.3750) beta=20.00
Iter  5000 | Total loss: 4018.0854 (MSE:0.0156, Reg:4018.0698) beta=18.88
Iter  6000 | Total loss: 2148.4392 (MSE:0.0143, Reg:2148.4248) beta=17.75
Iter  7000 | Total loss: 1421.1912 (MSE:0.0155, Reg:1421.1757) beta=16.62
Iter  8000 | Total loss: 1034.5302 (MSE:0.0150, Reg:1034.5151) beta=15.50
Iter  9000 | Total loss: 767.2800 (MSE:0.0148, Reg:767.2651) beta=14.38
Iter 10000 | Total loss: 570.1057 (MSE:0.0148, Reg:570.0908) beta=13.25
Iter 11000 | Total loss: 432.5657 (MSE:0.0152, Reg:432.5505) beta=12.12
Iter 12000 | Total loss: 327.1343 (MSE:0.0152, Reg:327.1191) beta=11.00
Iter 13000 | Total loss: 248.8826 (MSE:0.0150, Reg:248.8676) beta=9.88
Iter 14000 | Total loss: 157.0778 (MSE:0.0142, Reg:157.0636) beta=8.75
Iter 15000 | Total loss: 82.2605 (MSE:0.0147, Reg:82.2459) beta=7.62
Iter 16000 | Total loss: 35.0873 (MSE:0.0149, Reg:35.0723) beta=6.50
Iter 17000 | Total loss: 6.6256 (MSE:0.0152, Reg:6.6104) beta=5.38
Iter 18000 | Total loss: 0.3379 (MSE:0.0152, Reg:0.3228) beta=4.25
Iter 19000 | Total loss: 0.0147 (MSE:0.0147, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0153 (MSE:0.0153, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38783.4961 (MSE:0.0040, Reg:38783.4922) beta=20.00
Iter  5000 | Total loss: 1829.5867 (MSE:0.0048, Reg:1829.5819) beta=18.88
Iter  6000 | Total loss: 1005.8950 (MSE:0.0044, Reg:1005.8906) beta=17.75
Iter  7000 | Total loss: 701.5963 (MSE:0.0043, Reg:701.5919) beta=16.62
Iter  8000 | Total loss: 508.6880 (MSE:0.0046, Reg:508.6834) beta=15.50
Iter  9000 | Total loss: 408.1871 (MSE:0.0044, Reg:408.1827) beta=14.38
Iter 10000 | Total loss: 326.5422 (MSE:0.0048, Reg:326.5374) beta=13.25
Iter 11000 | Total loss: 253.6719 (MSE:0.0046, Reg:253.6673) beta=12.12
Iter 12000 | Total loss: 192.3283 (MSE:0.0045, Reg:192.3238) beta=11.00
Iter 13000 | Total loss: 130.7073 (MSE:0.0044, Reg:130.7029) beta=9.88
Iter 14000 | Total loss: 83.6860 (MSE:0.0045, Reg:83.6815) beta=8.75
Iter 15000 | Total loss: 40.3665 (MSE:0.0049, Reg:40.3616) beta=7.62
Iter 16000 | Total loss: 12.3421 (MSE:0.0044, Reg:12.3377) beta=6.50
Iter 17000 | Total loss: 4.0950 (MSE:0.0046, Reg:4.0904) beta=5.38
Iter 18000 | Total loss: 0.5919 (MSE:0.0045, Reg:0.5875) beta=4.25
Iter 19000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 429501.4375 (MSE:0.0062, Reg:429501.4375) beta=20.00
Iter  5000 | Total loss: 450.7947 (MSE:0.0065, Reg:450.7881) beta=18.88
Iter  6000 | Total loss: 271.5999 (MSE:0.0073, Reg:271.5926) beta=17.75
Iter  7000 | Total loss: 193.9977 (MSE:0.0073, Reg:193.9904) beta=16.62
Iter  8000 | Total loss: 129.2300 (MSE:0.0069, Reg:129.2230) beta=15.50
Iter  9000 | Total loss: 96.8683 (MSE:0.0069, Reg:96.8614) beta=14.38
Iter 10000 | Total loss: 80.5592 (MSE:0.0072, Reg:80.5520) beta=13.25
Iter 11000 | Total loss: 58.9990 (MSE:0.0073, Reg:58.9917) beta=12.12
Iter 12000 | Total loss: 47.9457 (MSE:0.0070, Reg:47.9387) beta=11.00
Iter 13000 | Total loss: 29.1364 (MSE:0.0071, Reg:29.1294) beta=9.88
Iter 14000 | Total loss: 23.8963 (MSE:0.0070, Reg:23.8893) beta=8.75
Iter 15000 | Total loss: 15.1284 (MSE:0.0073, Reg:15.1210) beta=7.62
Iter 16000 | Total loss: 7.4391 (MSE:0.0067, Reg:7.4324) beta=6.50
Iter 17000 | Total loss: 2.0075 (MSE:0.0075, Reg:2.0000) beta=5.38
Iter 18000 | Total loss: 0.1706 (MSE:0.0071, Reg:0.1635) beta=4.25
Iter 19000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.4324 (MSE:0.4324, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4523 (MSE:0.4523, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4438 (MSE:0.4438, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4149 (MSE:0.4149, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 367406.2188 (MSE:0.3923, Reg:367405.8125) beta=20.00
Iter  5000 | Total loss: 22431.9980 (MSE:0.4153, Reg:22431.5820) beta=18.88
Iter  6000 | Total loss: 13029.1357 (MSE:0.4258, Reg:13028.7100) beta=17.75
Iter  7000 | Total loss: 8812.4453 (MSE:0.4417, Reg:8812.0039) beta=16.62
Iter  8000 | Total loss: 6268.3628 (MSE:0.4517, Reg:6267.9111) beta=15.50
Iter  9000 | Total loss: 4904.5508 (MSE:0.4173, Reg:4904.1333) beta=14.38
Iter 10000 | Total loss: 3978.4292 (MSE:0.4030, Reg:3978.0261) beta=13.25
Iter 11000 | Total loss: 3222.0452 (MSE:0.4197, Reg:3221.6255) beta=12.12
Iter 12000 | Total loss: 2499.0369 (MSE:0.4343, Reg:2498.6025) beta=11.00
Iter 13000 | Total loss: 1875.9680 (MSE:0.3879, Reg:1875.5801) beta=9.88
Iter 14000 | Total loss: 1300.8464 (MSE:0.4467, Reg:1300.3997) beta=8.75
Iter 15000 | Total loss: 806.8417 (MSE:0.4295, Reg:806.4122) beta=7.62
Iter 16000 | Total loss: 419.0840 (MSE:0.4140, Reg:418.6700) beta=6.50
Iter 17000 | Total loss: 114.8605 (MSE:0.4362, Reg:114.4243) beta=5.38
Iter 18000 | Total loss: 6.0729 (MSE:0.4315, Reg:5.6414) beta=4.25
Iter 19000 | Total loss: 0.4252 (MSE:0.4252, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4120 (MSE:0.4120, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3636 (MSE:0.3636, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3269 (MSE:0.3269, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2866 (MSE:0.2866, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3028 (MSE:0.3028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 89320.7344 (MSE:0.2985, Reg:89320.4375) beta=20.00
Iter  5000 | Total loss: 2332.0583 (MSE:0.3767, Reg:2331.6816) beta=18.88
Iter  6000 | Total loss: 1071.2986 (MSE:0.3679, Reg:1070.9307) beta=17.75
Iter  7000 | Total loss: 711.9579 (MSE:0.4113, Reg:711.5465) beta=16.62
Iter  8000 | Total loss: 510.6532 (MSE:0.3521, Reg:510.3011) beta=15.50
Iter  9000 | Total loss: 394.1697 (MSE:0.3652, Reg:393.8045) beta=14.38
Iter 10000 | Total loss: 317.9915 (MSE:0.3428, Reg:317.6487) beta=13.25
Iter 11000 | Total loss: 261.0566 (MSE:0.3536, Reg:260.7030) beta=12.12
Iter 12000 | Total loss: 206.1630 (MSE:0.3149, Reg:205.8482) beta=11.00
Iter 13000 | Total loss: 164.1552 (MSE:0.4119, Reg:163.7433) beta=9.88
Iter 14000 | Total loss: 125.1673 (MSE:0.3260, Reg:124.8413) beta=8.75
Iter 15000 | Total loss: 80.5332 (MSE:0.3639, Reg:80.1693) beta=7.62
Iter 16000 | Total loss: 39.8860 (MSE:0.3468, Reg:39.5392) beta=6.50
Iter 17000 | Total loss: 19.4973 (MSE:0.3342, Reg:19.1631) beta=5.38
Iter 18000 | Total loss: 3.7483 (MSE:0.3638, Reg:3.3846) beta=4.25
Iter 19000 | Total loss: 0.3586 (MSE:0.3586, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3573 (MSE:0.3573, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 68.096%
Total time: 867.66 sec


Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A32_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1183.5317 (MSE:0.0005, Reg:1183.5312) beta=20.00
Iter  5000 | Total loss: 59.0010 (MSE:0.0010, Reg:59.0000) beta=18.88
Iter  6000 | Total loss: 40.0007 (MSE:0.0007, Reg:40.0000) beta=17.75
Iter  7000 | Total loss: 23.0009 (MSE:0.0009, Reg:23.0000) beta=16.62
Iter  8000 | Total loss: 18.0017 (MSE:0.0017, Reg:18.0000) beta=15.50
Iter  9000 | Total loss: 13.0007 (MSE:0.0007, Reg:13.0000) beta=14.38
Iter 10000 | Total loss: 8.0010 (MSE:0.0010, Reg:8.0000) beta=13.25
Iter 11000 | Total loss: 3.0010 (MSE:0.0010, Reg:3.0000) beta=12.12
Iter 12000 | Total loss: 3.0009 (MSE:0.0009, Reg:3.0000) beta=11.00
Iter 13000 | Total loss: 3.0009 (MSE:0.0009, Reg:3.0000) beta=9.88
Iter 14000 | Total loss: 2.0010 (MSE:0.0010, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5659.2466 (MSE:0.0004, Reg:5659.2461) beta=20.00
Iter  5000 | Total loss: 347.7043 (MSE:0.0007, Reg:347.7036) beta=18.88
Iter  6000 | Total loss: 182.1024 (MSE:0.0005, Reg:182.1019) beta=17.75
Iter  7000 | Total loss: 101.7768 (MSE:0.0004, Reg:101.7764) beta=16.62
Iter  8000 | Total loss: 72.5761 (MSE:0.0004, Reg:72.5757) beta=15.50
Iter  9000 | Total loss: 49.0007 (MSE:0.0008, Reg:48.9999) beta=14.38
Iter 10000 | Total loss: 34.0004 (MSE:0.0004, Reg:34.0000) beta=13.25
Iter 11000 | Total loss: 22.0004 (MSE:0.0004, Reg:22.0000) beta=12.12
Iter 12000 | Total loss: 10.3142 (MSE:0.0005, Reg:10.3137) beta=11.00
Iter 13000 | Total loss: 2.0004 (MSE:0.0004, Reg:2.0000) beta=9.88
Iter 14000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9831.3535 (MSE:0.0015, Reg:9831.3516) beta=20.00
Iter  5000 | Total loss: 791.7328 (MSE:0.0009, Reg:791.7319) beta=18.88
Iter  6000 | Total loss: 595.5872 (MSE:0.0024, Reg:595.5848) beta=17.75
Iter  7000 | Total loss: 475.0012 (MSE:0.0012, Reg:475.0000) beta=16.62
Iter  8000 | Total loss: 383.0014 (MSE:0.0014, Reg:383.0000) beta=15.50
Iter  9000 | Total loss: 285.7057 (MSE:0.0020, Reg:285.7037) beta=14.38
Iter 10000 | Total loss: 207.0008 (MSE:0.0011, Reg:206.9997) beta=13.25
Iter 11000 | Total loss: 159.9592 (MSE:0.0014, Reg:159.9578) beta=12.12
Iter 12000 | Total loss: 111.9014 (MSE:0.0020, Reg:111.8994) beta=11.00
Iter 13000 | Total loss: 63.8198 (MSE:0.0029, Reg:63.8169) beta=9.88
Iter 14000 | Total loss: 32.0018 (MSE:0.0018, Reg:32.0000) beta=8.75
Iter 15000 | Total loss: 7.9979 (MSE:0.0018, Reg:7.9961) beta=7.62
Iter 16000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10278.2666 (MSE:0.0003, Reg:10278.2666) beta=20.00
Iter  5000 | Total loss: 840.4924 (MSE:0.0004, Reg:840.4920) beta=18.88
Iter  6000 | Total loss: 574.9972 (MSE:0.0005, Reg:574.9966) beta=17.75
Iter  7000 | Total loss: 432.4799 (MSE:0.0004, Reg:432.4795) beta=16.62
Iter  8000 | Total loss: 329.7609 (MSE:0.0007, Reg:329.7602) beta=15.50
Iter  9000 | Total loss: 251.9469 (MSE:0.0005, Reg:251.9464) beta=14.38
Iter 10000 | Total loss: 172.1015 (MSE:0.0006, Reg:172.1009) beta=13.25
Iter 11000 | Total loss: 120.0004 (MSE:0.0004, Reg:119.9999) beta=12.12
Iter 12000 | Total loss: 58.0004 (MSE:0.0004, Reg:58.0000) beta=11.00
Iter 13000 | Total loss: 26.9454 (MSE:0.0005, Reg:26.9449) beta=9.88
Iter 14000 | Total loss: 14.0006 (MSE:0.0006, Reg:14.0000) beta=8.75
Iter 15000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15368.9512 (MSE:0.0027, Reg:15368.9482) beta=20.00
Iter  5000 | Total loss: 1548.7637 (MSE:0.0031, Reg:1548.7605) beta=18.88
Iter  6000 | Total loss: 1225.6420 (MSE:0.0033, Reg:1225.6387) beta=17.75
Iter  7000 | Total loss: 1023.7878 (MSE:0.0033, Reg:1023.7845) beta=16.62
Iter  8000 | Total loss: 809.9270 (MSE:0.0036, Reg:809.9235) beta=15.50
Iter  9000 | Total loss: 621.9905 (MSE:0.0031, Reg:621.9875) beta=14.38
Iter 10000 | Total loss: 441.9857 (MSE:0.0036, Reg:441.9821) beta=13.25
Iter 11000 | Total loss: 303.8685 (MSE:0.0033, Reg:303.8652) beta=12.12
Iter 12000 | Total loss: 175.9346 (MSE:0.0035, Reg:175.9311) beta=11.00
Iter 13000 | Total loss: 83.6325 (MSE:0.0033, Reg:83.6292) beta=9.88
Iter 14000 | Total loss: 50.0576 (MSE:0.0037, Reg:50.0539) beta=8.75
Iter 15000 | Total loss: 10.0036 (MSE:0.0036, Reg:10.0000) beta=7.62
Iter 16000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 27917.9102 (MSE:0.0005, Reg:27917.9102) beta=20.00
Iter  5000 | Total loss: 2235.3789 (MSE:0.0006, Reg:2235.3784) beta=18.88
Iter  6000 | Total loss: 1386.9675 (MSE:0.0005, Reg:1386.9670) beta=17.75
Iter  7000 | Total loss: 1032.3239 (MSE:0.0005, Reg:1032.3234) beta=16.62
Iter  8000 | Total loss: 844.3256 (MSE:0.0006, Reg:844.3250) beta=15.50
Iter  9000 | Total loss: 659.4760 (MSE:0.0005, Reg:659.4755) beta=14.38
Iter 10000 | Total loss: 492.4489 (MSE:0.0006, Reg:492.4482) beta=13.25
Iter 11000 | Total loss: 329.9779 (MSE:0.0005, Reg:329.9774) beta=12.12
Iter 12000 | Total loss: 208.4500 (MSE:0.0006, Reg:208.4494) beta=11.00
Iter 13000 | Total loss: 121.8287 (MSE:0.0005, Reg:121.8281) beta=9.88
Iter 14000 | Total loss: 32.0005 (MSE:0.0005, Reg:32.0000) beta=8.75
Iter 15000 | Total loss: 7.0006 (MSE:0.0006, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 72472.6016 (MSE:0.0032, Reg:72472.6016) beta=20.00
Iter  5000 | Total loss: 6111.3599 (MSE:0.0030, Reg:6111.3569) beta=18.88
Iter  6000 | Total loss: 4730.0981 (MSE:0.0029, Reg:4730.0952) beta=17.75
Iter  7000 | Total loss: 3920.3906 (MSE:0.0026, Reg:3920.3879) beta=16.62
Iter  8000 | Total loss: 3234.9717 (MSE:0.0034, Reg:3234.9683) beta=15.50
Iter  9000 | Total loss: 2497.0542 (MSE:0.0026, Reg:2497.0518) beta=14.38
Iter 10000 | Total loss: 1812.5013 (MSE:0.0038, Reg:1812.4976) beta=13.25
Iter 11000 | Total loss: 1235.1537 (MSE:0.0025, Reg:1235.1511) beta=12.12
Iter 12000 | Total loss: 770.0276 (MSE:0.0026, Reg:770.0250) beta=11.00
Iter 13000 | Total loss: 407.8617 (MSE:0.0026, Reg:407.8590) beta=9.88
Iter 14000 | Total loss: 194.1478 (MSE:0.0031, Reg:194.1447) beta=8.75
Iter 15000 | Total loss: 47.6341 (MSE:0.0033, Reg:47.6307) beta=7.62
Iter 16000 | Total loss: 0.7789 (MSE:0.0030, Reg:0.7759) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5452.9287 (MSE:0.0010, Reg:5452.9277) beta=20.00
Iter  5000 | Total loss: 889.3369 (MSE:0.0011, Reg:889.3358) beta=18.88
Iter  6000 | Total loss: 792.0013 (MSE:0.0013, Reg:792.0000) beta=17.75
Iter  7000 | Total loss: 660.5774 (MSE:0.0009, Reg:660.5765) beta=16.62
Iter  8000 | Total loss: 549.1758 (MSE:0.0016, Reg:549.1742) beta=15.50
Iter  9000 | Total loss: 475.8522 (MSE:0.0010, Reg:475.8512) beta=14.38
Iter 10000 | Total loss: 372.0013 (MSE:0.0013, Reg:372.0000) beta=13.25
Iter 11000 | Total loss: 279.7227 (MSE:0.0011, Reg:279.7216) beta=12.12
Iter 12000 | Total loss: 212.6692 (MSE:0.0014, Reg:212.6679) beta=11.00
Iter 13000 | Total loss: 126.9458 (MSE:0.0021, Reg:126.9437) beta=9.88
Iter 14000 | Total loss: 78.0011 (MSE:0.0012, Reg:77.9999) beta=8.75
Iter 15000 | Total loss: 36.7361 (MSE:0.0014, Reg:36.7347) beta=7.62
Iter 16000 | Total loss: 6.0004 (MSE:0.0014, Reg:5.9990) beta=6.50
Iter 17000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 51136.3047 (MSE:0.0004, Reg:51136.3047) beta=20.00
Iter  5000 | Total loss: 4453.2959 (MSE:0.0005, Reg:4453.2954) beta=18.88
Iter  6000 | Total loss: 2973.4514 (MSE:0.0005, Reg:2973.4509) beta=17.75
Iter  7000 | Total loss: 2257.5933 (MSE:0.0005, Reg:2257.5928) beta=16.62
Iter  8000 | Total loss: 1759.3079 (MSE:0.0005, Reg:1759.3074) beta=15.50
Iter  9000 | Total loss: 1311.7163 (MSE:0.0005, Reg:1311.7158) beta=14.38
Iter 10000 | Total loss: 907.0475 (MSE:0.0005, Reg:907.0471) beta=13.25
Iter 11000 | Total loss: 598.6346 (MSE:0.0007, Reg:598.6340) beta=12.12
Iter 12000 | Total loss: 367.8184 (MSE:0.0005, Reg:367.8178) beta=11.00
Iter 13000 | Total loss: 184.9987 (MSE:0.0006, Reg:184.9981) beta=9.88
Iter 14000 | Total loss: 79.7293 (MSE:0.0005, Reg:79.7288) beta=8.75
Iter 15000 | Total loss: 19.0006 (MSE:0.0006, Reg:19.0000) beta=7.62
Iter 16000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 69140.1094 (MSE:0.0022, Reg:69140.1094) beta=20.00
Iter  5000 | Total loss: 7515.1079 (MSE:0.0030, Reg:7515.1050) beta=18.88
Iter  6000 | Total loss: 5716.1104 (MSE:0.0028, Reg:5716.1074) beta=17.75
Iter  7000 | Total loss: 4721.5396 (MSE:0.0025, Reg:4721.5371) beta=16.62
Iter  8000 | Total loss: 3875.0173 (MSE:0.0025, Reg:3875.0149) beta=15.50
Iter  9000 | Total loss: 3054.0122 (MSE:0.0029, Reg:3054.0093) beta=14.38
Iter 10000 | Total loss: 2261.9070 (MSE:0.0026, Reg:2261.9043) beta=13.25
Iter 11000 | Total loss: 1494.8506 (MSE:0.0026, Reg:1494.8480) beta=12.12
Iter 12000 | Total loss: 905.8670 (MSE:0.0027, Reg:905.8643) beta=11.00
Iter 13000 | Total loss: 445.5506 (MSE:0.0027, Reg:445.5479) beta=9.88
Iter 14000 | Total loss: 189.0840 (MSE:0.0031, Reg:189.0809) beta=8.75
Iter 15000 | Total loss: 42.9680 (MSE:0.0027, Reg:42.9653) beta=7.62
Iter 16000 | Total loss: 1.0030 (MSE:0.0030, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 103702.4219 (MSE:0.0008, Reg:103702.4219) beta=20.00
Iter  5000 | Total loss: 9537.4248 (MSE:0.0007, Reg:9537.4238) beta=18.88
Iter  6000 | Total loss: 6246.4727 (MSE:0.0009, Reg:6246.4717) beta=17.75
Iter  7000 | Total loss: 4629.3618 (MSE:0.0008, Reg:4629.3608) beta=16.62
Iter  8000 | Total loss: 3591.9958 (MSE:0.0007, Reg:3591.9951) beta=15.50
Iter  9000 | Total loss: 2752.6477 (MSE:0.0007, Reg:2752.6470) beta=14.38
Iter 10000 | Total loss: 2020.7566 (MSE:0.0010, Reg:2020.7556) beta=13.25
Iter 11000 | Total loss: 1397.9899 (MSE:0.0010, Reg:1397.9889) beta=12.12
Iter 12000 | Total loss: 843.7851 (MSE:0.0008, Reg:843.7843) beta=11.00
Iter 13000 | Total loss: 455.8490 (MSE:0.0009, Reg:455.8481) beta=9.88
Iter 14000 | Total loss: 199.0582 (MSE:0.0008, Reg:199.0573) beta=8.75
Iter 15000 | Total loss: 41.5920 (MSE:0.0010, Reg:41.5910) beta=7.62
Iter 16000 | Total loss: 3.0008 (MSE:0.0008, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 199455.0156 (MSE:0.0023, Reg:199455.0156) beta=20.00
Iter  5000 | Total loss: 16576.5996 (MSE:0.0038, Reg:16576.5957) beta=18.88
Iter  6000 | Total loss: 11320.0010 (MSE:0.0039, Reg:11319.9971) beta=17.75
Iter  7000 | Total loss: 8730.2490 (MSE:0.0029, Reg:8730.2461) beta=16.62
Iter  8000 | Total loss: 6786.2437 (MSE:0.0033, Reg:6786.2402) beta=15.50
Iter  9000 | Total loss: 5178.6543 (MSE:0.0035, Reg:5178.6509) beta=14.38
Iter 10000 | Total loss: 3747.8955 (MSE:0.0031, Reg:3747.8923) beta=13.25
Iter 11000 | Total loss: 2405.1292 (MSE:0.0036, Reg:2405.1255) beta=12.12
Iter 12000 | Total loss: 1354.2938 (MSE:0.0030, Reg:1354.2908) beta=11.00
Iter 13000 | Total loss: 642.3783 (MSE:0.0032, Reg:642.3751) beta=9.88
Iter 14000 | Total loss: 244.7160 (MSE:0.0035, Reg:244.7126) beta=8.75
Iter 15000 | Total loss: 60.2599 (MSE:0.0038, Reg:60.2561) beta=7.62
Iter 16000 | Total loss: 1.1008 (MSE:0.0029, Reg:1.0979) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 20444.3672 (MSE:0.0002, Reg:20444.3672) beta=20.00
Iter  5000 | Total loss: 3087.6084 (MSE:0.0002, Reg:3087.6082) beta=18.88
Iter  6000 | Total loss: 2407.7878 (MSE:0.0003, Reg:2407.7876) beta=17.75
Iter  7000 | Total loss: 2008.0229 (MSE:0.0003, Reg:2008.0227) beta=16.62
Iter  8000 | Total loss: 1643.5903 (MSE:0.0003, Reg:1643.5901) beta=15.50
Iter  9000 | Total loss: 1352.4122 (MSE:0.0003, Reg:1352.4120) beta=14.38
Iter 10000 | Total loss: 1013.8588 (MSE:0.0003, Reg:1013.8585) beta=13.25
Iter 11000 | Total loss: 685.7405 (MSE:0.0003, Reg:685.7403) beta=12.12
Iter 12000 | Total loss: 439.6464 (MSE:0.0003, Reg:439.6461) beta=11.00
Iter 13000 | Total loss: 244.1698 (MSE:0.0003, Reg:244.1694) beta=9.88
Iter 14000 | Total loss: 119.9883 (MSE:0.0003, Reg:119.9880) beta=8.75
Iter 15000 | Total loss: 48.1113 (MSE:0.0003, Reg:48.1110) beta=7.62
Iter 16000 | Total loss: 8.2403 (MSE:0.0003, Reg:8.2400) beta=6.50
Iter 17000 | Total loss: 1.0003 (MSE:0.0003, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 161127.1719 (MSE:0.0003, Reg:161127.1719) beta=20.00
Iter  5000 | Total loss: 4815.0396 (MSE:0.0004, Reg:4815.0391) beta=18.88
Iter  6000 | Total loss: 2573.1387 (MSE:0.0004, Reg:2573.1382) beta=17.75
Iter  7000 | Total loss: 1755.5406 (MSE:0.0003, Reg:1755.5403) beta=16.62
Iter  8000 | Total loss: 1333.3014 (MSE:0.0004, Reg:1333.3010) beta=15.50
Iter  9000 | Total loss: 1006.4719 (MSE:0.0003, Reg:1006.4716) beta=14.38
Iter 10000 | Total loss: 750.2927 (MSE:0.0003, Reg:750.2924) beta=13.25
Iter 11000 | Total loss: 542.5810 (MSE:0.0004, Reg:542.5806) beta=12.12
Iter 12000 | Total loss: 385.0710 (MSE:0.0003, Reg:385.0707) beta=11.00
Iter 13000 | Total loss: 230.6308 (MSE:0.0003, Reg:230.6305) beta=9.88
Iter 14000 | Total loss: 112.4869 (MSE:0.0003, Reg:112.4865) beta=8.75
Iter 15000 | Total loss: 30.6877 (MSE:0.0003, Reg:30.6874) beta=7.62
Iter 16000 | Total loss: 1.0004 (MSE:0.0004, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 208943.7812 (MSE:0.0022, Reg:208943.7812) beta=20.00
Iter  5000 | Total loss: 15777.0547 (MSE:0.0021, Reg:15777.0527) beta=18.88
Iter  6000 | Total loss: 10426.1279 (MSE:0.0023, Reg:10426.1260) beta=17.75
Iter  7000 | Total loss: 7790.8633 (MSE:0.0027, Reg:7790.8608) beta=16.62
Iter  8000 | Total loss: 6038.8545 (MSE:0.0022, Reg:6038.8521) beta=15.50
Iter  9000 | Total loss: 4545.1265 (MSE:0.0022, Reg:4545.1240) beta=14.38
Iter 10000 | Total loss: 3303.5386 (MSE:0.0021, Reg:3303.5364) beta=13.25
Iter 11000 | Total loss: 2138.8574 (MSE:0.0022, Reg:2138.8552) beta=12.12
Iter 12000 | Total loss: 1234.6711 (MSE:0.0029, Reg:1234.6682) beta=11.00
Iter 13000 | Total loss: 574.5925 (MSE:0.0022, Reg:574.5903) beta=9.88
Iter 14000 | Total loss: 212.7218 (MSE:0.0023, Reg:212.7195) beta=8.75
Iter 15000 | Total loss: 45.5801 (MSE:0.0021, Reg:45.5779) beta=7.62
Iter 16000 | Total loss: 6.0022 (MSE:0.0022, Reg:6.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 269980.8125 (MSE:0.0004, Reg:269980.8125) beta=20.00
Iter  5000 | Total loss: 2876.1982 (MSE:0.0004, Reg:2876.1978) beta=18.88
Iter  6000 | Total loss: 1031.9520 (MSE:0.0004, Reg:1031.9517) beta=17.75
Iter  7000 | Total loss: 581.5148 (MSE:0.0004, Reg:581.5143) beta=16.62
Iter  8000 | Total loss: 404.9987 (MSE:0.0004, Reg:404.9983) beta=15.50
Iter  9000 | Total loss: 305.9933 (MSE:0.0005, Reg:305.9928) beta=14.38
Iter 10000 | Total loss: 229.6705 (MSE:0.0004, Reg:229.6700) beta=13.25
Iter 11000 | Total loss: 166.3172 (MSE:0.0004, Reg:166.3169) beta=12.12
Iter 12000 | Total loss: 110.5402 (MSE:0.0004, Reg:110.5398) beta=11.00
Iter 13000 | Total loss: 67.3847 (MSE:0.0004, Reg:67.3844) beta=9.88
Iter 14000 | Total loss: 34.0004 (MSE:0.0004, Reg:34.0000) beta=8.75
Iter 15000 | Total loss: 15.0004 (MSE:0.0004, Reg:15.0000) beta=7.62
Iter 16000 | Total loss: 4.9345 (MSE:0.0004, Reg:4.9341) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 643995.6250 (MSE:0.0065, Reg:643995.6250) beta=20.00
Iter  5000 | Total loss: 78164.0391 (MSE:0.0066, Reg:78164.0312) beta=18.88
Iter  6000 | Total loss: 53279.3594 (MSE:0.0066, Reg:53279.3516) beta=17.75
Iter  7000 | Total loss: 39729.5859 (MSE:0.0065, Reg:39729.5781) beta=16.62
Iter  8000 | Total loss: 29934.9062 (MSE:0.0069, Reg:29934.8984) beta=15.50
Iter  9000 | Total loss: 22021.0625 (MSE:0.0069, Reg:22021.0547) beta=14.38
Iter 10000 | Total loss: 15290.0693 (MSE:0.0067, Reg:15290.0625) beta=13.25
Iter 11000 | Total loss: 9556.6250 (MSE:0.0068, Reg:9556.6182) beta=12.12
Iter 12000 | Total loss: 5136.0957 (MSE:0.0072, Reg:5136.0884) beta=11.00
Iter 13000 | Total loss: 1996.0603 (MSE:0.0073, Reg:1996.0530) beta=9.88
Iter 14000 | Total loss: 509.9326 (MSE:0.0065, Reg:509.9261) beta=8.75
Iter 15000 | Total loss: 39.0045 (MSE:0.0069, Reg:38.9976) beta=7.62
Iter 16000 | Total loss: 1.0073 (MSE:0.0073, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 68523.2109 (MSE:0.0022, Reg:68523.2109) beta=20.00
Iter  5000 | Total loss: 11196.3213 (MSE:0.0028, Reg:11196.3184) beta=18.88
Iter  6000 | Total loss: 8921.7822 (MSE:0.0025, Reg:8921.7793) beta=17.75
Iter  7000 | Total loss: 7446.6157 (MSE:0.0029, Reg:7446.6128) beta=16.62
Iter  8000 | Total loss: 6185.5552 (MSE:0.0024, Reg:6185.5527) beta=15.50
Iter  9000 | Total loss: 4872.1650 (MSE:0.0031, Reg:4872.1621) beta=14.38
Iter 10000 | Total loss: 3618.4050 (MSE:0.0023, Reg:3618.4028) beta=13.25
Iter 11000 | Total loss: 2432.2581 (MSE:0.0023, Reg:2432.2556) beta=12.12
Iter 12000 | Total loss: 1445.7379 (MSE:0.0024, Reg:1445.7356) beta=11.00
Iter 13000 | Total loss: 754.6057 (MSE:0.0033, Reg:754.6024) beta=9.88
Iter 14000 | Total loss: 255.1062 (MSE:0.0023, Reg:255.1040) beta=8.75
Iter 15000 | Total loss: 43.3260 (MSE:0.0025, Reg:43.3235) beta=7.62
Iter 16000 | Total loss: 2.0027 (MSE:0.0027, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 446796.1875 (MSE:0.0005, Reg:446796.1875) beta=20.00
Iter  5000 | Total loss: 4095.3232 (MSE:0.0006, Reg:4095.3228) beta=18.88
Iter  6000 | Total loss: 861.9752 (MSE:0.0006, Reg:861.9746) beta=17.75
Iter  7000 | Total loss: 451.8554 (MSE:0.0006, Reg:451.8549) beta=16.62
Iter  8000 | Total loss: 294.3220 (MSE:0.0005, Reg:294.3214) beta=15.50
Iter  9000 | Total loss: 208.0005 (MSE:0.0006, Reg:207.9999) beta=14.38
Iter 10000 | Total loss: 154.9865 (MSE:0.0006, Reg:154.9859) beta=13.25
Iter 11000 | Total loss: 106.6107 (MSE:0.0006, Reg:106.6102) beta=12.12
Iter 12000 | Total loss: 67.0006 (MSE:0.0006, Reg:67.0000) beta=11.00
Iter 13000 | Total loss: 41.0006 (MSE:0.0006, Reg:41.0000) beta=9.88
Iter 14000 | Total loss: 23.0006 (MSE:0.0006, Reg:23.0000) beta=8.75
Iter 15000 | Total loss: 7.0005 (MSE:0.0005, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2825 (MSE:0.2825, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2507 (MSE:0.2507, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2578 (MSE:0.2578, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2412 (MSE:0.2412, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 420777.4375 (MSE:0.2372, Reg:420777.1875) beta=20.00
Iter  5000 | Total loss: 84503.5547 (MSE:0.2347, Reg:84503.3203) beta=18.88
Iter  6000 | Total loss: 56858.0820 (MSE:0.2553, Reg:56857.8281) beta=17.75
Iter  7000 | Total loss: 38880.8320 (MSE:0.2492, Reg:38880.5820) beta=16.62
Iter  8000 | Total loss: 25625.4688 (MSE:0.2390, Reg:25625.2305) beta=15.50
Iter  9000 | Total loss: 15768.0898 (MSE:0.2382, Reg:15767.8516) beta=14.38
Iter 10000 | Total loss: 8134.5659 (MSE:0.2480, Reg:8134.3179) beta=13.25
Iter 11000 | Total loss: 3489.5696 (MSE:0.2354, Reg:3489.3342) beta=12.12
Iter 12000 | Total loss: 1076.0422 (MSE:0.2552, Reg:1075.7870) beta=11.00
Iter 13000 | Total loss: 183.7000 (MSE:0.2293, Reg:183.4708) beta=9.88
Iter 14000 | Total loss: 17.2361 (MSE:0.2484, Reg:16.9876) beta=8.75
Iter 15000 | Total loss: 0.2527 (MSE:0.2527, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2349 (MSE:0.2349, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2553 (MSE:0.2553, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2510 (MSE:0.2510, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2439 (MSE:0.2439, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2411 (MSE:0.2411, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1877 (MSE:0.1877, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1040 (MSE:0.1040, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0961 (MSE:0.0961, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0983 (MSE:0.0983, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38687.7695 (MSE:0.0978, Reg:38687.6719) beta=20.00
Iter  5000 | Total loss: 7276.6826 (MSE:0.1011, Reg:7276.5815) beta=18.88
Iter  6000 | Total loss: 5417.3735 (MSE:0.1038, Reg:5417.2695) beta=17.75
Iter  7000 | Total loss: 4075.4099 (MSE:0.1195, Reg:4075.2905) beta=16.62
Iter  8000 | Total loss: 2981.8486 (MSE:0.1037, Reg:2981.7449) beta=15.50
Iter  9000 | Total loss: 1974.7872 (MSE:0.1008, Reg:1974.6864) beta=14.38
Iter 10000 | Total loss: 1196.1660 (MSE:0.0952, Reg:1196.0708) beta=13.25
Iter 11000 | Total loss: 582.9127 (MSE:0.1034, Reg:582.8092) beta=12.12
Iter 12000 | Total loss: 182.1038 (MSE:0.0992, Reg:182.0046) beta=11.00
Iter 13000 | Total loss: 50.8060 (MSE:0.1033, Reg:50.7027) beta=9.88
Iter 14000 | Total loss: 6.9748 (MSE:0.0969, Reg:6.8780) beta=8.75
Iter 15000 | Total loss: 0.1024 (MSE:0.1024, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0968 (MSE:0.0968, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1014 (MSE:0.1014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1016 (MSE:0.1016, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0965 (MSE:0.0965, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1031 (MSE:0.1031, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 69.176%
Total time: 935.76 sec


Case: [ resnet18_AdaRound_AbsMaxQuantizer_CH_W4A32_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is AbsMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1245.8418 (MSE:0.0004, Reg:1245.8413) beta=20.00
Iter  5000 | Total loss: 56.0010 (MSE:0.0010, Reg:56.0000) beta=18.88
Iter  6000 | Total loss: 38.9774 (MSE:0.0007, Reg:38.9767) beta=17.75
Iter  7000 | Total loss: 29.0009 (MSE:0.0009, Reg:29.0000) beta=16.62
Iter  8000 | Total loss: 17.0017 (MSE:0.0017, Reg:17.0000) beta=15.50
Iter  9000 | Total loss: 17.0008 (MSE:0.0008, Reg:17.0000) beta=14.38
Iter 10000 | Total loss: 12.0011 (MSE:0.0011, Reg:12.0000) beta=13.25
Iter 11000 | Total loss: 6.0011 (MSE:0.0011, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 3.0009 (MSE:0.0009, Reg:3.0000) beta=11.00
Iter 13000 | Total loss: 2.0009 (MSE:0.0009, Reg:2.0000) beta=9.88
Iter 14000 | Total loss: 1.0011 (MSE:0.0011, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5473.9849 (MSE:0.0005, Reg:5473.9844) beta=20.00
Iter  5000 | Total loss: 290.7596 (MSE:0.0007, Reg:290.7589) beta=18.88
Iter  6000 | Total loss: 190.0005 (MSE:0.0005, Reg:190.0000) beta=17.75
Iter  7000 | Total loss: 131.7013 (MSE:0.0005, Reg:131.7008) beta=16.62
Iter  8000 | Total loss: 83.0004 (MSE:0.0004, Reg:83.0000) beta=15.50
Iter  9000 | Total loss: 60.0008 (MSE:0.0008, Reg:60.0000) beta=14.38
Iter 10000 | Total loss: 44.9992 (MSE:0.0004, Reg:44.9987) beta=13.25
Iter 11000 | Total loss: 24.0005 (MSE:0.0005, Reg:24.0000) beta=12.12
Iter 12000 | Total loss: 15.0006 (MSE:0.0006, Reg:15.0000) beta=11.00
Iter 13000 | Total loss: 9.0005 (MSE:0.0005, Reg:9.0000) beta=9.88
Iter 14000 | Total loss: 3.9876 (MSE:0.0005, Reg:3.9871) beta=8.75
Iter 15000 | Total loss: 1.0004 (MSE:0.0004, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9831.8750 (MSE:0.0017, Reg:9831.8730) beta=20.00
Iter  5000 | Total loss: 760.6061 (MSE:0.0011, Reg:760.6049) beta=18.88
Iter  6000 | Total loss: 623.9311 (MSE:0.0026, Reg:623.9285) beta=17.75
Iter  7000 | Total loss: 523.9235 (MSE:0.0015, Reg:523.9220) beta=16.62
Iter  8000 | Total loss: 393.9701 (MSE:0.0016, Reg:393.9685) beta=15.50
Iter  9000 | Total loss: 303.9328 (MSE:0.0023, Reg:303.9305) beta=14.38
Iter 10000 | Total loss: 226.7890 (MSE:0.0013, Reg:226.7877) beta=13.25
Iter 11000 | Total loss: 143.8283 (MSE:0.0017, Reg:143.8266) beta=12.12
Iter 12000 | Total loss: 93.8775 (MSE:0.0022, Reg:93.8753) beta=11.00
Iter 13000 | Total loss: 51.5188 (MSE:0.0031, Reg:51.5157) beta=9.88
Iter 14000 | Total loss: 20.0019 (MSE:0.0020, Reg:19.9999) beta=8.75
Iter 15000 | Total loss: 7.1150 (MSE:0.0020, Reg:7.1129) beta=7.62
Iter 16000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10111.5898 (MSE:0.0004, Reg:10111.5898) beta=20.00
Iter  5000 | Total loss: 817.4217 (MSE:0.0005, Reg:817.4212) beta=18.88
Iter  6000 | Total loss: 547.2504 (MSE:0.0006, Reg:547.2498) beta=17.75
Iter  7000 | Total loss: 432.0004 (MSE:0.0004, Reg:432.0000) beta=16.62
Iter  8000 | Total loss: 352.0008 (MSE:0.0008, Reg:352.0000) beta=15.50
Iter  9000 | Total loss: 267.9958 (MSE:0.0006, Reg:267.9952) beta=14.38
Iter 10000 | Total loss: 183.7889 (MSE:0.0007, Reg:183.7882) beta=13.25
Iter 11000 | Total loss: 121.6872 (MSE:0.0005, Reg:121.6867) beta=12.12
Iter 12000 | Total loss: 80.4501 (MSE:0.0004, Reg:80.4496) beta=11.00
Iter 13000 | Total loss: 32.3616 (MSE:0.0005, Reg:32.3611) beta=9.88
Iter 14000 | Total loss: 10.5889 (MSE:0.0007, Reg:10.5882) beta=8.75
Iter 15000 | Total loss: 0.9840 (MSE:0.0005, Reg:0.9835) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15433.0771 (MSE:0.0032, Reg:15433.0742) beta=20.00
Iter  5000 | Total loss: 1554.6127 (MSE:0.0038, Reg:1554.6089) beta=18.88
Iter  6000 | Total loss: 1220.4635 (MSE:0.0039, Reg:1220.4596) beta=17.75
Iter  7000 | Total loss: 1038.4635 (MSE:0.0040, Reg:1038.4595) beta=16.62
Iter  8000 | Total loss: 838.4926 (MSE:0.0042, Reg:838.4885) beta=15.50
Iter  9000 | Total loss: 676.8795 (MSE:0.0037, Reg:676.8757) beta=14.38
Iter 10000 | Total loss: 461.1103 (MSE:0.0043, Reg:461.1060) beta=13.25
Iter 11000 | Total loss: 318.0039 (MSE:0.0039, Reg:318.0000) beta=12.12
Iter 12000 | Total loss: 184.2976 (MSE:0.0042, Reg:184.2933) beta=11.00
Iter 13000 | Total loss: 112.7100 (MSE:0.0040, Reg:112.7060) beta=9.88
Iter 14000 | Total loss: 44.9241 (MSE:0.0045, Reg:44.9196) beta=8.75
Iter 15000 | Total loss: 9.0043 (MSE:0.0043, Reg:9.0000) beta=7.62
Iter 16000 | Total loss: 1.0048 (MSE:0.0048, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 28108.9297 (MSE:0.0006, Reg:28108.9297) beta=20.00
Iter  5000 | Total loss: 2280.6421 (MSE:0.0007, Reg:2280.6414) beta=18.88
Iter  6000 | Total loss: 1536.9497 (MSE:0.0006, Reg:1536.9491) beta=17.75
Iter  7000 | Total loss: 1222.3766 (MSE:0.0006, Reg:1222.3760) beta=16.62
Iter  8000 | Total loss: 967.6149 (MSE:0.0007, Reg:967.6142) beta=15.50
Iter  9000 | Total loss: 732.0963 (MSE:0.0006, Reg:732.0956) beta=14.38
Iter 10000 | Total loss: 519.4260 (MSE:0.0007, Reg:519.4253) beta=13.25
Iter 11000 | Total loss: 354.1590 (MSE:0.0006, Reg:354.1584) beta=12.12
Iter 12000 | Total loss: 216.3112 (MSE:0.0007, Reg:216.3105) beta=11.00
Iter 13000 | Total loss: 143.7813 (MSE:0.0006, Reg:143.7807) beta=9.88
Iter 14000 | Total loss: 73.2874 (MSE:0.0006, Reg:73.2868) beta=8.75
Iter 15000 | Total loss: 16.8400 (MSE:0.0007, Reg:16.8394) beta=7.62
Iter 16000 | Total loss: 1.0006 (MSE:0.0006, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 72767.7812 (MSE:0.0036, Reg:72767.7812) beta=20.00
Iter  5000 | Total loss: 6262.2002 (MSE:0.0036, Reg:6262.1968) beta=18.88
Iter  6000 | Total loss: 5031.2915 (MSE:0.0036, Reg:5031.2881) beta=17.75
Iter  7000 | Total loss: 4292.7656 (MSE:0.0032, Reg:4292.7622) beta=16.62
Iter  8000 | Total loss: 3544.5461 (MSE:0.0041, Reg:3544.5420) beta=15.50
Iter  9000 | Total loss: 2668.1277 (MSE:0.0032, Reg:2668.1245) beta=14.38
Iter 10000 | Total loss: 1864.7648 (MSE:0.0045, Reg:1864.7603) beta=13.25
Iter 11000 | Total loss: 1196.6743 (MSE:0.0032, Reg:1196.6711) beta=12.12
Iter 12000 | Total loss: 744.4848 (MSE:0.0033, Reg:744.4816) beta=11.00
Iter 13000 | Total loss: 399.6965 (MSE:0.0033, Reg:399.6933) beta=9.88
Iter 14000 | Total loss: 167.9672 (MSE:0.0038, Reg:167.9634) beta=8.75
Iter 15000 | Total loss: 47.0039 (MSE:0.0040, Reg:46.9998) beta=7.62
Iter 16000 | Total loss: 2.0037 (MSE:0.0037, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5408.9985 (MSE:0.0011, Reg:5408.9976) beta=20.00
Iter  5000 | Total loss: 945.6031 (MSE:0.0013, Reg:945.6017) beta=18.88
Iter  6000 | Total loss: 836.6263 (MSE:0.0016, Reg:836.6248) beta=17.75
Iter  7000 | Total loss: 752.6840 (MSE:0.0012, Reg:752.6828) beta=16.62
Iter  8000 | Total loss: 625.2195 (MSE:0.0018, Reg:625.2177) beta=15.50
Iter  9000 | Total loss: 489.8551 (MSE:0.0013, Reg:489.8539) beta=14.38
Iter 10000 | Total loss: 392.8417 (MSE:0.0016, Reg:392.8401) beta=13.25
Iter 11000 | Total loss: 304.7005 (MSE:0.0014, Reg:304.6992) beta=12.12
Iter 12000 | Total loss: 193.2643 (MSE:0.0017, Reg:193.2626) beta=11.00
Iter 13000 | Total loss: 126.8902 (MSE:0.0024, Reg:126.8878) beta=9.88
Iter 14000 | Total loss: 78.0015 (MSE:0.0016, Reg:77.9999) beta=8.75
Iter 15000 | Total loss: 33.9949 (MSE:0.0017, Reg:33.9932) beta=7.62
Iter 16000 | Total loss: 7.0017 (MSE:0.0017, Reg:7.0000) beta=6.50
Iter 17000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 50278.8047 (MSE:0.0005, Reg:50278.8047) beta=20.00
Iter  5000 | Total loss: 4484.6304 (MSE:0.0006, Reg:4484.6299) beta=18.88
Iter  6000 | Total loss: 3009.9336 (MSE:0.0006, Reg:3009.9331) beta=17.75
Iter  7000 | Total loss: 2296.3494 (MSE:0.0006, Reg:2296.3489) beta=16.62
Iter  8000 | Total loss: 1756.4752 (MSE:0.0006, Reg:1756.4746) beta=15.50
Iter  9000 | Total loss: 1302.6071 (MSE:0.0006, Reg:1302.6064) beta=14.38
Iter 10000 | Total loss: 908.5549 (MSE:0.0006, Reg:908.5543) beta=13.25
Iter 11000 | Total loss: 582.4547 (MSE:0.0008, Reg:582.4539) beta=12.12
Iter 12000 | Total loss: 354.8996 (MSE:0.0006, Reg:354.8990) beta=11.00
Iter 13000 | Total loss: 195.9733 (MSE:0.0007, Reg:195.9727) beta=9.88
Iter 14000 | Total loss: 67.6246 (MSE:0.0006, Reg:67.6239) beta=8.75
Iter 15000 | Total loss: 14.0007 (MSE:0.0007, Reg:14.0000) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 69095.0859 (MSE:0.0027, Reg:69095.0859) beta=20.00
Iter  5000 | Total loss: 7586.1025 (MSE:0.0036, Reg:7586.0991) beta=18.88
Iter  6000 | Total loss: 5831.1333 (MSE:0.0034, Reg:5831.1299) beta=17.75
Iter  7000 | Total loss: 4874.1416 (MSE:0.0030, Reg:4874.1387) beta=16.62
Iter  8000 | Total loss: 4050.3379 (MSE:0.0031, Reg:4050.3347) beta=15.50
Iter  9000 | Total loss: 3153.0710 (MSE:0.0035, Reg:3153.0676) beta=14.38
Iter 10000 | Total loss: 2274.4905 (MSE:0.0032, Reg:2274.4873) beta=13.25
Iter 11000 | Total loss: 1526.5076 (MSE:0.0031, Reg:1526.5044) beta=12.12
Iter 12000 | Total loss: 926.7163 (MSE:0.0033, Reg:926.7131) beta=11.00
Iter 13000 | Total loss: 439.5374 (MSE:0.0033, Reg:439.5341) beta=9.88
Iter 14000 | Total loss: 171.4277 (MSE:0.0037, Reg:171.4241) beta=8.75
Iter 15000 | Total loss: 39.8870 (MSE:0.0033, Reg:39.8838) beta=7.62
Iter 16000 | Total loss: 1.0036 (MSE:0.0036, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 1.0035 (MSE:0.0035, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 103362.2422 (MSE:0.0010, Reg:103362.2422) beta=20.00
Iter  5000 | Total loss: 9901.0615 (MSE:0.0008, Reg:9901.0605) beta=18.88
Iter  6000 | Total loss: 6548.8540 (MSE:0.0011, Reg:6548.8530) beta=17.75
Iter  7000 | Total loss: 4942.7046 (MSE:0.0009, Reg:4942.7036) beta=16.62
Iter  8000 | Total loss: 3791.2441 (MSE:0.0009, Reg:3791.2432) beta=15.50
Iter  9000 | Total loss: 2904.5420 (MSE:0.0009, Reg:2904.5410) beta=14.38
Iter 10000 | Total loss: 2144.7976 (MSE:0.0012, Reg:2144.7964) beta=13.25
Iter 11000 | Total loss: 1444.8278 (MSE:0.0012, Reg:1444.8265) beta=12.12
Iter 12000 | Total loss: 857.7964 (MSE:0.0010, Reg:857.7954) beta=11.00
Iter 13000 | Total loss: 450.8773 (MSE:0.0010, Reg:450.8762) beta=9.88
Iter 14000 | Total loss: 182.8403 (MSE:0.0010, Reg:182.8393) beta=8.75
Iter 15000 | Total loss: 40.4196 (MSE:0.0011, Reg:40.4185) beta=7.62
Iter 16000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 198671.2500 (MSE:0.0028, Reg:198671.2500) beta=20.00
Iter  5000 | Total loss: 17366.8965 (MSE:0.0044, Reg:17366.8926) beta=18.88
Iter  6000 | Total loss: 11904.6904 (MSE:0.0044, Reg:11904.6855) beta=17.75
Iter  7000 | Total loss: 9196.6758 (MSE:0.0035, Reg:9196.6719) beta=16.62
Iter  8000 | Total loss: 7096.5249 (MSE:0.0039, Reg:7096.5210) beta=15.50
Iter  9000 | Total loss: 5322.2354 (MSE:0.0041, Reg:5322.2314) beta=14.38
Iter 10000 | Total loss: 3759.0356 (MSE:0.0037, Reg:3759.0320) beta=13.25
Iter 11000 | Total loss: 2445.9500 (MSE:0.0043, Reg:2445.9458) beta=12.12
Iter 12000 | Total loss: 1365.0229 (MSE:0.0036, Reg:1365.0193) beta=11.00
Iter 13000 | Total loss: 638.1565 (MSE:0.0038, Reg:638.1527) beta=9.88
Iter 14000 | Total loss: 214.0435 (MSE:0.0041, Reg:214.0394) beta=8.75
Iter 15000 | Total loss: 36.9921 (MSE:0.0044, Reg:36.9877) beta=7.62
Iter 16000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 20546.5527 (MSE:0.0003, Reg:20546.5527) beta=20.00
Iter  5000 | Total loss: 3093.4302 (MSE:0.0003, Reg:3093.4299) beta=18.88
Iter  6000 | Total loss: 2414.8643 (MSE:0.0003, Reg:2414.8640) beta=17.75
Iter  7000 | Total loss: 1987.4045 (MSE:0.0004, Reg:1987.4042) beta=16.62
Iter  8000 | Total loss: 1655.6284 (MSE:0.0003, Reg:1655.6281) beta=15.50
Iter  9000 | Total loss: 1303.7057 (MSE:0.0003, Reg:1303.7053) beta=14.38
Iter 10000 | Total loss: 992.1780 (MSE:0.0003, Reg:992.1777) beta=13.25
Iter 11000 | Total loss: 746.5649 (MSE:0.0003, Reg:746.5646) beta=12.12
Iter 12000 | Total loss: 506.1599 (MSE:0.0003, Reg:506.1595) beta=11.00
Iter 13000 | Total loss: 290.3330 (MSE:0.0004, Reg:290.3326) beta=9.88
Iter 14000 | Total loss: 153.9813 (MSE:0.0003, Reg:153.9809) beta=8.75
Iter 15000 | Total loss: 52.9596 (MSE:0.0004, Reg:52.9592) beta=7.62
Iter 16000 | Total loss: 13.0003 (MSE:0.0003, Reg:13.0000) beta=6.50
Iter 17000 | Total loss: 1.0003 (MSE:0.0003, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 159323.1094 (MSE:0.0003, Reg:159323.1094) beta=20.00
Iter  5000 | Total loss: 5767.7427 (MSE:0.0004, Reg:5767.7422) beta=18.88
Iter  6000 | Total loss: 3176.8125 (MSE:0.0004, Reg:3176.8120) beta=17.75
Iter  7000 | Total loss: 2150.7798 (MSE:0.0004, Reg:2150.7793) beta=16.62
Iter  8000 | Total loss: 1564.3490 (MSE:0.0004, Reg:1564.3486) beta=15.50
Iter  9000 | Total loss: 1208.3239 (MSE:0.0004, Reg:1208.3235) beta=14.38
Iter 10000 | Total loss: 907.8982 (MSE:0.0004, Reg:907.8978) beta=13.25
Iter 11000 | Total loss: 661.0897 (MSE:0.0004, Reg:661.0892) beta=12.12
Iter 12000 | Total loss: 435.3681 (MSE:0.0004, Reg:435.3677) beta=11.00
Iter 13000 | Total loss: 264.0681 (MSE:0.0003, Reg:264.0677) beta=9.88
Iter 14000 | Total loss: 113.4079 (MSE:0.0004, Reg:113.4075) beta=8.75
Iter 15000 | Total loss: 29.9717 (MSE:0.0004, Reg:29.9713) beta=7.62
Iter 16000 | Total loss: 2.0004 (MSE:0.0004, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 207064.4531 (MSE:0.0026, Reg:207064.4531) beta=20.00
Iter  5000 | Total loss: 16240.7588 (MSE:0.0025, Reg:16240.7559) beta=18.88
Iter  6000 | Total loss: 10657.4482 (MSE:0.0027, Reg:10657.4453) beta=17.75
Iter  7000 | Total loss: 8086.7886 (MSE:0.0031, Reg:8086.7856) beta=16.62
Iter  8000 | Total loss: 6331.9771 (MSE:0.0027, Reg:6331.9746) beta=15.50
Iter  9000 | Total loss: 4794.3555 (MSE:0.0027, Reg:4794.3525) beta=14.38
Iter 10000 | Total loss: 3439.4600 (MSE:0.0025, Reg:3439.4575) beta=13.25
Iter 11000 | Total loss: 2274.2688 (MSE:0.0026, Reg:2274.2661) beta=12.12
Iter 12000 | Total loss: 1352.1835 (MSE:0.0034, Reg:1352.1802) beta=11.00
Iter 13000 | Total loss: 667.8857 (MSE:0.0026, Reg:667.8831) beta=9.88
Iter 14000 | Total loss: 246.3392 (MSE:0.0027, Reg:246.3365) beta=8.75
Iter 15000 | Total loss: 52.1043 (MSE:0.0026, Reg:52.1017) beta=7.62
Iter 16000 | Total loss: 6.0025 (MSE:0.0026, Reg:5.9999) beta=6.50
Iter 17000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 274136.4688 (MSE:0.0004, Reg:274136.4688) beta=20.00
Iter  5000 | Total loss: 4639.6616 (MSE:0.0004, Reg:4639.6611) beta=18.88
Iter  6000 | Total loss: 1845.2705 (MSE:0.0005, Reg:1845.2700) beta=17.75
Iter  7000 | Total loss: 1143.1265 (MSE:0.0005, Reg:1143.1260) beta=16.62
Iter  8000 | Total loss: 827.0002 (MSE:0.0005, Reg:826.9998) beta=15.50
Iter  9000 | Total loss: 587.9692 (MSE:0.0005, Reg:587.9688) beta=14.38
Iter 10000 | Total loss: 428.8151 (MSE:0.0005, Reg:428.8146) beta=13.25
Iter 11000 | Total loss: 303.6464 (MSE:0.0004, Reg:303.6459) beta=12.12
Iter 12000 | Total loss: 200.7832 (MSE:0.0004, Reg:200.7827) beta=11.00
Iter 13000 | Total loss: 125.3370 (MSE:0.0004, Reg:125.3366) beta=9.88
Iter 14000 | Total loss: 65.5451 (MSE:0.0005, Reg:65.5446) beta=8.75
Iter 15000 | Total loss: 16.0005 (MSE:0.0005, Reg:16.0000) beta=7.62
Iter 16000 | Total loss: 1.7441 (MSE:0.0004, Reg:1.7436) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0111 (MSE:0.0111, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 635755.7500 (MSE:0.0075, Reg:635755.7500) beta=20.00
Iter  5000 | Total loss: 80209.1328 (MSE:0.0077, Reg:80209.1250) beta=18.88
Iter  6000 | Total loss: 54615.4961 (MSE:0.0078, Reg:54615.4883) beta=17.75
Iter  7000 | Total loss: 40635.1914 (MSE:0.0078, Reg:40635.1836) beta=16.62
Iter  8000 | Total loss: 30583.5215 (MSE:0.0082, Reg:30583.5137) beta=15.50
Iter  9000 | Total loss: 22259.4902 (MSE:0.0081, Reg:22259.4824) beta=14.38
Iter 10000 | Total loss: 15065.4043 (MSE:0.0079, Reg:15065.3965) beta=13.25
Iter 11000 | Total loss: 9052.7285 (MSE:0.0081, Reg:9052.7207) beta=12.12
Iter 12000 | Total loss: 4645.6890 (MSE:0.0084, Reg:4645.6807) beta=11.00
Iter 13000 | Total loss: 1767.7202 (MSE:0.0086, Reg:1767.7117) beta=9.88
Iter 14000 | Total loss: 406.7942 (MSE:0.0079, Reg:406.7863) beta=8.75
Iter 15000 | Total loss: 27.0939 (MSE:0.0081, Reg:27.0858) beta=7.62
Iter 16000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 68943.7500 (MSE:0.0026, Reg:68943.7500) beta=20.00
Iter  5000 | Total loss: 11174.0479 (MSE:0.0032, Reg:11174.0449) beta=18.88
Iter  6000 | Total loss: 8951.8066 (MSE:0.0029, Reg:8951.8037) beta=17.75
Iter  7000 | Total loss: 7516.4336 (MSE:0.0034, Reg:7516.4302) beta=16.62
Iter  8000 | Total loss: 6119.1104 (MSE:0.0028, Reg:6119.1074) beta=15.50
Iter  9000 | Total loss: 4771.6958 (MSE:0.0036, Reg:4771.6924) beta=14.38
Iter 10000 | Total loss: 3501.4341 (MSE:0.0027, Reg:3501.4314) beta=13.25
Iter 11000 | Total loss: 2357.5186 (MSE:0.0028, Reg:2357.5159) beta=12.12
Iter 12000 | Total loss: 1369.6488 (MSE:0.0028, Reg:1369.6460) beta=11.00
Iter 13000 | Total loss: 638.2361 (MSE:0.0038, Reg:638.2323) beta=9.88
Iter 14000 | Total loss: 218.9324 (MSE:0.0027, Reg:218.9296) beta=8.75
Iter 15000 | Total loss: 34.1577 (MSE:0.0030, Reg:34.1547) beta=7.62
Iter 16000 | Total loss: 1.5901 (MSE:0.0032, Reg:1.5869) beta=6.50
Iter 17000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 460556.0938 (MSE:0.0006, Reg:460556.0938) beta=20.00
Iter  5000 | Total loss: 7078.3140 (MSE:0.0007, Reg:7078.3135) beta=18.88
Iter  6000 | Total loss: 2117.0278 (MSE:0.0006, Reg:2117.0271) beta=17.75
Iter  7000 | Total loss: 1177.7206 (MSE:0.0007, Reg:1177.7200) beta=16.62
Iter  8000 | Total loss: 831.3381 (MSE:0.0006, Reg:831.3375) beta=15.50
Iter  9000 | Total loss: 589.0001 (MSE:0.0007, Reg:588.9994) beta=14.38
Iter 10000 | Total loss: 429.7559 (MSE:0.0007, Reg:429.7552) beta=13.25
Iter 11000 | Total loss: 278.2265 (MSE:0.0006, Reg:278.2259) beta=12.12
Iter 12000 | Total loss: 188.1448 (MSE:0.0007, Reg:188.1441) beta=11.00
Iter 13000 | Total loss: 100.4557 (MSE:0.0007, Reg:100.4550) beta=9.88
Iter 14000 | Total loss: 38.9950 (MSE:0.0006, Reg:38.9944) beta=8.75
Iter 15000 | Total loss: 10.0006 (MSE:0.0006, Reg:10.0000) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3488 (MSE:0.3488, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2860 (MSE:0.2860, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2924 (MSE:0.2924, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2752 (MSE:0.2752, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 439458.9688 (MSE:0.2711, Reg:439458.6875) beta=20.00
Iter  5000 | Total loss: 90161.2891 (MSE:0.2747, Reg:90161.0156) beta=18.88
Iter  6000 | Total loss: 61921.6680 (MSE:0.2926, Reg:61921.3750) beta=17.75
Iter  7000 | Total loss: 43117.8477 (MSE:0.2860, Reg:43117.5625) beta=16.62
Iter  8000 | Total loss: 29139.9785 (MSE:0.2758, Reg:29139.7031) beta=15.50
Iter  9000 | Total loss: 18560.0664 (MSE:0.2765, Reg:18559.7891) beta=14.38
Iter 10000 | Total loss: 10134.4590 (MSE:0.2874, Reg:10134.1719) beta=13.25
Iter 11000 | Total loss: 4591.9736 (MSE:0.2750, Reg:4591.6987) beta=12.12
Iter 12000 | Total loss: 1514.7644 (MSE:0.2937, Reg:1514.4707) beta=11.00
Iter 13000 | Total loss: 287.9256 (MSE:0.2728, Reg:287.6528) beta=9.88
Iter 14000 | Total loss: 27.1022 (MSE:0.2924, Reg:26.8098) beta=8.75
Iter 15000 | Total loss: 0.2913 (MSE:0.2913, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2735 (MSE:0.2735, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2924 (MSE:0.2924, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2884 (MSE:0.2884, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2830 (MSE:0.2830, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2799 (MSE:0.2799, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2702 (MSE:0.2702, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1409 (MSE:0.1409, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1305 (MSE:0.1305, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1333 (MSE:0.1333, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38057.0156 (MSE:0.1403, Reg:38056.8750) beta=20.00
Iter  5000 | Total loss: 7639.2021 (MSE:0.1385, Reg:7639.0635) beta=18.88
Iter  6000 | Total loss: 5811.6255 (MSE:0.1451, Reg:5811.4805) beta=17.75
Iter  7000 | Total loss: 4528.5752 (MSE:0.1524, Reg:4528.4229) beta=16.62
Iter  8000 | Total loss: 3385.5471 (MSE:0.1409, Reg:3385.4062) beta=15.50
Iter  9000 | Total loss: 2354.5825 (MSE:0.1422, Reg:2354.4404) beta=14.38
Iter 10000 | Total loss: 1516.7219 (MSE:0.1362, Reg:1516.5857) beta=13.25
Iter 11000 | Total loss: 831.5244 (MSE:0.1431, Reg:831.3812) beta=12.12
Iter 12000 | Total loss: 356.8407 (MSE:0.1393, Reg:356.7014) beta=11.00
Iter 13000 | Total loss: 91.9897 (MSE:0.1423, Reg:91.8474) beta=9.88
Iter 14000 | Total loss: 17.1275 (MSE:0.1275, Reg:17.0000) beta=8.75
Iter 15000 | Total loss: 2.1436 (MSE:0.1436, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.1319 (MSE:0.1319, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1436 (MSE:0.1436, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1425 (MSE:0.1425, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1384 (MSE:0.1384, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1475 (MSE:0.1475, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 68.992%
Total time: 935.65 sec


Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A8_BNFold_AdaRoundLR0.001 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT8
    - per_channel: False

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1822.6110 (MSE:0.0003, Reg:1822.6106) beta=20.00
Iter  5000 | Total loss: 7.8750 (MSE:0.0027, Reg:7.8723) beta=18.88
Iter  6000 | Total loss: 5.0029 (MSE:0.0029, Reg:5.0000) beta=17.75
Iter  7000 | Total loss: 4.0031 (MSE:0.0031, Reg:4.0000) beta=16.62
Iter  8000 | Total loss: 4.0027 (MSE:0.0027, Reg:4.0000) beta=15.50
Iter  9000 | Total loss: 1.0007 (MSE:0.0024, Reg:0.9983) beta=14.38
Iter 10000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6004.0132 (MSE:0.0004, Reg:6004.0127) beta=20.00
Iter  5000 | Total loss: 322.8328 (MSE:0.0017, Reg:322.8311) beta=18.88
Iter  6000 | Total loss: 181.1573 (MSE:0.0017, Reg:181.1556) beta=17.75
Iter  7000 | Total loss: 120.7097 (MSE:0.0015, Reg:120.7082) beta=16.62
Iter  8000 | Total loss: 82.8636 (MSE:0.0016, Reg:82.8620) beta=15.50
Iter  9000 | Total loss: 53.2815 (MSE:0.0016, Reg:53.2799) beta=14.38
Iter 10000 | Total loss: 35.4632 (MSE:0.0016, Reg:35.4615) beta=13.25
Iter 11000 | Total loss: 16.9988 (MSE:0.0018, Reg:16.9970) beta=12.12
Iter 12000 | Total loss: 10.9751 (MSE:0.0016, Reg:10.9735) beta=11.00
Iter 13000 | Total loss: 6.2858 (MSE:0.0017, Reg:6.2841) beta=9.88
Iter 14000 | Total loss: 4.0016 (MSE:0.0016, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 2.0016 (MSE:0.0016, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 1.0016 (MSE:0.0016, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8081.3359 (MSE:0.0029, Reg:8081.3330) beta=20.00
Iter  5000 | Total loss: 1100.2816 (MSE:0.0040, Reg:1100.2776) beta=18.88
Iter  6000 | Total loss: 726.0797 (MSE:0.0034, Reg:726.0762) beta=17.75
Iter  7000 | Total loss: 520.0817 (MSE:0.0038, Reg:520.0779) beta=16.62
Iter  8000 | Total loss: 383.7073 (MSE:0.0036, Reg:383.7037) beta=15.50
Iter  9000 | Total loss: 283.3464 (MSE:0.0038, Reg:283.3427) beta=14.38
Iter 10000 | Total loss: 197.0208 (MSE:0.0038, Reg:197.0170) beta=13.25
Iter 11000 | Total loss: 134.1068 (MSE:0.0035, Reg:134.1032) beta=12.12
Iter 12000 | Total loss: 85.5196 (MSE:0.0035, Reg:85.5160) beta=11.00
Iter 13000 | Total loss: 46.7637 (MSE:0.0038, Reg:46.7599) beta=9.88
Iter 14000 | Total loss: 25.7963 (MSE:0.0038, Reg:25.7925) beta=8.75
Iter 15000 | Total loss: 16.3312 (MSE:0.0037, Reg:16.3275) beta=7.62
Iter 16000 | Total loss: 2.5888 (MSE:0.0037, Reg:2.5851) beta=6.50
Iter 17000 | Total loss: 1.0035 (MSE:0.0035, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5225.3252 (MSE:0.0010, Reg:5225.3242) beta=20.00
Iter  5000 | Total loss: 384.2438 (MSE:0.0018, Reg:384.2420) beta=18.88
Iter  6000 | Total loss: 212.3711 (MSE:0.0018, Reg:212.3694) beta=17.75
Iter  7000 | Total loss: 154.3986 (MSE:0.0017, Reg:154.3969) beta=16.62
Iter  8000 | Total loss: 106.4295 (MSE:0.0017, Reg:106.4278) beta=15.50
Iter  9000 | Total loss: 59.0120 (MSE:0.0016, Reg:59.0104) beta=14.38
Iter 10000 | Total loss: 42.0548 (MSE:0.0017, Reg:42.0531) beta=13.25
Iter 11000 | Total loss: 31.2658 (MSE:0.0017, Reg:31.2641) beta=12.12
Iter 12000 | Total loss: 15.7854 (MSE:0.0017, Reg:15.7837) beta=11.00
Iter 13000 | Total loss: 8.9462 (MSE:0.0018, Reg:8.9444) beta=9.88
Iter 14000 | Total loss: 4.0753 (MSE:0.0017, Reg:4.0736) beta=8.75
Iter 15000 | Total loss: 3.0017 (MSE:0.0017, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 3.0017 (MSE:0.0017, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 1.1294 (MSE:0.0017, Reg:1.1277) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0188 (MSE:0.0188, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0091 (MSE:0.0091, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0087 (MSE:0.0087, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8029.2949 (MSE:0.0092, Reg:8029.2856) beta=20.00
Iter  5000 | Total loss: 1284.3997 (MSE:0.0103, Reg:1284.3894) beta=18.88
Iter  6000 | Total loss: 887.2205 (MSE:0.0100, Reg:887.2104) beta=17.75
Iter  7000 | Total loss: 686.4601 (MSE:0.0088, Reg:686.4513) beta=16.62
Iter  8000 | Total loss: 531.4387 (MSE:0.0088, Reg:531.4299) beta=15.50
Iter  9000 | Total loss: 423.0372 (MSE:0.0092, Reg:423.0280) beta=14.38
Iter 10000 | Total loss: 334.0211 (MSE:0.0096, Reg:334.0115) beta=13.25
Iter 11000 | Total loss: 248.3427 (MSE:0.0096, Reg:248.3331) beta=12.12
Iter 12000 | Total loss: 177.7804 (MSE:0.0097, Reg:177.7707) beta=11.00
Iter 13000 | Total loss: 111.7748 (MSE:0.0096, Reg:111.7652) beta=9.88
Iter 14000 | Total loss: 66.8262 (MSE:0.0104, Reg:66.8158) beta=8.75
Iter 15000 | Total loss: 27.5074 (MSE:0.0089, Reg:27.4986) beta=7.62
Iter 16000 | Total loss: 8.1095 (MSE:0.0090, Reg:8.1005) beta=6.50
Iter 17000 | Total loss: 1.8530 (MSE:0.0098, Reg:1.8432) beta=5.38
Iter 18000 | Total loss: 0.0119 (MSE:0.0100, Reg:0.0019) beta=4.25
Iter 19000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0099 (MSE:0.0099, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 12377.9746 (MSE:0.0014, Reg:12377.9736) beta=20.00
Iter  5000 | Total loss: 661.4899 (MSE:0.0018, Reg:661.4882) beta=18.88
Iter  6000 | Total loss: 336.2318 (MSE:0.0018, Reg:336.2300) beta=17.75
Iter  7000 | Total loss: 191.6306 (MSE:0.0017, Reg:191.6289) beta=16.62
Iter  8000 | Total loss: 130.3165 (MSE:0.0018, Reg:130.3147) beta=15.50
Iter  9000 | Total loss: 88.6651 (MSE:0.0017, Reg:88.6634) beta=14.38
Iter 10000 | Total loss: 61.2305 (MSE:0.0017, Reg:61.2288) beta=13.25
Iter 11000 | Total loss: 38.5057 (MSE:0.0017, Reg:38.5039) beta=12.12
Iter 12000 | Total loss: 24.3159 (MSE:0.0018, Reg:24.3141) beta=11.00
Iter 13000 | Total loss: 9.0018 (MSE:0.0018, Reg:9.0000) beta=9.88
Iter 14000 | Total loss: 4.5015 (MSE:0.0017, Reg:4.4998) beta=8.75
Iter 15000 | Total loss: 4.0018 (MSE:0.0018, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 3.0018 (MSE:0.0018, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.6869 (MSE:0.0018, Reg:0.6851) beta=5.38
Iter 18000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 24572.5723 (MSE:0.0062, Reg:24572.5664) beta=20.00
Iter  5000 | Total loss: 1948.5101 (MSE:0.0067, Reg:1948.5034) beta=18.88
Iter  6000 | Total loss: 1222.6161 (MSE:0.0072, Reg:1222.6089) beta=17.75
Iter  7000 | Total loss: 816.5761 (MSE:0.0071, Reg:816.5690) beta=16.62
Iter  8000 | Total loss: 630.7698 (MSE:0.0069, Reg:630.7629) beta=15.50
Iter  9000 | Total loss: 505.7844 (MSE:0.0066, Reg:505.7778) beta=14.38
Iter 10000 | Total loss: 392.8212 (MSE:0.0073, Reg:392.8138) beta=13.25
Iter 11000 | Total loss: 295.3533 (MSE:0.0066, Reg:295.3467) beta=12.12
Iter 12000 | Total loss: 205.6116 (MSE:0.0065, Reg:205.6051) beta=11.00
Iter 13000 | Total loss: 130.0435 (MSE:0.0074, Reg:130.0362) beta=9.88
Iter 14000 | Total loss: 54.0788 (MSE:0.0068, Reg:54.0720) beta=8.75
Iter 15000 | Total loss: 20.3529 (MSE:0.0067, Reg:20.3461) beta=7.62
Iter 16000 | Total loss: 3.0173 (MSE:0.0068, Reg:3.0105) beta=6.50
Iter 17000 | Total loss: 0.2295 (MSE:0.0072, Reg:0.2224) beta=5.38
Iter 18000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2499.3467 (MSE:0.0025, Reg:2499.3442) beta=20.00
Iter  5000 | Total loss: 268.4343 (MSE:0.0026, Reg:268.4317) beta=18.88
Iter  6000 | Total loss: 169.8314 (MSE:0.0029, Reg:169.8285) beta=17.75
Iter  7000 | Total loss: 136.0647 (MSE:0.0027, Reg:136.0621) beta=16.62
Iter  8000 | Total loss: 108.6744 (MSE:0.0029, Reg:108.6715) beta=15.50
Iter  9000 | Total loss: 88.8863 (MSE:0.0028, Reg:88.8836) beta=14.38
Iter 10000 | Total loss: 74.1202 (MSE:0.0031, Reg:74.1171) beta=13.25
Iter 11000 | Total loss: 54.5242 (MSE:0.0028, Reg:54.5213) beta=12.12
Iter 12000 | Total loss: 39.2794 (MSE:0.0028, Reg:39.2766) beta=11.00
Iter 13000 | Total loss: 33.2543 (MSE:0.0027, Reg:33.2515) beta=9.88
Iter 14000 | Total loss: 18.0917 (MSE:0.0029, Reg:18.0888) beta=8.75
Iter 15000 | Total loss: 6.0451 (MSE:0.0029, Reg:6.0423) beta=7.62
Iter 16000 | Total loss: 2.5601 (MSE:0.0029, Reg:2.5572) beta=6.50
Iter 17000 | Total loss: 0.3976 (MSE:0.0029, Reg:0.3948) beta=5.38
Iter 18000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 26924.0137 (MSE:0.0012, Reg:26924.0117) beta=20.00
Iter  5000 | Total loss: 494.7612 (MSE:0.0014, Reg:494.7598) beta=18.88
Iter  6000 | Total loss: 191.1486 (MSE:0.0014, Reg:191.1472) beta=17.75
Iter  7000 | Total loss: 121.4696 (MSE:0.0015, Reg:121.4681) beta=16.62
Iter  8000 | Total loss: 81.1759 (MSE:0.0014, Reg:81.1744) beta=15.50
Iter  9000 | Total loss: 65.5255 (MSE:0.0014, Reg:65.5241) beta=14.38
Iter 10000 | Total loss: 51.2941 (MSE:0.0015, Reg:51.2926) beta=13.25
Iter 11000 | Total loss: 40.7388 (MSE:0.0013, Reg:40.7374) beta=12.12
Iter 12000 | Total loss: 33.1074 (MSE:0.0014, Reg:33.1060) beta=11.00
Iter 13000 | Total loss: 25.8762 (MSE:0.0014, Reg:25.8749) beta=9.88
Iter 14000 | Total loss: 14.1091 (MSE:0.0015, Reg:14.1076) beta=8.75
Iter 15000 | Total loss: 6.9739 (MSE:0.0014, Reg:6.9725) beta=7.62
Iter 16000 | Total loss: 2.9165 (MSE:0.0014, Reg:2.9151) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 37484.9531 (MSE:0.0064, Reg:37484.9453) beta=20.00
Iter  5000 | Total loss: 3608.5347 (MSE:0.0065, Reg:3608.5281) beta=18.88
Iter  6000 | Total loss: 2040.0688 (MSE:0.0066, Reg:2040.0623) beta=17.75
Iter  7000 | Total loss: 1398.1093 (MSE:0.0067, Reg:1398.1025) beta=16.62
Iter  8000 | Total loss: 1063.1460 (MSE:0.0063, Reg:1063.1396) beta=15.50
Iter  9000 | Total loss: 847.4476 (MSE:0.0066, Reg:847.4409) beta=14.38
Iter 10000 | Total loss: 683.3831 (MSE:0.0068, Reg:683.3762) beta=13.25
Iter 11000 | Total loss: 531.3441 (MSE:0.0069, Reg:531.3372) beta=12.12
Iter 12000 | Total loss: 389.1170 (MSE:0.0068, Reg:389.1102) beta=11.00
Iter 13000 | Total loss: 250.2641 (MSE:0.0068, Reg:250.2574) beta=9.88
Iter 14000 | Total loss: 136.3917 (MSE:0.0068, Reg:136.3849) beta=8.75
Iter 15000 | Total loss: 38.2401 (MSE:0.0066, Reg:38.2336) beta=7.62
Iter 16000 | Total loss: 10.5010 (MSE:0.0064, Reg:10.4945) beta=6.50
Iter 17000 | Total loss: 0.7253 (MSE:0.0065, Reg:0.7188) beta=5.38
Iter 18000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 59875.3359 (MSE:0.0017, Reg:59875.3359) beta=20.00
Iter  5000 | Total loss: 514.3586 (MSE:0.0020, Reg:514.3566) beta=18.88
Iter  6000 | Total loss: 241.0485 (MSE:0.0020, Reg:241.0465) beta=17.75
Iter  7000 | Total loss: 154.4289 (MSE:0.0020, Reg:154.4268) beta=16.62
Iter  8000 | Total loss: 118.9707 (MSE:0.0020, Reg:118.9687) beta=15.50
Iter  9000 | Total loss: 96.2178 (MSE:0.0020, Reg:96.2158) beta=14.38
Iter 10000 | Total loss: 73.9897 (MSE:0.0019, Reg:73.9878) beta=13.25
Iter 11000 | Total loss: 57.2374 (MSE:0.0020, Reg:57.2353) beta=12.12
Iter 12000 | Total loss: 43.2033 (MSE:0.0021, Reg:43.2012) beta=11.00
Iter 13000 | Total loss: 28.3666 (MSE:0.0020, Reg:28.3646) beta=9.88
Iter 14000 | Total loss: 19.8487 (MSE:0.0019, Reg:19.8467) beta=8.75
Iter 15000 | Total loss: 8.8948 (MSE:0.0020, Reg:8.8928) beta=7.62
Iter 16000 | Total loss: 2.0020 (MSE:0.0020, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 1.0019 (MSE:0.0019, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 105671.7031 (MSE:0.0059, Reg:105671.6953) beta=20.00
Iter  5000 | Total loss: 2843.3118 (MSE:0.0067, Reg:2843.3052) beta=18.88
Iter  6000 | Total loss: 1352.2605 (MSE:0.0067, Reg:1352.2538) beta=17.75
Iter  7000 | Total loss: 841.8375 (MSE:0.0070, Reg:841.8306) beta=16.62
Iter  8000 | Total loss: 575.1944 (MSE:0.0067, Reg:575.1877) beta=15.50
Iter  9000 | Total loss: 440.8181 (MSE:0.0066, Reg:440.8114) beta=14.38
Iter 10000 | Total loss: 346.8564 (MSE:0.0068, Reg:346.8495) beta=13.25
Iter 11000 | Total loss: 253.7117 (MSE:0.0068, Reg:253.7049) beta=12.12
Iter 12000 | Total loss: 182.7858 (MSE:0.0068, Reg:182.7791) beta=11.00
Iter 13000 | Total loss: 130.8888 (MSE:0.0068, Reg:130.8821) beta=9.88
Iter 14000 | Total loss: 86.5356 (MSE:0.0066, Reg:86.5290) beta=8.75
Iter 15000 | Total loss: 41.7575 (MSE:0.0066, Reg:41.7508) beta=7.62
Iter 16000 | Total loss: 15.9459 (MSE:0.0069, Reg:15.9390) beta=6.50
Iter 17000 | Total loss: 3.2412 (MSE:0.0068, Reg:3.2344) beta=5.38
Iter 18000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 12087.4189 (MSE:0.0005, Reg:12087.4180) beta=20.00
Iter  5000 | Total loss: 256.9762 (MSE:0.0006, Reg:256.9756) beta=18.88
Iter  6000 | Total loss: 113.7398 (MSE:0.0006, Reg:113.7391) beta=17.75
Iter  7000 | Total loss: 82.9899 (MSE:0.0006, Reg:82.9893) beta=16.62
Iter  8000 | Total loss: 65.6789 (MSE:0.0006, Reg:65.6783) beta=15.50
Iter  9000 | Total loss: 48.2810 (MSE:0.0006, Reg:48.2803) beta=14.38
Iter 10000 | Total loss: 40.9995 (MSE:0.0006, Reg:40.9988) beta=13.25
Iter 11000 | Total loss: 34.2012 (MSE:0.0006, Reg:34.2006) beta=12.12
Iter 12000 | Total loss: 22.9929 (MSE:0.0006, Reg:22.9923) beta=11.00
Iter 13000 | Total loss: 19.7512 (MSE:0.0007, Reg:19.7505) beta=9.88
Iter 14000 | Total loss: 13.5007 (MSE:0.0006, Reg:13.5001) beta=8.75
Iter 15000 | Total loss: 9.9747 (MSE:0.0007, Reg:9.9740) beta=7.62
Iter 16000 | Total loss: 5.4732 (MSE:0.0006, Reg:5.4726) beta=6.50
Iter 17000 | Total loss: 1.7899 (MSE:0.0006, Reg:1.7892) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 103107.8594 (MSE:0.0006, Reg:103107.8594) beta=20.00
Iter  5000 | Total loss: 45.1273 (MSE:0.0007, Reg:45.1266) beta=18.88
Iter  6000 | Total loss: 21.0008 (MSE:0.0008, Reg:21.0000) beta=17.75
Iter  7000 | Total loss: 10.0008 (MSE:0.0008, Reg:10.0000) beta=16.62
Iter  8000 | Total loss: 5.9624 (MSE:0.0008, Reg:5.9616) beta=15.50
Iter  9000 | Total loss: 2.0008 (MSE:0.0008, Reg:2.0000) beta=14.38
Iter 10000 | Total loss: 2.0008 (MSE:0.0008, Reg:2.0000) beta=13.25
Iter 11000 | Total loss: 2.0008 (MSE:0.0008, Reg:2.0000) beta=12.12
Iter 12000 | Total loss: 2.0008 (MSE:0.0008, Reg:2.0000) beta=11.00
Iter 13000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 1.0004 (MSE:0.0008, Reg:0.9996) beta=7.62
Iter 16000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 140943.1875 (MSE:0.0052, Reg:140943.1875) beta=20.00
Iter  5000 | Total loss: 2936.1582 (MSE:0.0059, Reg:2936.1523) beta=18.88
Iter  6000 | Total loss: 1416.7404 (MSE:0.0055, Reg:1416.7349) beta=17.75
Iter  7000 | Total loss: 910.0826 (MSE:0.0055, Reg:910.0771) beta=16.62
Iter  8000 | Total loss: 660.9816 (MSE:0.0059, Reg:660.9757) beta=15.50
Iter  9000 | Total loss: 480.4885 (MSE:0.0057, Reg:480.4828) beta=14.38
Iter 10000 | Total loss: 362.0779 (MSE:0.0058, Reg:362.0720) beta=13.25
Iter 11000 | Total loss: 264.7427 (MSE:0.0056, Reg:264.7371) beta=12.12
Iter 12000 | Total loss: 196.1582 (MSE:0.0057, Reg:196.1526) beta=11.00
Iter 13000 | Total loss: 145.5659 (MSE:0.0059, Reg:145.5600) beta=9.88
Iter 14000 | Total loss: 86.6206 (MSE:0.0059, Reg:86.6147) beta=8.75
Iter 15000 | Total loss: 46.1532 (MSE:0.0059, Reg:46.1473) beta=7.62
Iter 16000 | Total loss: 12.8463 (MSE:0.0055, Reg:12.8408) beta=6.50
Iter 17000 | Total loss: 2.1200 (MSE:0.0058, Reg:2.1143) beta=5.38
Iter 18000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 189618.8438 (MSE:0.0007, Reg:189618.8438) beta=20.00
Iter  5000 | Total loss: 14.0009 (MSE:0.0009, Reg:14.0000) beta=18.88
Iter  6000 | Total loss: 6.9917 (MSE:0.0009, Reg:6.9908) beta=17.75
Iter  7000 | Total loss: 2.0009 (MSE:0.0009, Reg:2.0000) beta=16.62
Iter  8000 | Total loss: 1.0009 (MSE:0.0009, Reg:1.0000) beta=15.50
Iter  9000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=14.38
Iter 10000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=13.25
Iter 11000 | Total loss: 1.0009 (MSE:0.0009, Reg:1.0000) beta=12.12
Iter 12000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=11.00
Iter 13000 | Total loss: 1.0009 (MSE:0.0009, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0009 (MSE:0.0009, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 1.0009 (MSE:0.0009, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.9087 (MSE:0.0008, Reg:0.9079) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0143 (MSE:0.0143, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 446585.7500 (MSE:0.0131, Reg:446585.7500) beta=20.00
Iter  5000 | Total loss: 4416.8350 (MSE:0.0154, Reg:4416.8193) beta=18.88
Iter  6000 | Total loss: 2392.8635 (MSE:0.0155, Reg:2392.8479) beta=17.75
Iter  7000 | Total loss: 1582.3920 (MSE:0.0154, Reg:1582.3765) beta=16.62
Iter  8000 | Total loss: 1142.5371 (MSE:0.0176, Reg:1142.5195) beta=15.50
Iter  9000 | Total loss: 876.1401 (MSE:0.0169, Reg:876.1232) beta=14.38
Iter 10000 | Total loss: 647.9245 (MSE:0.0169, Reg:647.9076) beta=13.25
Iter 11000 | Total loss: 487.9881 (MSE:0.0151, Reg:487.9730) beta=12.12
Iter 12000 | Total loss: 353.0637 (MSE:0.0162, Reg:353.0475) beta=11.00
Iter 13000 | Total loss: 244.7203 (MSE:0.0163, Reg:244.7040) beta=9.88
Iter 14000 | Total loss: 161.5339 (MSE:0.0163, Reg:161.5176) beta=8.75
Iter 15000 | Total loss: 93.9552 (MSE:0.0165, Reg:93.9387) beta=7.62
Iter 16000 | Total loss: 45.8115 (MSE:0.0155, Reg:45.7959) beta=6.50
Iter 17000 | Total loss: 11.0511 (MSE:0.0162, Reg:11.0349) beta=5.38
Iter 18000 | Total loss: 0.5290 (MSE:0.0162, Reg:0.5128) beta=4.25
Iter 19000 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0157 (MSE:0.0157, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38496.0859 (MSE:0.0045, Reg:38496.0820) beta=20.00
Iter  5000 | Total loss: 2131.8264 (MSE:0.0050, Reg:2131.8215) beta=18.88
Iter  6000 | Total loss: 1156.0504 (MSE:0.0050, Reg:1156.0454) beta=17.75
Iter  7000 | Total loss: 826.1061 (MSE:0.0052, Reg:826.1009) beta=16.62
Iter  8000 | Total loss: 633.3495 (MSE:0.0052, Reg:633.3443) beta=15.50
Iter  9000 | Total loss: 510.7888 (MSE:0.0051, Reg:510.7838) beta=14.38
Iter 10000 | Total loss: 410.5576 (MSE:0.0050, Reg:410.5526) beta=13.25
Iter 11000 | Total loss: 332.2412 (MSE:0.0050, Reg:332.2362) beta=12.12
Iter 12000 | Total loss: 232.2470 (MSE:0.0051, Reg:232.2420) beta=11.00
Iter 13000 | Total loss: 158.2497 (MSE:0.0049, Reg:158.2448) beta=9.88
Iter 14000 | Total loss: 93.6461 (MSE:0.0050, Reg:93.6411) beta=8.75
Iter 15000 | Total loss: 40.6111 (MSE:0.0046, Reg:40.6065) beta=7.62
Iter 16000 | Total loss: 18.2172 (MSE:0.0050, Reg:18.2123) beta=6.50
Iter 17000 | Total loss: 4.2170 (MSE:0.0051, Reg:4.2119) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 261209.0938 (MSE:0.0011, Reg:261209.0938) beta=20.00
Iter  5000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.4639 (MSE:0.4639, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4598 (MSE:0.4598, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4622 (MSE:0.4622, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4318 (MSE:0.4318, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 373970.8438 (MSE:0.4646, Reg:373970.3750) beta=20.00
Iter  5000 | Total loss: 23418.5566 (MSE:0.4881, Reg:23418.0684) beta=18.88
Iter  6000 | Total loss: 13529.7578 (MSE:0.4767, Reg:13529.2812) beta=17.75
Iter  7000 | Total loss: 9371.0215 (MSE:0.4848, Reg:9370.5371) beta=16.62
Iter  8000 | Total loss: 6749.9185 (MSE:0.4763, Reg:6749.4424) beta=15.50
Iter  9000 | Total loss: 5328.1948 (MSE:0.4644, Reg:5327.7305) beta=14.38
Iter 10000 | Total loss: 4289.2090 (MSE:0.4582, Reg:4288.7510) beta=13.25
Iter 11000 | Total loss: 3471.8914 (MSE:0.4448, Reg:3471.4465) beta=12.12
Iter 12000 | Total loss: 2751.6594 (MSE:0.4681, Reg:2751.1914) beta=11.00
Iter 13000 | Total loss: 2074.2185 (MSE:0.4593, Reg:2073.7593) beta=9.88
Iter 14000 | Total loss: 1493.9180 (MSE:0.4551, Reg:1493.4629) beta=8.75
Iter 15000 | Total loss: 936.1487 (MSE:0.4821, Reg:935.6666) beta=7.62
Iter 16000 | Total loss: 500.2844 (MSE:0.4957, Reg:499.7886) beta=6.50
Iter 17000 | Total loss: 152.1020 (MSE:0.4730, Reg:151.6289) beta=5.38
Iter 18000 | Total loss: 6.9919 (MSE:0.4383, Reg:6.5537) beta=4.25
Iter 19000 | Total loss: 0.4591 (MSE:0.4591, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4631 (MSE:0.4631, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.4426 (MSE:0.4426, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3364 (MSE:0.3364, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.3449 (MSE:0.3449, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3419 (MSE:0.3419, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 91277.7188 (MSE:0.3431, Reg:91277.3750) beta=20.00
Iter  5000 | Total loss: 2324.8230 (MSE:0.3461, Reg:2324.4771) beta=18.88
Iter  6000 | Total loss: 1024.6310 (MSE:0.3475, Reg:1024.2834) beta=17.75
Iter  7000 | Total loss: 704.5026 (MSE:0.3858, Reg:704.1168) beta=16.62
Iter  8000 | Total loss: 514.3174 (MSE:0.3878, Reg:513.9297) beta=15.50
Iter  9000 | Total loss: 394.0173 (MSE:0.3616, Reg:393.6557) beta=14.38
Iter 10000 | Total loss: 312.3849 (MSE:0.3909, Reg:311.9940) beta=13.25
Iter 11000 | Total loss: 251.8121 (MSE:0.3318, Reg:251.4803) beta=12.12
Iter 12000 | Total loss: 205.1974 (MSE:0.3743, Reg:204.8230) beta=11.00
Iter 13000 | Total loss: 157.5378 (MSE:0.3826, Reg:157.1552) beta=9.88
Iter 14000 | Total loss: 114.6925 (MSE:0.3755, Reg:114.3171) beta=8.75
Iter 15000 | Total loss: 77.6668 (MSE:0.3730, Reg:77.2938) beta=7.62
Iter 16000 | Total loss: 43.2388 (MSE:0.3550, Reg:42.8838) beta=6.50
Iter 17000 | Total loss: 13.6116 (MSE:0.3962, Reg:13.2153) beta=5.38
Iter 18000 | Total loss: 1.8572 (MSE:0.3676, Reg:1.4895) beta=4.25
Iter 19000 | Total loss: 0.3558 (MSE:0.3558, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3858 (MSE:0.3858, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 67.764%
Total time: 1235.41 sec

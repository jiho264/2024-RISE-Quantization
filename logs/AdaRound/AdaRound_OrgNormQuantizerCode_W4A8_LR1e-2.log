
Case: [ resnet18_AdaRound_OrgNormQuantizerCode_CH_W4A8_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: OrgNormQuantizerCode
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: OrgNormQuantizerCode
    - dstDtype: INT8
    - per_channel: False

Replace to QuantModule
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    ReLU merged
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
    2D search with INT4
    Parent class is OrgNormQuantizerCode
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1193.0927 (MSE:0.0007, Reg:1193.0920) beta=20.00
Iter  5000 | Total loss: 61.0005 (MSE:0.0005, Reg:61.0000) beta=18.88
Iter  6000 | Total loss: 40.0011 (MSE:0.0011, Reg:40.0000) beta=17.75
Iter  7000 | Total loss: 23.0013 (MSE:0.0013, Reg:23.0000) beta=16.62
Iter  8000 | Total loss: 18.9990 (MSE:0.0007, Reg:18.9984) beta=15.50
Iter  9000 | Total loss: 17.0007 (MSE:0.0007, Reg:17.0000) beta=14.38
Iter 10000 | Total loss: 15.0006 (MSE:0.0007, Reg:14.9999) beta=13.25
Iter 11000 | Total loss: 6.0009 (MSE:0.0009, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 4.6005 (MSE:0.0007, Reg:4.5998) beta=11.00
Iter 13000 | Total loss: 3.0011 (MSE:0.0011, Reg:3.0000) beta=9.88
Iter 14000 | Total loss: 3.0006 (MSE:0.0006, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5376.0825 (MSE:0.0003, Reg:5376.0820) beta=20.00
Iter  5000 | Total loss: 338.8235 (MSE:0.0003, Reg:338.8232) beta=18.88
Iter  6000 | Total loss: 190.9981 (MSE:0.0005, Reg:190.9976) beta=17.75
Iter  7000 | Total loss: 130.9984 (MSE:0.0005, Reg:130.9979) beta=16.62
Iter  8000 | Total loss: 96.3376 (MSE:0.0003, Reg:96.3372) beta=15.50
Iter  9000 | Total loss: 58.9774 (MSE:0.0003, Reg:58.9771) beta=14.38
Iter 10000 | Total loss: 36.7091 (MSE:0.0003, Reg:36.7087) beta=13.25
Iter 11000 | Total loss: 20.5478 (MSE:0.0007, Reg:20.5471) beta=12.12
Iter 12000 | Total loss: 14.0003 (MSE:0.0003, Reg:14.0000) beta=11.00
Iter 13000 | Total loss: 10.0003 (MSE:0.0003, Reg:10.0000) beta=9.88
Iter 14000 | Total loss: 5.0007 (MSE:0.0007, Reg:5.0000) beta=8.75
Iter 15000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9923.2803 (MSE:0.0008, Reg:9923.2793) beta=20.00
Iter  5000 | Total loss: 760.5130 (MSE:0.0020, Reg:760.5110) beta=18.88
Iter  6000 | Total loss: 554.4524 (MSE:0.0014, Reg:554.4510) beta=17.75
Iter  7000 | Total loss: 425.0012 (MSE:0.0012, Reg:424.9999) beta=16.62
Iter  8000 | Total loss: 363.0011 (MSE:0.0011, Reg:363.0000) beta=15.50
Iter  9000 | Total loss: 271.4891 (MSE:0.0011, Reg:271.4880) beta=14.38
Iter 10000 | Total loss: 196.2249 (MSE:0.0019, Reg:196.2231) beta=13.25
Iter 11000 | Total loss: 138.0013 (MSE:0.0014, Reg:137.9999) beta=12.12
Iter 12000 | Total loss: 94.0011 (MSE:0.0012, Reg:94.0000) beta=11.00
Iter 13000 | Total loss: 57.9982 (MSE:0.0011, Reg:57.9971) beta=9.88
Iter 14000 | Total loss: 31.0009 (MSE:0.0012, Reg:30.9997) beta=8.75
Iter 15000 | Total loss: 14.9971 (MSE:0.0019, Reg:14.9952) beta=7.62
Iter 16000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10774.9072 (MSE:0.0006, Reg:10774.9062) beta=20.00
Iter  5000 | Total loss: 818.3892 (MSE:0.0006, Reg:818.3885) beta=18.88
Iter  6000 | Total loss: 518.7625 (MSE:0.0005, Reg:518.7620) beta=17.75
Iter  7000 | Total loss: 384.4373 (MSE:0.0005, Reg:384.4368) beta=16.62
Iter  8000 | Total loss: 300.9885 (MSE:0.0005, Reg:300.9880) beta=15.50
Iter  9000 | Total loss: 229.1543 (MSE:0.0004, Reg:229.1539) beta=14.38
Iter 10000 | Total loss: 160.1888 (MSE:0.0004, Reg:160.1884) beta=13.25
Iter 11000 | Total loss: 126.9405 (MSE:0.0004, Reg:126.9401) beta=12.12
Iter 12000 | Total loss: 76.0005 (MSE:0.0005, Reg:76.0000) beta=11.00
Iter 13000 | Total loss: 43.0005 (MSE:0.0005, Reg:43.0000) beta=9.88
Iter 14000 | Total loss: 16.7692 (MSE:0.0005, Reg:16.7687) beta=8.75
Iter 15000 | Total loss: 4.0004 (MSE:0.0004, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15499.5312 (MSE:0.0027, Reg:15499.5283) beta=20.00
Iter  5000 | Total loss: 1512.6412 (MSE:0.0040, Reg:1512.6373) beta=18.88
Iter  6000 | Total loss: 1211.6372 (MSE:0.0036, Reg:1211.6335) beta=17.75
Iter  7000 | Total loss: 1001.9319 (MSE:0.0040, Reg:1001.9279) beta=16.62
Iter  8000 | Total loss: 781.4963 (MSE:0.0036, Reg:781.4927) beta=15.50
Iter  9000 | Total loss: 612.4348 (MSE:0.0032, Reg:612.4316) beta=14.38
Iter 10000 | Total loss: 449.9034 (MSE:0.0033, Reg:449.9001) beta=13.25
Iter 11000 | Total loss: 294.2358 (MSE:0.0036, Reg:294.2322) beta=12.12
Iter 12000 | Total loss: 154.9613 (MSE:0.0036, Reg:154.9577) beta=11.00
Iter 13000 | Total loss: 73.9945 (MSE:0.0032, Reg:73.9913) beta=9.88
Iter 14000 | Total loss: 29.8321 (MSE:0.0040, Reg:29.8281) beta=8.75
Iter 15000 | Total loss: 5.0039 (MSE:0.0039, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 27894.1953 (MSE:0.0006, Reg:27894.1953) beta=20.00
Iter  5000 | Total loss: 2411.7825 (MSE:0.0005, Reg:2411.7820) beta=18.88
Iter  6000 | Total loss: 1545.4894 (MSE:0.0006, Reg:1545.4888) beta=17.75
Iter  7000 | Total loss: 1178.6692 (MSE:0.0005, Reg:1178.6687) beta=16.62
Iter  8000 | Total loss: 907.5699 (MSE:0.0006, Reg:907.5693) beta=15.50
Iter  9000 | Total loss: 691.0751 (MSE:0.0005, Reg:691.0745) beta=14.38
Iter 10000 | Total loss: 500.0793 (MSE:0.0006, Reg:500.0787) beta=13.25
Iter 11000 | Total loss: 320.7231 (MSE:0.0006, Reg:320.7226) beta=12.12
Iter 12000 | Total loss: 198.5672 (MSE:0.0007, Reg:198.5665) beta=11.00
Iter 13000 | Total loss: 112.9970 (MSE:0.0007, Reg:112.9964) beta=9.88
Iter 14000 | Total loss: 47.6940 (MSE:0.0006, Reg:47.6934) beta=8.75
Iter 15000 | Total loss: 16.0006 (MSE:0.0006, Reg:16.0000) beta=7.62
Iter 16000 | Total loss: 1.0006 (MSE:0.0006, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 71643.6250 (MSE:0.0021, Reg:71643.6250) beta=20.00
Iter  5000 | Total loss: 6265.4150 (MSE:0.0024, Reg:6265.4126) beta=18.88
Iter  6000 | Total loss: 4770.3965 (MSE:0.0030, Reg:4770.3936) beta=17.75
Iter  7000 | Total loss: 3886.8594 (MSE:0.0033, Reg:3886.8560) beta=16.62
Iter  8000 | Total loss: 3086.8491 (MSE:0.0028, Reg:3086.8462) beta=15.50
Iter  9000 | Total loss: 2335.7632 (MSE:0.0039, Reg:2335.7593) beta=14.38
Iter 10000 | Total loss: 1716.5514 (MSE:0.0037, Reg:1716.5476) beta=13.25
Iter 11000 | Total loss: 1190.1700 (MSE:0.0026, Reg:1190.1674) beta=12.12
Iter 12000 | Total loss: 741.3291 (MSE:0.0026, Reg:741.3265) beta=11.00
Iter 13000 | Total loss: 394.8813 (MSE:0.0029, Reg:394.8784) beta=9.88
Iter 14000 | Total loss: 161.8255 (MSE:0.0025, Reg:161.8230) beta=8.75
Iter 15000 | Total loss: 43.6275 (MSE:0.0023, Reg:43.6252) beta=7.62
Iter 16000 | Total loss: 3.0024 (MSE:0.0024, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5341.2305 (MSE:0.0008, Reg:5341.2295) beta=20.00
Iter  5000 | Total loss: 923.9077 (MSE:0.0012, Reg:923.9065) beta=18.88
Iter  6000 | Total loss: 824.5283 (MSE:0.0013, Reg:824.5270) beta=17.75
Iter  7000 | Total loss: 725.4128 (MSE:0.0010, Reg:725.4118) beta=16.62
Iter  8000 | Total loss: 610.0011 (MSE:0.0011, Reg:610.0000) beta=15.50
Iter  9000 | Total loss: 501.1410 (MSE:0.0011, Reg:501.1398) beta=14.38
Iter 10000 | Total loss: 399.0889 (MSE:0.0020, Reg:399.0870) beta=13.25
Iter 11000 | Total loss: 270.7422 (MSE:0.0020, Reg:270.7401) beta=12.12
Iter 12000 | Total loss: 183.3345 (MSE:0.0012, Reg:183.3334) beta=11.00
Iter 13000 | Total loss: 115.9185 (MSE:0.0012, Reg:115.9174) beta=9.88
Iter 14000 | Total loss: 85.0012 (MSE:0.0012, Reg:85.0000) beta=8.75
Iter 15000 | Total loss: 39.9856 (MSE:0.0012, Reg:39.9844) beta=7.62
Iter 16000 | Total loss: 7.0012 (MSE:0.0012, Reg:7.0000) beta=6.50
Iter 17000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 50609.4141 (MSE:0.0004, Reg:50609.4141) beta=20.00
Iter  5000 | Total loss: 4954.2651 (MSE:0.0005, Reg:4954.2646) beta=18.88
Iter  6000 | Total loss: 3218.3140 (MSE:0.0005, Reg:3218.3135) beta=17.75
Iter  7000 | Total loss: 2397.1240 (MSE:0.0005, Reg:2397.1235) beta=16.62
Iter  8000 | Total loss: 1826.2435 (MSE:0.0005, Reg:1826.2429) beta=15.50
Iter  9000 | Total loss: 1357.0867 (MSE:0.0005, Reg:1357.0862) beta=14.38
Iter 10000 | Total loss: 995.5845 (MSE:0.0005, Reg:995.5840) beta=13.25
Iter 11000 | Total loss: 649.9416 (MSE:0.0005, Reg:649.9411) beta=12.12
Iter 12000 | Total loss: 364.5756 (MSE:0.0005, Reg:364.5750) beta=11.00
Iter 13000 | Total loss: 187.4385 (MSE:0.0005, Reg:187.4380) beta=9.88
Iter 14000 | Total loss: 86.9948 (MSE:0.0005, Reg:86.9944) beta=8.75
Iter 15000 | Total loss: 19.8708 (MSE:0.0005, Reg:19.8703) beta=7.62
Iter 16000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 67253.7812 (MSE:0.0021, Reg:67253.7812) beta=20.00
Iter  5000 | Total loss: 7604.4414 (MSE:0.0022, Reg:7604.4390) beta=18.88
Iter  6000 | Total loss: 5782.6431 (MSE:0.0023, Reg:5782.6406) beta=17.75
Iter  7000 | Total loss: 4720.4116 (MSE:0.0023, Reg:4720.4092) beta=16.62
Iter  8000 | Total loss: 3819.1104 (MSE:0.0023, Reg:3819.1079) beta=15.50
Iter  9000 | Total loss: 3054.0725 (MSE:0.0025, Reg:3054.0701) beta=14.38
Iter 10000 | Total loss: 2261.5759 (MSE:0.0023, Reg:2261.5737) beta=13.25
Iter 11000 | Total loss: 1544.9840 (MSE:0.0026, Reg:1544.9814) beta=12.12
Iter 12000 | Total loss: 906.1774 (MSE:0.0023, Reg:906.1751) beta=11.00
Iter 13000 | Total loss: 478.6507 (MSE:0.0024, Reg:478.6483) beta=9.88
Iter 14000 | Total loss: 212.2077 (MSE:0.0024, Reg:212.2053) beta=8.75
Iter 15000 | Total loss: 58.8742 (MSE:0.0026, Reg:58.8716) beta=7.62
Iter 16000 | Total loss: 5.9991 (MSE:0.0027, Reg:5.9964) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 103484.1562 (MSE:0.0008, Reg:103484.1562) beta=20.00
Iter  5000 | Total loss: 9679.8916 (MSE:0.0007, Reg:9679.8906) beta=18.88
Iter  6000 | Total loss: 6232.9189 (MSE:0.0008, Reg:6232.9180) beta=17.75
Iter  7000 | Total loss: 4562.6558 (MSE:0.0010, Reg:4562.6548) beta=16.62
Iter  8000 | Total loss: 3497.3494 (MSE:0.0007, Reg:3497.3486) beta=15.50
Iter  9000 | Total loss: 2715.5298 (MSE:0.0008, Reg:2715.5291) beta=14.38
Iter 10000 | Total loss: 2017.0214 (MSE:0.0008, Reg:2017.0206) beta=13.25
Iter 11000 | Total loss: 1425.7034 (MSE:0.0007, Reg:1425.7026) beta=12.12
Iter 12000 | Total loss: 915.9739 (MSE:0.0009, Reg:915.9731) beta=11.00
Iter 13000 | Total loss: 497.2363 (MSE:0.0008, Reg:497.2355) beta=9.88
Iter 14000 | Total loss: 194.4317 (MSE:0.0007, Reg:194.4311) beta=8.75
Iter 15000 | Total loss: 38.4745 (MSE:0.0007, Reg:38.4738) beta=7.62
Iter 16000 | Total loss: 1.0007 (MSE:0.0007, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 202225.7500 (MSE:0.0024, Reg:202225.7500) beta=20.00
Iter  5000 | Total loss: 16860.2188 (MSE:0.0023, Reg:16860.2168) beta=18.88
Iter  6000 | Total loss: 11216.3008 (MSE:0.0024, Reg:11216.2988) beta=17.75
Iter  7000 | Total loss: 8448.5010 (MSE:0.0025, Reg:8448.4980) beta=16.62
Iter  8000 | Total loss: 6505.9248 (MSE:0.0024, Reg:6505.9224) beta=15.50
Iter  9000 | Total loss: 4940.7939 (MSE:0.0031, Reg:4940.7910) beta=14.38
Iter 10000 | Total loss: 3596.9939 (MSE:0.0029, Reg:3596.9910) beta=13.25
Iter 11000 | Total loss: 2377.4268 (MSE:0.0026, Reg:2377.4241) beta=12.12
Iter 12000 | Total loss: 1423.2882 (MSE:0.0027, Reg:1423.2855) beta=11.00
Iter 13000 | Total loss: 732.0707 (MSE:0.0025, Reg:732.0682) beta=9.88
Iter 14000 | Total loss: 295.9195 (MSE:0.0024, Reg:295.9171) beta=8.75
Iter 15000 | Total loss: 50.9936 (MSE:0.0026, Reg:50.9910) beta=7.62
Iter 16000 | Total loss: 5.0027 (MSE:0.0027, Reg:5.0000) beta=6.50
Iter 17000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 20426.2871 (MSE:0.0002, Reg:20426.2871) beta=20.00
Iter  5000 | Total loss: 3003.4470 (MSE:0.0002, Reg:3003.4468) beta=18.88
Iter  6000 | Total loss: 2391.8772 (MSE:0.0002, Reg:2391.8770) beta=17.75
Iter  7000 | Total loss: 2025.1118 (MSE:0.0003, Reg:2025.1116) beta=16.62
Iter  8000 | Total loss: 1690.0082 (MSE:0.0002, Reg:1690.0079) beta=15.50
Iter  9000 | Total loss: 1361.2667 (MSE:0.0003, Reg:1361.2665) beta=14.38
Iter 10000 | Total loss: 1033.6715 (MSE:0.0003, Reg:1033.6713) beta=13.25
Iter 11000 | Total loss: 722.2150 (MSE:0.0003, Reg:722.2147) beta=12.12
Iter 12000 | Total loss: 467.5465 (MSE:0.0003, Reg:467.5463) beta=11.00
Iter 13000 | Total loss: 254.7937 (MSE:0.0003, Reg:254.7934) beta=9.88
Iter 14000 | Total loss: 127.9894 (MSE:0.0003, Reg:127.9891) beta=8.75
Iter 15000 | Total loss: 66.4426 (MSE:0.0003, Reg:66.4422) beta=7.62
Iter 16000 | Total loss: 13.0003 (MSE:0.0003, Reg:13.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 149769.8125 (MSE:0.0003, Reg:149769.8125) beta=20.00
Iter  5000 | Total loss: 3223.8586 (MSE:0.0003, Reg:3223.8584) beta=18.88
Iter  6000 | Total loss: 1477.0411 (MSE:0.0003, Reg:1477.0408) beta=17.75
Iter  7000 | Total loss: 927.5582 (MSE:0.0003, Reg:927.5579) beta=16.62
Iter  8000 | Total loss: 660.6949 (MSE:0.0003, Reg:660.6946) beta=15.50
Iter  9000 | Total loss: 477.5805 (MSE:0.0003, Reg:477.5802) beta=14.38
Iter 10000 | Total loss: 354.8018 (MSE:0.0004, Reg:354.8014) beta=13.25
Iter 11000 | Total loss: 261.7277 (MSE:0.0003, Reg:261.7274) beta=12.12
Iter 12000 | Total loss: 177.8787 (MSE:0.0003, Reg:177.8784) beta=11.00
Iter 13000 | Total loss: 99.1558 (MSE:0.0003, Reg:99.1555) beta=9.88
Iter 14000 | Total loss: 47.3784 (MSE:0.0003, Reg:47.3781) beta=8.75
Iter 15000 | Total loss: 22.9032 (MSE:0.0003, Reg:22.9029) beta=7.62
Iter 16000 | Total loss: 3.0003 (MSE:0.0003, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 197398.4531 (MSE:0.0021, Reg:197398.4531) beta=20.00
Iter  5000 | Total loss: 15155.8164 (MSE:0.0019, Reg:15155.8145) beta=18.88
Iter  6000 | Total loss: 9716.2451 (MSE:0.0022, Reg:9716.2432) beta=17.75
Iter  7000 | Total loss: 7221.9580 (MSE:0.0020, Reg:7221.9561) beta=16.62
Iter  8000 | Total loss: 5477.2593 (MSE:0.0021, Reg:5477.2573) beta=15.50
Iter  9000 | Total loss: 4135.4961 (MSE:0.0019, Reg:4135.4941) beta=14.38
Iter 10000 | Total loss: 2969.6377 (MSE:0.0020, Reg:2969.6357) beta=13.25
Iter 11000 | Total loss: 2048.9712 (MSE:0.0023, Reg:2048.9688) beta=12.12
Iter 12000 | Total loss: 1209.7650 (MSE:0.0020, Reg:1209.7629) beta=11.00
Iter 13000 | Total loss: 670.4354 (MSE:0.0021, Reg:670.4333) beta=9.88
Iter 14000 | Total loss: 274.9277 (MSE:0.0022, Reg:274.9255) beta=8.75
Iter 15000 | Total loss: 54.6906 (MSE:0.0024, Reg:54.6882) beta=7.62
Iter 16000 | Total loss: 6.0022 (MSE:0.0022, Reg:6.0000) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 249574.4375 (MSE:0.0003, Reg:249574.4375) beta=20.00
Iter  5000 | Total loss: 1639.6095 (MSE:0.0004, Reg:1639.6091) beta=18.88
Iter  6000 | Total loss: 440.9368 (MSE:0.0004, Reg:440.9365) beta=17.75
Iter  7000 | Total loss: 252.8651 (MSE:0.0004, Reg:252.8647) beta=16.62
Iter  8000 | Total loss: 166.6973 (MSE:0.0004, Reg:166.6969) beta=15.50
Iter  9000 | Total loss: 111.8708 (MSE:0.0005, Reg:111.8702) beta=14.38
Iter 10000 | Total loss: 84.0004 (MSE:0.0004, Reg:84.0000) beta=13.25
Iter 11000 | Total loss: 65.0004 (MSE:0.0004, Reg:65.0000) beta=12.12
Iter 12000 | Total loss: 39.8172 (MSE:0.0004, Reg:39.8168) beta=11.00
Iter 13000 | Total loss: 21.0003 (MSE:0.0003, Reg:21.0000) beta=9.88
Iter 14000 | Total loss: 12.9859 (MSE:0.0004, Reg:12.9855) beta=8.75
Iter 15000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 613748.1875 (MSE:0.0058, Reg:613748.1875) beta=20.00
Iter  5000 | Total loss: 71880.8594 (MSE:0.0060, Reg:71880.8516) beta=18.88
Iter  6000 | Total loss: 47577.0781 (MSE:0.0053, Reg:47577.0742) beta=17.75
Iter  7000 | Total loss: 34727.3672 (MSE:0.0064, Reg:34727.3594) beta=16.62
Iter  8000 | Total loss: 25990.4375 (MSE:0.0069, Reg:25990.4297) beta=15.50
Iter  9000 | Total loss: 19323.5273 (MSE:0.0071, Reg:19323.5195) beta=14.38
Iter 10000 | Total loss: 13756.4219 (MSE:0.0056, Reg:13756.4160) beta=13.25
Iter 11000 | Total loss: 9074.8555 (MSE:0.0070, Reg:9074.8486) beta=12.12
Iter 12000 | Total loss: 5311.7222 (MSE:0.0055, Reg:5311.7168) beta=11.00
Iter 13000 | Total loss: 2471.9551 (MSE:0.0056, Reg:2471.9495) beta=9.88
Iter 14000 | Total loss: 750.4673 (MSE:0.0061, Reg:750.4612) beta=8.75
Iter 15000 | Total loss: 108.9258 (MSE:0.0060, Reg:108.9199) beta=7.62
Iter 16000 | Total loss: 3.0069 (MSE:0.0069, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 65569.9531 (MSE:0.0018, Reg:65569.9531) beta=20.00
Iter  5000 | Total loss: 11146.0742 (MSE:0.0023, Reg:11146.0723) beta=18.88
Iter  6000 | Total loss: 8971.5947 (MSE:0.0022, Reg:8971.5928) beta=17.75
Iter  7000 | Total loss: 7598.5659 (MSE:0.0029, Reg:7598.5630) beta=16.62
Iter  8000 | Total loss: 6269.3096 (MSE:0.0023, Reg:6269.3071) beta=15.50
Iter  9000 | Total loss: 4973.9619 (MSE:0.0023, Reg:4973.9595) beta=14.38
Iter 10000 | Total loss: 3651.2607 (MSE:0.0029, Reg:3651.2578) beta=13.25
Iter 11000 | Total loss: 2455.1973 (MSE:0.0023, Reg:2455.1951) beta=12.12
Iter 12000 | Total loss: 1449.2782 (MSE:0.0025, Reg:1449.2756) beta=11.00
Iter 13000 | Total loss: 736.6510 (MSE:0.0026, Reg:736.6484) beta=9.88
Iter 14000 | Total loss: 265.1786 (MSE:0.0024, Reg:265.1762) beta=8.75
Iter 15000 | Total loss: 32.2390 (MSE:0.0026, Reg:32.2364) beta=7.62
Iter 16000 | Total loss: 2.0026 (MSE:0.0026, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    1D search with UINT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 403312.2500 (MSE:0.0006, Reg:403312.2500) beta=20.00
Iter  5000 | Total loss: 1849.2698 (MSE:0.0005, Reg:1849.2693) beta=18.88
Iter  6000 | Total loss: 212.0488 (MSE:0.0006, Reg:212.0482) beta=17.75
Iter  7000 | Total loss: 88.9531 (MSE:0.0005, Reg:88.9526) beta=16.62
Iter  8000 | Total loss: 58.0005 (MSE:0.0005, Reg:57.9999) beta=15.50
Iter  9000 | Total loss: 41.0005 (MSE:0.0005, Reg:41.0000) beta=14.38
Iter 10000 | Total loss: 31.0005 (MSE:0.0005, Reg:31.0000) beta=13.25
Iter 11000 | Total loss: 23.9914 (MSE:0.0005, Reg:23.9909) beta=12.12
Iter 12000 | Total loss: 16.0005 (MSE:0.0005, Reg:16.0000) beta=11.00
Iter 13000 | Total loss: 8.1847 (MSE:0.0005, Reg:8.1842) beta=9.88
Iter 14000 | Total loss: 4.0006 (MSE:0.0006, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.2468 (MSE:0.2468, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2457 (MSE:0.2457, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2255 (MSE:0.2255, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2280 (MSE:0.2280, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 385327.6250 (MSE:0.2429, Reg:385327.3750) beta=20.00
Iter  5000 | Total loss: 75039.1719 (MSE:0.2433, Reg:75038.9297) beta=18.88
Iter  6000 | Total loss: 49391.1719 (MSE:0.2442, Reg:49390.9258) beta=17.75
Iter  7000 | Total loss: 32546.8789 (MSE:0.2686, Reg:32546.6094) beta=16.62
Iter  8000 | Total loss: 20641.7129 (MSE:0.2589, Reg:20641.4531) beta=15.50
Iter  9000 | Total loss: 12123.0596 (MSE:0.2341, Reg:12122.8252) beta=14.38
Iter 10000 | Total loss: 5990.9346 (MSE:0.2280, Reg:5990.7065) beta=13.25
Iter 11000 | Total loss: 2319.6860 (MSE:0.2363, Reg:2319.4497) beta=12.12
Iter 12000 | Total loss: 671.1565 (MSE:0.2299, Reg:670.9266) beta=11.00
Iter 13000 | Total loss: 95.2055 (MSE:0.2450, Reg:94.9605) beta=9.88
Iter 14000 | Total loss: 7.2172 (MSE:0.2172, Reg:7.0000) beta=8.75
Iter 15000 | Total loss: 0.2344 (MSE:0.2344, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2587 (MSE:0.2587, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2272 (MSE:0.2272, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2214 (MSE:0.2214, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2253 (MSE:0.2253, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2207 (MSE:0.2207, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT8
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.2257 (MSE:0.2257, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0963 (MSE:0.0963, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0956 (MSE:0.0956, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0928 (MSE:0.0928, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 39925.7031 (MSE:0.0948, Reg:39925.6094) beta=20.00
Iter  5000 | Total loss: 7319.5093 (MSE:0.0896, Reg:7319.4199) beta=18.88
Iter  6000 | Total loss: 5382.5513 (MSE:0.0890, Reg:5382.4624) beta=17.75
Iter  7000 | Total loss: 3978.4189 (MSE:0.0935, Reg:3978.3254) beta=16.62
Iter  8000 | Total loss: 2787.9167 (MSE:0.0949, Reg:2787.8218) beta=15.50
Iter  9000 | Total loss: 1771.1016 (MSE:0.0930, Reg:1771.0085) beta=14.38
Iter 10000 | Total loss: 908.6993 (MSE:0.0988, Reg:908.6006) beta=13.25
Iter 11000 | Total loss: 435.5501 (MSE:0.0870, Reg:435.4631) beta=12.12
Iter 12000 | Total loss: 157.6270 (MSE:0.0956, Reg:157.5314) beta=11.00
Iter 13000 | Total loss: 36.8214 (MSE:0.0939, Reg:36.7275) beta=9.88
Iter 14000 | Total loss: 5.0835 (MSE:0.0835, Reg:5.0000) beta=8.75
Iter 15000 | Total loss: 0.0938 (MSE:0.0938, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0978 (MSE:0.0978, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0919 (MSE:0.0919, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0920 (MSE:0.0920, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0877 (MSE:0.0877, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0930 (MSE:0.0930, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 69.222%
Total time: 2083.11 sec

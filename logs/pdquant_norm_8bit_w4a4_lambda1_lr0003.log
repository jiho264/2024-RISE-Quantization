
Case: [ resnet18_PDquant_NormQuantizer_head_stem_8bit_CH_W4A4_p2.4_RoundingLR0.0003 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.0003
    - head_stem_8bit: True

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - PDquant: True

- activation params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: False
    - p: 2.4

Replace to QuantLayer
    2D search with INT8
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT8
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantLayer: conv1, torch.Size([64, 3, 7, 7])
    QuantLayer: layer1.0.conv_bn_relu_1, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.0.conv_bn_2, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.1.conv_bn_relu_1, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.1.conv_bn_2, torch.Size([64, 64, 3, 3])
    QuantLayer: layer2.0.conv_bn_relu_1, torch.Size([128, 64, 3, 3])
    QuantLayer: layer2.0.conv_bn_2, torch.Size([128, 128, 3, 3])
    QuantLayer: layer2.0.conv_bn_down, torch.Size([128, 64, 1, 1])
    QuantLayer: layer2.1.conv_bn_relu_1, torch.Size([128, 128, 3, 3])
    QuantLayer: layer2.1.conv_bn_2, torch.Size([128, 128, 3, 3])
    QuantLayer: layer3.0.conv_bn_relu_1, torch.Size([256, 128, 3, 3])
    QuantLayer: layer3.0.conv_bn_2, torch.Size([256, 256, 3, 3])
    QuantLayer: layer3.0.conv_bn_down, torch.Size([256, 128, 1, 1])
    QuantLayer: layer3.1.conv_bn_relu_1, torch.Size([256, 256, 3, 3])
    QuantLayer: layer3.1.conv_bn_2, torch.Size([256, 256, 3, 3])
    QuantLayer: layer4.0.conv_bn_relu_1, torch.Size([512, 256, 3, 3])
    QuantLayer: layer4.0.conv_bn_2, torch.Size([512, 512, 3, 3])
    QuantLayer: layer4.0.conv_bn_down, torch.Size([512, 256, 1, 1])
    QuantLayer: layer4.1.conv_bn_relu_1, torch.Size([512, 512, 3, 3])
    QuantLayer: layer4.1.conv_bn_2, torch.Size([512, 512, 3, 3])
    QuantLayer: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
conv1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_down <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_down <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_down <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
fc <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])
0 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
0 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
0 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
0 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])

[1/21] PDquant computing: conv1
 <- Commas indicate the INT inference.
    1D search with UINT8
    p = 2.4
Activation quantizer initialized from QuantLayer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.4684 (MSE:0.0008, PD: 0.4677, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.6423 (MSE:0.0005, PD: 0.6418, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4631 (MSE:0.0004, PD: 0.4627, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.6439 (MSE:0.0009, PD: 0.6430, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 760.9775 (MSE:0.0005, PD: 0.4270, Reg:760.5499) beta=20.00
Iter  5000 | Total loss: 0.5961 (MSE:0.0003, PD: 0.5958, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.5095 (MSE:0.0009, PD: 0.5086, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.5151 (MSE:0.0011, PD: 0.5140, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.5064 (MSE:0.0005, PD: 0.5059, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.5427 (MSE:0.0005, PD: 0.5422, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.6827 (MSE:0.0005, PD: 0.6822, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.5136 (MSE:0.0007, PD: 0.5129, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.4498 (MSE:0.0004, PD: 0.4493, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.5674 (MSE:0.0009, PD: 0.5665, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.4170 (MSE:0.0004, PD: 0.4166, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.5544 (MSE:0.0002, PD: 0.5541, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.5496 (MSE:0.0021, PD: 0.5475, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.7814 (MSE:0.0008, PD: 0.7806, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.4289 (MSE:0.0005, PD: 0.4283, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.6957 (MSE:0.0001, PD: 0.6955, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4779 (MSE:0.0005, PD: 0.4774, Reg:0.0000) beta=2.00

[2/21] PDquant computing: 0
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3899 (MSE:0.0032, PD: 0.3867, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.5357 (MSE:0.0035, PD: 0.5322, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.6444 (MSE:0.0030, PD: 0.6414, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3925 (MSE:0.0033, PD: 0.3892, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5962.0630 (MSE:0.0041, PD: 0.3572, Reg:5961.7017) beta=20.00
Iter  5000 | Total loss: 0.5716 (MSE:0.0032, PD: 0.5684, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.5025 (MSE:0.0039, PD: 0.4986, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.5268 (MSE:0.0037, PD: 0.5231, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.4751 (MSE:0.0033, PD: 0.4717, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.6168 (MSE:0.0030, PD: 0.6138, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.5978 (MSE:0.0032, PD: 0.5946, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.8170 (MSE:0.0029, PD: 0.8142, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.3090 (MSE:0.0031, PD: 0.3059, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.6876 (MSE:0.0031, PD: 0.6844, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.5529 (MSE:0.0039, PD: 0.5491, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.3951 (MSE:0.0030, PD: 0.3921, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.8048 (MSE:0.0044, PD: 0.8003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.4539 (MSE:0.0031, PD: 0.4507, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.7640 (MSE:0.0033, PD: 0.7607, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.5374 (MSE:0.0032, PD: 0.5343, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.7147 (MSE:0.0028, PD: 0.7119, Reg:0.0000) beta=2.00

[3/21] PDquant computing: 1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3810 (MSE:0.0083, PD: 0.3728, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4172 (MSE:0.0084, PD: 0.4088, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.5105 (MSE:0.0083, PD: 0.5022, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.5072 (MSE:0.0091, PD: 0.4982, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6236.9814 (MSE:0.0082, PD: 0.4638, Reg:6236.5093) beta=20.00
Iter  5000 | Total loss: 0.5626 (MSE:0.0084, PD: 0.5542, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.5368 (MSE:0.0090, PD: 0.5279, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.3511 (MSE:0.0090, PD: 0.3422, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.4986 (MSE:0.0080, PD: 0.4906, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.5645 (MSE:0.0088, PD: 0.5557, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.6094 (MSE:0.0085, PD: 0.6009, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.6634 (MSE:0.0083, PD: 0.6550, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.5046 (MSE:0.0083, PD: 0.4963, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.6617 (MSE:0.0084, PD: 0.6532, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.3885 (MSE:0.0086, PD: 0.3798, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.5045 (MSE:0.0087, PD: 0.4958, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.4364 (MSE:0.0091, PD: 0.4273, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.4598 (MSE:0.0082, PD: 0.4516, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.4745 (MSE:0.0086, PD: 0.4659, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.4564 (MSE:0.0082, PD: 0.4482, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.6021 (MSE:0.0088, PD: 0.5933, Reg:0.0000) beta=2.00

[4/21] PDquant computing: 0
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.4469 (MSE:0.0040, PD: 0.4429, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.6081 (MSE:0.0041, PD: 0.6040, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4319 (MSE:0.0042, PD: 0.4277, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4753 (MSE:0.0040, PD: 0.4713, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 19206.4258 (MSE:0.0041, PD: 0.5323, Reg:19205.8887) beta=20.00
Iter  5000 | Total loss: 0.5140 (MSE:0.0042, PD: 0.5098, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.5040 (MSE:0.0038, PD: 0.5001, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.4301 (MSE:0.0041, PD: 0.4260, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.6264 (MSE:0.0043, PD: 0.6221, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.5024 (MSE:0.0039, PD: 0.4985, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.4180 (MSE:0.0042, PD: 0.4139, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.6426 (MSE:0.0041, PD: 0.6385, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.5417 (MSE:0.0047, PD: 0.5370, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.5409 (MSE:0.0040, PD: 0.5369, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.7428 (MSE:0.0040, PD: 0.7388, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.4217 (MSE:0.0044, PD: 0.4173, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.4929 (MSE:0.0042, PD: 0.4888, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.6264 (MSE:0.0038, PD: 0.6226, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.5392 (MSE:0.0041, PD: 0.5351, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.3902 (MSE:0.0042, PD: 0.3860, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.5387 (MSE:0.0040, PD: 0.5347, Reg:0.0000) beta=2.00

[5/21] PDquant computing: 1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.4488 (MSE:0.0055, PD: 0.4433, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4576 (MSE:0.0055, PD: 0.4521, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.6207 (MSE:0.0057, PD: 0.6150, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3899 (MSE:0.0058, PD: 0.3841, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 25681.5957 (MSE:0.0054, PD: 0.3828, Reg:25681.2070) beta=20.00
Iter  5000 | Total loss: 0.3859 (MSE:0.0054, PD: 0.3805, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.4389 (MSE:0.0058, PD: 0.4331, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.3993 (MSE:0.0058, PD: 0.3935, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.4963 (MSE:0.0056, PD: 0.4907, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.5307 (MSE:0.0054, PD: 0.5253, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.5305 (MSE:0.0057, PD: 0.5249, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.4404 (MSE:0.0054, PD: 0.4350, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.6000 (MSE:0.0056, PD: 0.5944, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.4947 (MSE:0.0057, PD: 0.4890, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.5648 (MSE:0.0056, PD: 0.5592, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.4225 (MSE:0.0059, PD: 0.4166, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.5389 (MSE:0.0055, PD: 0.5334, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.5155 (MSE:0.0054, PD: 0.5101, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.4966 (MSE:0.0058, PD: 0.4909, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.6265 (MSE:0.0057, PD: 0.6207, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.5923 (MSE:0.0056, PD: 0.5867, Reg:0.0000) beta=2.00

[6/21] PDquant computing: 0
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.7954 (MSE:0.0031, PD: 0.7923, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.5371 (MSE:0.0030, PD: 0.5341, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4462 (MSE:0.0030, PD: 0.4432, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.5037 (MSE:0.0030, PD: 0.5007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 87110.7500 (MSE:0.0032, PD: 0.4258, Reg:87110.3203) beta=20.00
Iter  5000 | Total loss: 0.4753 (MSE:0.0034, PD: 0.4719, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.5764 (MSE:0.0031, PD: 0.5733, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.2520 (MSE:0.0030, PD: 0.2490, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.4525 (MSE:0.0030, PD: 0.4495, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.4701 (MSE:0.0031, PD: 0.4669, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.6332 (MSE:0.0031, PD: 0.6301, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.6187 (MSE:0.0032, PD: 0.6156, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.5540 (MSE:0.0033, PD: 0.5507, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.6146 (MSE:0.0030, PD: 0.6116, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.4546 (MSE:0.0033, PD: 0.4513, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.2857 (MSE:0.0032, PD: 0.2824, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.3841 (MSE:0.0030, PD: 0.3811, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.3899 (MSE:0.0031, PD: 0.3868, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.3304 (MSE:0.0031, PD: 0.3273, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.4213 (MSE:0.0034, PD: 0.4179, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4799 (MSE:0.0032, PD: 0.4767, Reg:0.0000) beta=2.00

[7/21] PDquant computing: 1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3971 (MSE:0.0035, PD: 0.3936, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4039 (MSE:0.0039, PD: 0.4000, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.5160 (MSE:0.0036, PD: 0.5124, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4982 (MSE:0.0040, PD: 0.4942, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 99461.5391 (MSE:0.0039, PD: 0.4574, Reg:99461.0781) beta=20.00
Iter  5000 | Total loss: 0.5721 (MSE:0.0037, PD: 0.5684, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.4680 (MSE:0.0038, PD: 0.4642, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.4600 (MSE:0.0038, PD: 0.4562, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.3597 (MSE:0.0036, PD: 0.3561, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.4100 (MSE:0.0037, PD: 0.4063, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.5031 (MSE:0.0040, PD: 0.4991, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.5094 (MSE:0.0038, PD: 0.5056, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.2760 (MSE:0.0037, PD: 0.2723, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.3844 (MSE:0.0036, PD: 0.3808, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.4174 (MSE:0.0035, PD: 0.4139, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.3318 (MSE:0.0039, PD: 0.3279, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.4503 (MSE:0.0038, PD: 0.4465, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.5018 (MSE:0.0036, PD: 0.4982, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.3814 (MSE:0.0035, PD: 0.3779, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.4715 (MSE:0.0036, PD: 0.4679, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3621 (MSE:0.0040, PD: 0.3581, Reg:0.0000) beta=2.00

[8/21] PDquant computing: 0
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3584 (MSE:0.0049, PD: 0.3536, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.5461 (MSE:0.0049, PD: 0.5412, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4360 (MSE:0.0048, PD: 0.4313, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4721 (MSE:0.0047, PD: 0.4674, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 283769.4688 (MSE:0.0047, PD: 0.4180, Reg:283769.0625) beta=20.00
Iter  5000 | Total loss: 0.5609 (MSE:0.0048, PD: 0.5561, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.4911 (MSE:0.0047, PD: 0.4864, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.4518 (MSE:0.0047, PD: 0.4471, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.3354 (MSE:0.0047, PD: 0.3306, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.5253 (MSE:0.0051, PD: 0.5202, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.4788 (MSE:0.0047, PD: 0.4741, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.4753 (MSE:0.0047, PD: 0.4706, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.4608 (MSE:0.0046, PD: 0.4562, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.4896 (MSE:0.0047, PD: 0.4849, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.6150 (MSE:0.0051, PD: 0.6099, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.5169 (MSE:0.0047, PD: 0.5122, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.4523 (MSE:0.0045, PD: 0.4479, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.4702 (MSE:0.0049, PD: 0.4652, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.4574 (MSE:0.0049, PD: 0.4525, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.6696 (MSE:0.0053, PD: 0.6644, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.5879 (MSE:0.0049, PD: 0.5830, Reg:0.0000) beta=2.00

[9/21] PDquant computing: 1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.7873 (MSE:0.3595, PD: 0.4277, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.8235 (MSE:0.3550, PD: 0.4685, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.6953 (MSE:0.3891, PD: 0.3062, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 1.0633 (MSE:0.3759, PD: 0.6873, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 416421.8750 (MSE:0.3516, PD: 0.4195, Reg:416421.1250) beta=20.00
Iter  5000 | Total loss: 0.7656 (MSE:0.3789, PD: 0.3866, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.7410 (MSE:0.3659, PD: 0.3751, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.6931 (MSE:0.3602, PD: 0.3328, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.7935 (MSE:0.3455, PD: 0.4480, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.7304 (MSE:0.3467, PD: 0.3837, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.7205 (MSE:0.3597, PD: 0.3608, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.8937 (MSE:0.3668, PD: 0.5268, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.7344 (MSE:0.3495, PD: 0.3849, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.8452 (MSE:0.3681, PD: 0.4772, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.8872 (MSE:0.3479, PD: 0.5393, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.7738 (MSE:0.3497, PD: 0.4241, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.6834 (MSE:0.3533, PD: 0.3301, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.7807 (MSE:0.3579, PD: 0.4227, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.9554 (MSE:0.3588, PD: 0.5966, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.8223 (MSE:0.3840, PD: 0.4384, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.8537 (MSE:0.3665, PD: 0.4871, Reg:0.0000) beta=2.00

[10/21] PDquant computing: fc
 <- Commas indicate the INT inference.
    2D search with INT8
    p = 2.4
Activation quantizer initialized from QuantLayer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 1.6821 (MSE:1.2537, PD: 0.4284, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 1.8506 (MSE:1.1659, PD: 0.6846, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 1.6519 (MSE:1.0884, PD: 0.5635, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 1.3902 (MSE:1.0719, PD: 0.3182, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 178827.8594 (MSE:1.0923, PD: 0.4416, Reg:178826.3281) beta=20.00
Iter  5000 | Total loss: 9540.4238 (MSE:1.0065, PD: 0.4427, Reg:9538.9746) beta=18.88
Iter  6000 | Total loss: 1.3961 (MSE:0.9961, PD: 0.4000, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 1.3353 (MSE:0.9901, PD: 0.3453, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 1.5230 (MSE:1.0919, PD: 0.4311, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 1.6794 (MSE:1.1878, PD: 0.4917, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 1.4136 (MSE:1.0622, PD: 0.3513, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 1.3510 (MSE:1.0429, PD: 0.3081, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 1.5309 (MSE:1.0478, PD: 0.4831, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 1.4098 (MSE:1.0423, PD: 0.3674, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 1.3883 (MSE:1.0324, PD: 0.3559, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 1.6687 (MSE:1.1744, PD: 0.4942, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 1.4279 (MSE:1.0224, PD: 0.4055, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 1.5245 (MSE:1.0421, PD: 0.4824, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 1.5520 (MSE:1.0107, PD: 0.5413, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 1.4826 (MSE:0.9599, PD: 0.5226, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 1.7714 (MSE:1.1282, PD: 0.6432, Reg:0.0000) beta=2.00
PDQuant computing done!

    Quantized model Evaluation accuracy on 50000 images, 67.944%
Total time: 6618.08 sec

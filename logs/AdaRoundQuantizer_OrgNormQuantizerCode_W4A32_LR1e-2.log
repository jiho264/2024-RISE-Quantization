
Case: [ resnet18_AdaRound_OrgNormQuantizerCode_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: OrgNormQuantizerCode
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Parent class is OrgNormQuantizerCode
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 983.6678 (MSE:0.0002, Reg:983.6676) beta=20.00
Iter  5000 | Total loss: 41.0228 (MSE:0.0002, Reg:41.0225) beta=18.88
Iter  6000 | Total loss: 20.9130 (MSE:0.0002, Reg:20.9128) beta=17.75
Iter  7000 | Total loss: 17.0002 (MSE:0.0002, Reg:17.0000) beta=16.62
Iter  8000 | Total loss: 11.0003 (MSE:0.0003, Reg:11.0000) beta=15.50
Iter  9000 | Total loss: 10.0002 (MSE:0.0002, Reg:10.0000) beta=14.38
Iter 10000 | Total loss: 7.0003 (MSE:0.0003, Reg:7.0000) beta=13.25
Iter 11000 | Total loss: 6.0003 (MSE:0.0003, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 5.0003 (MSE:0.0003, Reg:5.0000) beta=11.00
Iter 13000 | Total loss: 4.0003 (MSE:0.0003, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 1.0002 (MSE:0.0002, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 1.0002 (MSE:0.0002, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2000.0110 (MSE:0.0005, Reg:2000.0105) beta=20.00
Iter  5000 | Total loss: 112.9870 (MSE:0.0004, Reg:112.9865) beta=18.88
Iter  6000 | Total loss: 85.7294 (MSE:0.0005, Reg:85.7289) beta=17.75
Iter  7000 | Total loss: 63.0006 (MSE:0.0006, Reg:63.0000) beta=16.62
Iter  8000 | Total loss: 50.9047 (MSE:0.0005, Reg:50.9041) beta=15.50
Iter  9000 | Total loss: 43.0005 (MSE:0.0005, Reg:43.0000) beta=14.38
Iter 10000 | Total loss: 27.0004 (MSE:0.0004, Reg:27.0000) beta=13.25
Iter 11000 | Total loss: 14.0005 (MSE:0.0005, Reg:14.0000) beta=12.12
Iter 12000 | Total loss: 7.0005 (MSE:0.0005, Reg:7.0000) beta=11.00
Iter 13000 | Total loss: 7.0005 (MSE:0.0005, Reg:7.0000) beta=9.88
Iter 14000 | Total loss: 3.0004 (MSE:0.0004, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2877.9158 (MSE:0.0011, Reg:2877.9146) beta=20.00
Iter  5000 | Total loss: 308.9428 (MSE:0.0010, Reg:308.9419) beta=18.88
Iter  6000 | Total loss: 231.8413 (MSE:0.0010, Reg:231.8404) beta=17.75
Iter  7000 | Total loss: 171.0010 (MSE:0.0011, Reg:171.0000) beta=16.62
Iter  8000 | Total loss: 135.9786 (MSE:0.0011, Reg:135.9775) beta=15.50
Iter  9000 | Total loss: 86.0011 (MSE:0.0011, Reg:86.0000) beta=14.38
Iter 10000 | Total loss: 53.0011 (MSE:0.0011, Reg:53.0000) beta=13.25
Iter 11000 | Total loss: 30.9509 (MSE:0.0009, Reg:30.9500) beta=12.12
Iter 12000 | Total loss: 21.0011 (MSE:0.0011, Reg:21.0000) beta=11.00
Iter 13000 | Total loss: 10.0008 (MSE:0.0009, Reg:9.9999) beta=9.88
Iter 14000 | Total loss: 3.0010 (MSE:0.0010, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2521.5459 (MSE:0.0012, Reg:2521.5447) beta=20.00
Iter  5000 | Total loss: 226.0011 (MSE:0.0012, Reg:225.9999) beta=18.88
Iter  6000 | Total loss: 171.0014 (MSE:0.0015, Reg:171.0000) beta=17.75
Iter  7000 | Total loss: 137.9697 (MSE:0.0011, Reg:137.9686) beta=16.62
Iter  8000 | Total loss: 100.0012 (MSE:0.0012, Reg:100.0000) beta=15.50
Iter  9000 | Total loss: 84.9692 (MSE:0.0010, Reg:84.9682) beta=14.38
Iter 10000 | Total loss: 53.0013 (MSE:0.0013, Reg:53.0000) beta=13.25
Iter 11000 | Total loss: 26.0012 (MSE:0.0012, Reg:26.0000) beta=12.12
Iter 12000 | Total loss: 15.0010 (MSE:0.0012, Reg:14.9998) beta=11.00
Iter 13000 | Total loss: 9.0012 (MSE:0.0012, Reg:9.0000) beta=9.88
Iter 14000 | Total loss: 1.0012 (MSE:0.0012, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 1.0012 (MSE:0.0012, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4400.9434 (MSE:0.0038, Reg:4400.9395) beta=20.00
Iter  5000 | Total loss: 577.4406 (MSE:0.0039, Reg:577.4366) beta=18.88
Iter  6000 | Total loss: 437.0038 (MSE:0.0038, Reg:437.0000) beta=17.75
Iter  7000 | Total loss: 377.0034 (MSE:0.0043, Reg:376.9991) beta=16.62
Iter  8000 | Total loss: 273.0302 (MSE:0.0037, Reg:273.0265) beta=15.50
Iter  9000 | Total loss: 208.2284 (MSE:0.0036, Reg:208.2247) beta=14.38
Iter 10000 | Total loss: 151.0021 (MSE:0.0041, Reg:150.9980) beta=13.25
Iter 11000 | Total loss: 99.6870 (MSE:0.0037, Reg:99.6833) beta=12.12
Iter 12000 | Total loss: 58.0040 (MSE:0.0040, Reg:58.0000) beta=11.00
Iter 13000 | Total loss: 23.0042 (MSE:0.0042, Reg:23.0000) beta=9.88
Iter 14000 | Total loss: 10.9962 (MSE:0.0042, Reg:10.9920) beta=8.75
Iter 15000 | Total loss: 2.0041 (MSE:0.0041, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5589.8857 (MSE:0.0019, Reg:5589.8838) beta=20.00
Iter  5000 | Total loss: 466.5545 (MSE:0.0018, Reg:466.5527) beta=18.88
Iter  6000 | Total loss: 338.9999 (MSE:0.0018, Reg:338.9982) beta=17.75
Iter  7000 | Total loss: 284.9507 (MSE:0.0018, Reg:284.9489) beta=16.62
Iter  8000 | Total loss: 223.9964 (MSE:0.0019, Reg:223.9945) beta=15.50
Iter  9000 | Total loss: 151.7314 (MSE:0.0018, Reg:151.7296) beta=14.38
Iter 10000 | Total loss: 92.0520 (MSE:0.0020, Reg:92.0500) beta=13.25
Iter 11000 | Total loss: 65.8468 (MSE:0.0019, Reg:65.8449) beta=12.12
Iter 12000 | Total loss: 31.0020 (MSE:0.0020, Reg:31.0000) beta=11.00
Iter 13000 | Total loss: 15.9940 (MSE:0.0019, Reg:15.9921) beta=9.88
Iter 14000 | Total loss: 4.0018 (MSE:0.0018, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 1.0019 (MSE:0.0019, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 18122.6660 (MSE:0.0030, Reg:18122.6621) beta=20.00
Iter  5000 | Total loss: 1588.2284 (MSE:0.0030, Reg:1588.2255) beta=18.88
Iter  6000 | Total loss: 1264.9552 (MSE:0.0028, Reg:1264.9524) beta=17.75
Iter  7000 | Total loss: 1064.4828 (MSE:0.0027, Reg:1064.4801) beta=16.62
Iter  8000 | Total loss: 850.7125 (MSE:0.0029, Reg:850.7096) beta=15.50
Iter  9000 | Total loss: 633.6439 (MSE:0.0028, Reg:633.6411) beta=14.38
Iter 10000 | Total loss: 456.7517 (MSE:0.0031, Reg:456.7487) beta=13.25
Iter 11000 | Total loss: 289.9673 (MSE:0.0027, Reg:289.9646) beta=12.12
Iter 12000 | Total loss: 132.0026 (MSE:0.0028, Reg:131.9997) beta=11.00
Iter 13000 | Total loss: 64.7124 (MSE:0.0027, Reg:64.7097) beta=9.88
Iter 14000 | Total loss: 22.9831 (MSE:0.0029, Reg:22.9802) beta=8.75
Iter 15000 | Total loss: 2.9958 (MSE:0.0030, Reg:2.9928) beta=7.62
Iter 16000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2037.9033 (MSE:0.0010, Reg:2037.9022) beta=20.00
Iter  5000 | Total loss: 190.9650 (MSE:0.0011, Reg:190.9639) beta=18.88
Iter  6000 | Total loss: 175.0012 (MSE:0.0012, Reg:175.0000) beta=17.75
Iter  7000 | Total loss: 158.0011 (MSE:0.0011, Reg:158.0000) beta=16.62
Iter  8000 | Total loss: 133.0012 (MSE:0.0012, Reg:133.0000) beta=15.50
Iter  9000 | Total loss: 111.7252 (MSE:0.0011, Reg:111.7241) beta=14.38
Iter 10000 | Total loss: 84.0012 (MSE:0.0012, Reg:84.0000) beta=13.25
Iter 11000 | Total loss: 49.0012 (MSE:0.0012, Reg:49.0000) beta=12.12
Iter 12000 | Total loss: 36.9755 (MSE:0.0012, Reg:36.9743) beta=11.00
Iter 13000 | Total loss: 22.0012 (MSE:0.0012, Reg:22.0000) beta=9.88
Iter 14000 | Total loss: 7.0011 (MSE:0.0011, Reg:7.0000) beta=8.75
Iter 15000 | Total loss: 6.0012 (MSE:0.0012, Reg:6.0000) beta=7.62
Iter 16000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15739.5693 (MSE:0.0024, Reg:15739.5674) beta=20.00
Iter  5000 | Total loss: 1488.4957 (MSE:0.0025, Reg:1488.4933) beta=18.88
Iter  6000 | Total loss: 1137.0355 (MSE:0.0022, Reg:1137.0333) beta=17.75
Iter  7000 | Total loss: 901.4651 (MSE:0.0024, Reg:901.4627) beta=16.62
Iter  8000 | Total loss: 737.9123 (MSE:0.0023, Reg:737.9100) beta=15.50
Iter  9000 | Total loss: 585.4269 (MSE:0.0023, Reg:585.4246) beta=14.38
Iter 10000 | Total loss: 369.9576 (MSE:0.0023, Reg:369.9553) beta=13.25
Iter 11000 | Total loss: 207.6224 (MSE:0.0028, Reg:207.6196) beta=12.12
Iter 12000 | Total loss: 99.0484 (MSE:0.0023, Reg:99.0461) beta=11.00
Iter 13000 | Total loss: 41.9827 (MSE:0.0025, Reg:41.9801) beta=9.88
Iter 14000 | Total loss: 14.0024 (MSE:0.0024, Reg:14.0000) beta=8.75
Iter 15000 | Total loss: 3.0026 (MSE:0.0026, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 29075.5254 (MSE:0.0028, Reg:29075.5234) beta=20.00
Iter  5000 | Total loss: 2811.9429 (MSE:0.0030, Reg:2811.9399) beta=18.88
Iter  6000 | Total loss: 2160.1125 (MSE:0.0026, Reg:2160.1099) beta=17.75
Iter  7000 | Total loss: 1739.9314 (MSE:0.0027, Reg:1739.9287) beta=16.62
Iter  8000 | Total loss: 1406.0298 (MSE:0.0027, Reg:1406.0271) beta=15.50
Iter  9000 | Total loss: 1081.6248 (MSE:0.0031, Reg:1081.6216) beta=14.38
Iter 10000 | Total loss: 791.5978 (MSE:0.0029, Reg:791.5949) beta=13.25
Iter 11000 | Total loss: 506.7402 (MSE:0.0027, Reg:506.7374) beta=12.12
Iter 12000 | Total loss: 253.8591 (MSE:0.0029, Reg:253.8562) beta=11.00
Iter 13000 | Total loss: 113.5368 (MSE:0.0029, Reg:113.5338) beta=9.88
Iter 14000 | Total loss: 28.0020 (MSE:0.0027, Reg:27.9993) beta=8.75
Iter 15000 | Total loss: 5.0029 (MSE:0.0029, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 31250.6406 (MSE:0.0023, Reg:31250.6387) beta=20.00
Iter  5000 | Total loss: 2757.6038 (MSE:0.0022, Reg:2757.6016) beta=18.88
Iter  6000 | Total loss: 1990.7372 (MSE:0.0021, Reg:1990.7351) beta=17.75
Iter  7000 | Total loss: 1570.7996 (MSE:0.0022, Reg:1570.7974) beta=16.62
Iter  8000 | Total loss: 1281.9590 (MSE:0.0022, Reg:1281.9568) beta=15.50
Iter  9000 | Total loss: 987.1132 (MSE:0.0022, Reg:987.1110) beta=14.38
Iter 10000 | Total loss: 683.3854 (MSE:0.0023, Reg:683.3832) beta=13.25
Iter 11000 | Total loss: 404.1445 (MSE:0.0024, Reg:404.1421) beta=12.12
Iter 12000 | Total loss: 218.2104 (MSE:0.0023, Reg:218.2080) beta=11.00
Iter 13000 | Total loss: 81.3347 (MSE:0.0023, Reg:81.3323) beta=9.88
Iter 14000 | Total loss: 21.0022 (MSE:0.0022, Reg:21.0000) beta=8.75
Iter 15000 | Total loss: 1.0022 (MSE:0.0022, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 64841.5664 (MSE:0.0025, Reg:64841.5625) beta=20.00
Iter  5000 | Total loss: 5369.0845 (MSE:0.0027, Reg:5369.0815) beta=18.88
Iter  6000 | Total loss: 3774.0784 (MSE:0.0024, Reg:3774.0759) beta=17.75
Iter  7000 | Total loss: 2877.0112 (MSE:0.0026, Reg:2877.0085) beta=16.62
Iter  8000 | Total loss: 2199.6628 (MSE:0.0026, Reg:2199.6602) beta=15.50
Iter  9000 | Total loss: 1693.6157 (MSE:0.0027, Reg:1693.6130) beta=14.38
Iter 10000 | Total loss: 1221.8160 (MSE:0.0026, Reg:1221.8134) beta=13.25
Iter 11000 | Total loss: 775.6461 (MSE:0.0026, Reg:775.6436) beta=12.12
Iter 12000 | Total loss: 401.0255 (MSE:0.0026, Reg:401.0229) beta=11.00
Iter 13000 | Total loss: 164.2352 (MSE:0.0025, Reg:164.2328) beta=9.88
Iter 14000 | Total loss: 55.2472 (MSE:0.0025, Reg:55.2447) beta=8.75
Iter 15000 | Total loss: 8.9554 (MSE:0.0024, Reg:8.9530) beta=7.62
Iter 16000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9526.8457 (MSE:0.0003, Reg:9526.8457) beta=20.00
Iter  5000 | Total loss: 660.0001 (MSE:0.0003, Reg:659.9998) beta=18.88
Iter  6000 | Total loss: 503.0002 (MSE:0.0003, Reg:502.9999) beta=17.75
Iter  7000 | Total loss: 417.7022 (MSE:0.0003, Reg:417.7020) beta=16.62
Iter  8000 | Total loss: 327.7009 (MSE:0.0003, Reg:327.7006) beta=15.50
Iter  9000 | Total loss: 249.0501 (MSE:0.0003, Reg:249.0498) beta=14.38
Iter 10000 | Total loss: 193.9994 (MSE:0.0003, Reg:193.9992) beta=13.25
Iter 11000 | Total loss: 121.9789 (MSE:0.0003, Reg:121.9786) beta=12.12
Iter 12000 | Total loss: 81.9487 (MSE:0.0003, Reg:81.9484) beta=11.00
Iter 13000 | Total loss: 52.2815 (MSE:0.0003, Reg:52.2812) beta=9.88
Iter 14000 | Total loss: 30.6495 (MSE:0.0003, Reg:30.6492) beta=8.75
Iter 15000 | Total loss: 13.0003 (MSE:0.0003, Reg:13.0000) beta=7.62
Iter 16000 | Total loss: 3.0003 (MSE:0.0003, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 58657.4492 (MSE:0.0015, Reg:58657.4492) beta=20.00
Iter  5000 | Total loss: 3901.6794 (MSE:0.0017, Reg:3901.6777) beta=18.88
Iter  6000 | Total loss: 2555.1370 (MSE:0.0016, Reg:2555.1353) beta=17.75
Iter  7000 | Total loss: 1900.3855 (MSE:0.0016, Reg:1900.3839) beta=16.62
Iter  8000 | Total loss: 1437.5278 (MSE:0.0016, Reg:1437.5262) beta=15.50
Iter  9000 | Total loss: 1058.3735 (MSE:0.0016, Reg:1058.3719) beta=14.38
Iter 10000 | Total loss: 782.6755 (MSE:0.0015, Reg:782.6740) beta=13.25
Iter 11000 | Total loss: 514.0369 (MSE:0.0015, Reg:514.0353) beta=12.12
Iter 12000 | Total loss: 290.7221 (MSE:0.0016, Reg:290.7205) beta=11.00
Iter 13000 | Total loss: 147.9229 (MSE:0.0016, Reg:147.9213) beta=9.88
Iter 14000 | Total loss: 34.5780 (MSE:0.0015, Reg:34.5764) beta=8.75
Iter 15000 | Total loss: 8.0017 (MSE:0.0017, Reg:8.0000) beta=7.62
Iter 16000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 100773.1562 (MSE:0.0020, Reg:100773.1562) beta=20.00
Iter  5000 | Total loss: 5650.3447 (MSE:0.0021, Reg:5650.3428) beta=18.88
Iter  6000 | Total loss: 3771.7505 (MSE:0.0021, Reg:3771.7485) beta=17.75
Iter  7000 | Total loss: 2789.6606 (MSE:0.0021, Reg:2789.6587) beta=16.62
Iter  8000 | Total loss: 2095.9194 (MSE:0.0021, Reg:2095.9172) beta=15.50
Iter  9000 | Total loss: 1553.6888 (MSE:0.0020, Reg:1553.6868) beta=14.38
Iter 10000 | Total loss: 1143.0334 (MSE:0.0020, Reg:1143.0315) beta=13.25
Iter 11000 | Total loss: 757.8818 (MSE:0.0021, Reg:757.8796) beta=12.12
Iter 12000 | Total loss: 422.1225 (MSE:0.0022, Reg:422.1203) beta=11.00
Iter 13000 | Total loss: 225.1513 (MSE:0.0021, Reg:225.1492) beta=9.88
Iter 14000 | Total loss: 78.8568 (MSE:0.0019, Reg:78.8549) beta=8.75
Iter 15000 | Total loss: 18.0021 (MSE:0.0021, Reg:18.0000) beta=7.62
Iter 16000 | Total loss: 1.0021 (MSE:0.0021, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 135548.8125 (MSE:0.0017, Reg:135548.8125) beta=20.00
Iter  5000 | Total loss: 3762.6594 (MSE:0.0016, Reg:3762.6577) beta=18.88
Iter  6000 | Total loss: 2269.8000 (MSE:0.0018, Reg:2269.7983) beta=17.75
Iter  7000 | Total loss: 1597.6442 (MSE:0.0018, Reg:1597.6423) beta=16.62
Iter  8000 | Total loss: 1183.3367 (MSE:0.0017, Reg:1183.3350) beta=15.50
Iter  9000 | Total loss: 901.6437 (MSE:0.0017, Reg:901.6420) beta=14.38
Iter 10000 | Total loss: 660.9514 (MSE:0.0018, Reg:660.9496) beta=13.25
Iter 11000 | Total loss: 457.2250 (MSE:0.0018, Reg:457.2232) beta=12.12
Iter 12000 | Total loss: 273.5927 (MSE:0.0017, Reg:273.5911) beta=11.00
Iter 13000 | Total loss: 143.8236 (MSE:0.0017, Reg:143.8219) beta=9.88
Iter 14000 | Total loss: 61.9776 (MSE:0.0017, Reg:61.9759) beta=8.75
Iter 15000 | Total loss: 15.0017 (MSE:0.0017, Reg:15.0000) beta=7.62
Iter 16000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 244943.3438 (MSE:0.0059, Reg:244943.3438) beta=20.00
Iter  5000 | Total loss: 16832.8145 (MSE:0.0061, Reg:16832.8086) beta=18.88
Iter  6000 | Total loss: 11959.9746 (MSE:0.0057, Reg:11959.9688) beta=17.75
Iter  7000 | Total loss: 8959.1328 (MSE:0.0062, Reg:8959.1270) beta=16.62
Iter  8000 | Total loss: 6766.5557 (MSE:0.0061, Reg:6766.5498) beta=15.50
Iter  9000 | Total loss: 5045.7012 (MSE:0.0058, Reg:5045.6953) beta=14.38
Iter 10000 | Total loss: 3658.8789 (MSE:0.0059, Reg:3658.8730) beta=13.25
Iter 11000 | Total loss: 2379.0508 (MSE:0.0060, Reg:2379.0447) beta=12.12
Iter 12000 | Total loss: 1356.0807 (MSE:0.0060, Reg:1356.0747) beta=11.00
Iter 13000 | Total loss: 558.3335 (MSE:0.0061, Reg:558.3275) beta=9.88
Iter 14000 | Total loss: 145.4076 (MSE:0.0059, Reg:145.4017) beta=8.75
Iter 15000 | Total loss: 17.8535 (MSE:0.0058, Reg:17.8477) beta=7.62
Iter 16000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 27689.9707 (MSE:0.0019, Reg:27689.9688) beta=20.00
Iter  5000 | Total loss: 2455.1978 (MSE:0.0021, Reg:2455.1956) beta=18.88
Iter  6000 | Total loss: 2000.9661 (MSE:0.0019, Reg:2000.9641) beta=17.75
Iter  7000 | Total loss: 1666.6360 (MSE:0.0020, Reg:1666.6340) beta=16.62
Iter  8000 | Total loss: 1407.0698 (MSE:0.0021, Reg:1407.0677) beta=15.50
Iter  9000 | Total loss: 1116.9241 (MSE:0.0019, Reg:1116.9221) beta=14.38
Iter 10000 | Total loss: 805.6927 (MSE:0.0021, Reg:805.6906) beta=13.25
Iter 11000 | Total loss: 495.9008 (MSE:0.0021, Reg:495.8988) beta=12.12
Iter 12000 | Total loss: 281.7626 (MSE:0.0020, Reg:281.7606) beta=11.00
Iter 13000 | Total loss: 130.5354 (MSE:0.0020, Reg:130.5335) beta=9.88
Iter 14000 | Total loss: 50.7663 (MSE:0.0020, Reg:50.7642) beta=8.75
Iter 15000 | Total loss: 5.0023 (MSE:0.0023, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 477415.3438 (MSE:0.0029, Reg:477415.3438) beta=20.00
Iter  5000 | Total loss: 12401.6152 (MSE:0.0029, Reg:12401.6123) beta=18.88
Iter  6000 | Total loss: 8539.7441 (MSE:0.0032, Reg:8539.7412) beta=17.75
Iter  7000 | Total loss: 6012.6768 (MSE:0.0030, Reg:6012.6738) beta=16.62
Iter  8000 | Total loss: 4347.5020 (MSE:0.0030, Reg:4347.4990) beta=15.50
Iter  9000 | Total loss: 3166.9524 (MSE:0.0031, Reg:3166.9492) beta=14.38
Iter 10000 | Total loss: 2285.1375 (MSE:0.0031, Reg:2285.1343) beta=13.25
Iter 11000 | Total loss: 1544.6445 (MSE:0.0031, Reg:1544.6415) beta=12.12
Iter 12000 | Total loss: 952.2233 (MSE:0.0031, Reg:952.2202) beta=11.00
Iter 13000 | Total loss: 512.1342 (MSE:0.0030, Reg:512.1312) beta=9.88
Iter 14000 | Total loss: 187.4758 (MSE:0.0029, Reg:187.4729) beta=8.75
Iter 15000 | Total loss: 36.6635 (MSE:0.0029, Reg:36.6606) beta=7.62
Iter 16000 | Total loss: 1.0027 (MSE:0.0027, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2727 (MSE:0.2727, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2302 (MSE:0.2302, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2270 (MSE:0.2270, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2057 (MSE:0.2057, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 190047.2969 (MSE:0.2089, Reg:190047.0938) beta=20.00
Iter  5000 | Total loss: 33916.0586 (MSE:0.2152, Reg:33915.8438) beta=18.88
Iter  6000 | Total loss: 22819.9570 (MSE:0.2182, Reg:22819.7383) beta=17.75
Iter  7000 | Total loss: 15169.6641 (MSE:0.2125, Reg:15169.4512) beta=16.62
Iter  8000 | Total loss: 9498.0605 (MSE:0.2105, Reg:9497.8496) beta=15.50
Iter  9000 | Total loss: 5511.8413 (MSE:0.2182, Reg:5511.6230) beta=14.38
Iter 10000 | Total loss: 2620.1328 (MSE:0.2125, Reg:2619.9202) beta=13.25
Iter 11000 | Total loss: 939.6098 (MSE:0.2168, Reg:939.3931) beta=12.12
Iter 12000 | Total loss: 201.1619 (MSE:0.2225, Reg:200.9394) beta=11.00
Iter 13000 | Total loss: 24.4306 (MSE:0.2060, Reg:24.2246) beta=9.88
Iter 14000 | Total loss: 1.2124 (MSE:0.2124, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.2131 (MSE:0.2131, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2186 (MSE:0.2186, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2243 (MSE:0.2243, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2135 (MSE:0.2135, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2096 (MSE:0.2096, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2085 (MSE:0.2085, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2231 (MSE:0.2231, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1551 (MSE:0.1551, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1535 (MSE:0.1535, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1459 (MSE:0.1459, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30419.6504 (MSE:0.1552, Reg:30419.4961) beta=20.00
Iter  5000 | Total loss: 6115.0859 (MSE:0.1639, Reg:6114.9219) beta=18.88
Iter  6000 | Total loss: 4477.1675 (MSE:0.1813, Reg:4476.9863) beta=17.75
Iter  7000 | Total loss: 3253.9617 (MSE:0.1687, Reg:3253.7930) beta=16.62
Iter  8000 | Total loss: 2255.7512 (MSE:0.1718, Reg:2255.5793) beta=15.50
Iter  9000 | Total loss: 1390.6504 (MSE:0.1785, Reg:1390.4719) beta=14.38
Iter 10000 | Total loss: 756.3416 (MSE:0.1505, Reg:756.1912) beta=13.25
Iter 11000 | Total loss: 309.2234 (MSE:0.1536, Reg:309.0697) beta=12.12
Iter 12000 | Total loss: 89.0668 (MSE:0.1584, Reg:88.9084) beta=11.00
Iter 13000 | Total loss: 20.4619 (MSE:0.1734, Reg:20.2885) beta=9.88
Iter 14000 | Total loss: 1.1488 (MSE:0.1488, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.1696 (MSE:0.1696, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1553 (MSE:0.1553, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1481 (MSE:0.1481, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1654 (MSE:0.1654, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1615 (MSE:0.1615, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1656 (MSE:0.1656, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 68.684%
Total time: 881.86 sec

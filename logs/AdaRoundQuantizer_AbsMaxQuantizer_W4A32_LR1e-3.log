
Case: [ resnet18_AdaRound_AbsMaxQuantizer_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2051.3699 (MSE:0.0003, Reg:2051.3696) beta=20.00
Iter  5000 | Total loss: 12.4396 (MSE:0.0078, Reg:12.4318) beta=18.88
Iter  6000 | Total loss: 5.0054 (MSE:0.0054, Reg:5.0000) beta=17.75
Iter  7000 | Total loss: 4.0054 (MSE:0.0054, Reg:4.0000) beta=16.62
Iter  8000 | Total loss: 3.0055 (MSE:0.0055, Reg:3.0000) beta=15.50
Iter  9000 | Total loss: 3.0057 (MSE:0.0057, Reg:3.0000) beta=14.38
Iter 10000 | Total loss: 1.0060 (MSE:0.0060, Reg:1.0000) beta=13.25
Iter 11000 | Total loss: 1.0046 (MSE:0.0046, Reg:1.0000) beta=12.12
Iter 12000 | Total loss: 1.0064 (MSE:0.0064, Reg:1.0000) beta=11.00
Iter 13000 | Total loss: 1.0050 (MSE:0.0050, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0054 (MSE:0.0054, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0111 (MSE:0.0111, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5299.1768 (MSE:0.0015, Reg:5299.1753) beta=20.00
Iter  5000 | Total loss: 488.6764 (MSE:0.0034, Reg:488.6730) beta=18.88
Iter  6000 | Total loss: 320.5717 (MSE:0.0040, Reg:320.5677) beta=17.75
Iter  7000 | Total loss: 228.4582 (MSE:0.0040, Reg:228.4542) beta=16.62
Iter  8000 | Total loss: 174.5251 (MSE:0.0038, Reg:174.5213) beta=15.50
Iter  9000 | Total loss: 126.5099 (MSE:0.0039, Reg:126.5060) beta=14.38
Iter 10000 | Total loss: 91.8061 (MSE:0.0036, Reg:91.8025) beta=13.25
Iter 11000 | Total loss: 60.8035 (MSE:0.0037, Reg:60.7999) beta=12.12
Iter 12000 | Total loss: 37.1071 (MSE:0.0040, Reg:37.1031) beta=11.00
Iter 13000 | Total loss: 22.4108 (MSE:0.0038, Reg:22.4070) beta=9.88
Iter 14000 | Total loss: 7.1946 (MSE:0.0035, Reg:7.1912) beta=8.75
Iter 15000 | Total loss: 1.2556 (MSE:0.0037, Reg:1.2518) beta=7.62
Iter 16000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7079.0820 (MSE:0.0036, Reg:7079.0786) beta=20.00
Iter  5000 | Total loss: 1063.9021 (MSE:0.0036, Reg:1063.8984) beta=18.88
Iter  6000 | Total loss: 673.8204 (MSE:0.0035, Reg:673.8169) beta=17.75
Iter  7000 | Total loss: 502.2532 (MSE:0.0037, Reg:502.2495) beta=16.62
Iter  8000 | Total loss: 371.1964 (MSE:0.0038, Reg:371.1926) beta=15.50
Iter  9000 | Total loss: 270.1445 (MSE:0.0037, Reg:270.1408) beta=14.38
Iter 10000 | Total loss: 210.2646 (MSE:0.0036, Reg:210.2610) beta=13.25
Iter 11000 | Total loss: 150.3272 (MSE:0.0034, Reg:150.3238) beta=12.12
Iter 12000 | Total loss: 101.8351 (MSE:0.0037, Reg:101.8314) beta=11.00
Iter 13000 | Total loss: 55.9788 (MSE:0.0032, Reg:55.9756) beta=9.88
Iter 14000 | Total loss: 24.8266 (MSE:0.0036, Reg:24.8230) beta=8.75
Iter 15000 | Total loss: 6.6995 (MSE:0.0037, Reg:6.6959) beta=7.62
Iter 16000 | Total loss: 0.8526 (MSE:0.0038, Reg:0.8488) beta=6.50
Iter 17000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0130 (MSE:0.0130, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5991.1538 (MSE:0.0034, Reg:5991.1504) beta=20.00
Iter  5000 | Total loss: 521.8571 (MSE:0.0073, Reg:521.8498) beta=18.88
Iter  6000 | Total loss: 291.7911 (MSE:0.0078, Reg:291.7832) beta=17.75
Iter  7000 | Total loss: 216.8102 (MSE:0.0068, Reg:216.8034) beta=16.62
Iter  8000 | Total loss: 169.8046 (MSE:0.0071, Reg:169.7975) beta=15.50
Iter  9000 | Total loss: 127.9149 (MSE:0.0068, Reg:127.9082) beta=14.38
Iter 10000 | Total loss: 92.8267 (MSE:0.0075, Reg:92.8192) beta=13.25
Iter 11000 | Total loss: 65.1667 (MSE:0.0072, Reg:65.1595) beta=12.12
Iter 12000 | Total loss: 47.6971 (MSE:0.0069, Reg:47.6902) beta=11.00
Iter 13000 | Total loss: 25.5859 (MSE:0.0070, Reg:25.5789) beta=9.88
Iter 14000 | Total loss: 7.5998 (MSE:0.0070, Reg:7.5928) beta=8.75
Iter 15000 | Total loss: 3.0073 (MSE:0.0073, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0400 (MSE:0.0400, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0195 (MSE:0.0195, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0159 (MSE:0.0159, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0139 (MSE:0.0139, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9157.7148 (MSE:0.0134, Reg:9157.7012) beta=20.00
Iter  5000 | Total loss: 1505.9609 (MSE:0.0136, Reg:1505.9474) beta=18.88
Iter  6000 | Total loss: 1044.9019 (MSE:0.0131, Reg:1044.8888) beta=17.75
Iter  7000 | Total loss: 789.6158 (MSE:0.0145, Reg:789.6013) beta=16.62
Iter  8000 | Total loss: 598.0568 (MSE:0.0130, Reg:598.0438) beta=15.50
Iter  9000 | Total loss: 462.4437 (MSE:0.0128, Reg:462.4308) beta=14.38
Iter 10000 | Total loss: 349.1648 (MSE:0.0141, Reg:349.1506) beta=13.25
Iter 11000 | Total loss: 263.3398 (MSE:0.0129, Reg:263.3269) beta=12.12
Iter 12000 | Total loss: 186.6107 (MSE:0.0136, Reg:186.5971) beta=11.00
Iter 13000 | Total loss: 116.9281 (MSE:0.0142, Reg:116.9139) beta=9.88
Iter 14000 | Total loss: 65.3270 (MSE:0.0144, Reg:65.3126) beta=8.75
Iter 15000 | Total loss: 24.9605 (MSE:0.0140, Reg:24.9465) beta=7.62
Iter 16000 | Total loss: 7.1956 (MSE:0.0146, Reg:7.1811) beta=6.50
Iter 17000 | Total loss: 1.2129 (MSE:0.0133, Reg:1.1996) beta=5.38
Iter 18000 | Total loss: 0.0134 (MSE:0.0134, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0127 (MSE:0.0127, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10705.3301 (MSE:0.0060, Reg:10705.3242) beta=20.00
Iter  5000 | Total loss: 1001.3326 (MSE:0.0059, Reg:1001.3267) beta=18.88
Iter  6000 | Total loss: 594.3417 (MSE:0.0059, Reg:594.3358) beta=17.75
Iter  7000 | Total loss: 422.7733 (MSE:0.0060, Reg:422.7673) beta=16.62
Iter  8000 | Total loss: 310.1853 (MSE:0.0061, Reg:310.1793) beta=15.50
Iter  9000 | Total loss: 229.3435 (MSE:0.0058, Reg:229.3378) beta=14.38
Iter 10000 | Total loss: 171.6806 (MSE:0.0063, Reg:171.6744) beta=13.25
Iter 11000 | Total loss: 115.3927 (MSE:0.0063, Reg:115.3864) beta=12.12
Iter 12000 | Total loss: 74.7955 (MSE:0.0064, Reg:74.7890) beta=11.00
Iter 13000 | Total loss: 41.7959 (MSE:0.0061, Reg:41.7898) beta=9.88
Iter 14000 | Total loss: 21.3341 (MSE:0.0061, Reg:21.3280) beta=8.75
Iter 15000 | Total loss: 5.6800 (MSE:0.0062, Reg:5.6739) beta=7.62
Iter 16000 | Total loss: 1.3943 (MSE:0.0061, Reg:1.3882) beta=6.50
Iter 17000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0118 (MSE:0.0118, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 24545.0547 (MSE:0.0087, Reg:24545.0469) beta=20.00
Iter  5000 | Total loss: 2153.6685 (MSE:0.0089, Reg:2153.6597) beta=18.88
Iter  6000 | Total loss: 1309.6769 (MSE:0.0085, Reg:1309.6683) beta=17.75
Iter  7000 | Total loss: 844.8665 (MSE:0.0083, Reg:844.8582) beta=16.62
Iter  8000 | Total loss: 620.3765 (MSE:0.0087, Reg:620.3677) beta=15.50
Iter  9000 | Total loss: 487.6749 (MSE:0.0084, Reg:487.6665) beta=14.38
Iter 10000 | Total loss: 394.8148 (MSE:0.0092, Reg:394.8056) beta=13.25
Iter 11000 | Total loss: 286.5098 (MSE:0.0082, Reg:286.5016) beta=12.12
Iter 12000 | Total loss: 201.0233 (MSE:0.0085, Reg:201.0148) beta=11.00
Iter 13000 | Total loss: 128.7861 (MSE:0.0082, Reg:128.7778) beta=9.88
Iter 14000 | Total loss: 68.1155 (MSE:0.0086, Reg:68.1070) beta=8.75
Iter 15000 | Total loss: 25.0208 (MSE:0.0089, Reg:25.0119) beta=7.62
Iter 16000 | Total loss: 2.1997 (MSE:0.0087, Reg:2.1910) beta=6.50
Iter 17000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0087 (MSE:0.0087, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2589.0610 (MSE:0.0035, Reg:2589.0576) beta=20.00
Iter  5000 | Total loss: 332.9299 (MSE:0.0036, Reg:332.9263) beta=18.88
Iter  6000 | Total loss: 208.2277 (MSE:0.0037, Reg:208.2239) beta=17.75
Iter  7000 | Total loss: 173.6768 (MSE:0.0037, Reg:173.6731) beta=16.62
Iter  8000 | Total loss: 142.4229 (MSE:0.0039, Reg:142.4190) beta=15.50
Iter  9000 | Total loss: 122.1587 (MSE:0.0037, Reg:122.1550) beta=14.38
Iter 10000 | Total loss: 108.2560 (MSE:0.0038, Reg:108.2522) beta=13.25
Iter 11000 | Total loss: 87.9818 (MSE:0.0037, Reg:87.9781) beta=12.12
Iter 12000 | Total loss: 63.7782 (MSE:0.0039, Reg:63.7743) beta=11.00
Iter 13000 | Total loss: 36.8547 (MSE:0.0036, Reg:36.8511) beta=9.88
Iter 14000 | Total loss: 18.9222 (MSE:0.0036, Reg:18.9186) beta=8.75
Iter 15000 | Total loss: 13.5860 (MSE:0.0038, Reg:13.5821) beta=7.62
Iter 16000 | Total loss: 7.3741 (MSE:0.0035, Reg:7.3705) beta=6.50
Iter 17000 | Total loss: 2.2017 (MSE:0.0037, Reg:2.1980) beta=5.38
Iter 18000 | Total loss: 0.1512 (MSE:0.0038, Reg:0.1474) beta=4.25
Iter 19000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0089 (MSE:0.0089, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 23571.2051 (MSE:0.0064, Reg:23571.1992) beta=20.00
Iter  5000 | Total loss: 2290.4958 (MSE:0.0070, Reg:2290.4888) beta=18.88
Iter  6000 | Total loss: 1319.9250 (MSE:0.0064, Reg:1319.9187) beta=17.75
Iter  7000 | Total loss: 876.2954 (MSE:0.0068, Reg:876.2886) beta=16.62
Iter  8000 | Total loss: 657.3362 (MSE:0.0066, Reg:657.3297) beta=15.50
Iter  9000 | Total loss: 505.6330 (MSE:0.0067, Reg:505.6262) beta=14.38
Iter 10000 | Total loss: 389.6534 (MSE:0.0067, Reg:389.6468) beta=13.25
Iter 11000 | Total loss: 292.3244 (MSE:0.0075, Reg:292.3170) beta=12.12
Iter 12000 | Total loss: 207.7039 (MSE:0.0065, Reg:207.6974) beta=11.00
Iter 13000 | Total loss: 132.7577 (MSE:0.0070, Reg:132.7507) beta=9.88
Iter 14000 | Total loss: 70.9111 (MSE:0.0069, Reg:70.9042) beta=8.75
Iter 15000 | Total loss: 27.1737 (MSE:0.0071, Reg:27.1667) beta=7.62
Iter 16000 | Total loss: 6.6262 (MSE:0.0069, Reg:6.6193) beta=6.50
Iter 17000 | Total loss: 1.0067 (MSE:0.0067, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 34767.1758 (MSE:0.0075, Reg:34767.1680) beta=20.00
Iter  5000 | Total loss: 2799.2810 (MSE:0.0079, Reg:2799.2732) beta=18.88
Iter  6000 | Total loss: 1595.0469 (MSE:0.0070, Reg:1595.0398) beta=17.75
Iter  7000 | Total loss: 1120.5934 (MSE:0.0076, Reg:1120.5858) beta=16.62
Iter  8000 | Total loss: 863.0058 (MSE:0.0073, Reg:862.9985) beta=15.50
Iter  9000 | Total loss: 689.7035 (MSE:0.0080, Reg:689.6954) beta=14.38
Iter 10000 | Total loss: 549.5112 (MSE:0.0078, Reg:549.5034) beta=13.25
Iter 11000 | Total loss: 427.7375 (MSE:0.0073, Reg:427.7302) beta=12.12
Iter 12000 | Total loss: 298.6085 (MSE:0.0078, Reg:298.6006) beta=11.00
Iter 13000 | Total loss: 201.1662 (MSE:0.0077, Reg:201.1584) beta=9.88
Iter 14000 | Total loss: 116.0369 (MSE:0.0073, Reg:116.0296) beta=8.75
Iter 15000 | Total loss: 50.8202 (MSE:0.0078, Reg:50.8124) beta=7.62
Iter 16000 | Total loss: 23.1474 (MSE:0.0074, Reg:23.1400) beta=6.50
Iter 17000 | Total loss: 4.7107 (MSE:0.0073, Reg:4.7034) beta=5.38
Iter 18000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 56434.8672 (MSE:0.0062, Reg:56434.8594) beta=20.00
Iter  5000 | Total loss: 2890.5955 (MSE:0.0068, Reg:2890.5886) beta=18.88
Iter  6000 | Total loss: 1483.5031 (MSE:0.0064, Reg:1483.4966) beta=17.75
Iter  7000 | Total loss: 907.8382 (MSE:0.0068, Reg:907.8314) beta=16.62
Iter  8000 | Total loss: 658.4335 (MSE:0.0070, Reg:658.4265) beta=15.50
Iter  9000 | Total loss: 490.2791 (MSE:0.0070, Reg:490.2722) beta=14.38
Iter 10000 | Total loss: 383.0064 (MSE:0.0066, Reg:382.9998) beta=13.25
Iter 11000 | Total loss: 281.1629 (MSE:0.0071, Reg:281.1558) beta=12.12
Iter 12000 | Total loss: 206.2928 (MSE:0.0069, Reg:206.2859) beta=11.00
Iter 13000 | Total loss: 139.8793 (MSE:0.0071, Reg:139.8722) beta=9.88
Iter 14000 | Total loss: 78.9708 (MSE:0.0069, Reg:78.9639) beta=8.75
Iter 15000 | Total loss: 38.3033 (MSE:0.0067, Reg:38.2966) beta=7.62
Iter 16000 | Total loss: 9.2787 (MSE:0.0070, Reg:9.2717) beta=6.50
Iter 17000 | Total loss: 1.3120 (MSE:0.0067, Reg:1.3054) beta=5.38
Iter 18000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 102459.6797 (MSE:0.0071, Reg:102459.6719) beta=20.00
Iter  5000 | Total loss: 2650.0591 (MSE:0.0083, Reg:2650.0508) beta=18.88
Iter  6000 | Total loss: 1303.6647 (MSE:0.0075, Reg:1303.6572) beta=17.75
Iter  7000 | Total loss: 802.3540 (MSE:0.0083, Reg:802.3457) beta=16.62
Iter  8000 | Total loss: 568.1316 (MSE:0.0081, Reg:568.1235) beta=15.50
Iter  9000 | Total loss: 410.0004 (MSE:0.0083, Reg:409.9921) beta=14.38
Iter 10000 | Total loss: 328.3528 (MSE:0.0083, Reg:328.3445) beta=13.25
Iter 11000 | Total loss: 252.0225 (MSE:0.0081, Reg:252.0144) beta=12.12
Iter 12000 | Total loss: 189.3841 (MSE:0.0082, Reg:189.3760) beta=11.00
Iter 13000 | Total loss: 138.2972 (MSE:0.0077, Reg:138.2895) beta=9.88
Iter 14000 | Total loss: 91.7810 (MSE:0.0080, Reg:91.7730) beta=8.75
Iter 15000 | Total loss: 48.0119 (MSE:0.0076, Reg:48.0043) beta=7.62
Iter 16000 | Total loss: 23.1563 (MSE:0.0079, Reg:23.1483) beta=6.50
Iter 17000 | Total loss: 3.9763 (MSE:0.0084, Reg:3.9679) beta=5.38
Iter 18000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 12457.3652 (MSE:0.0007, Reg:12457.3643) beta=20.00
Iter  5000 | Total loss: 403.1892 (MSE:0.0007, Reg:403.1885) beta=18.88
Iter  6000 | Total loss: 241.7849 (MSE:0.0008, Reg:241.7841) beta=17.75
Iter  7000 | Total loss: 171.0750 (MSE:0.0008, Reg:171.0742) beta=16.62
Iter  8000 | Total loss: 129.8994 (MSE:0.0007, Reg:129.8986) beta=15.50
Iter  9000 | Total loss: 105.8330 (MSE:0.0007, Reg:105.8322) beta=14.38
Iter 10000 | Total loss: 77.4204 (MSE:0.0007, Reg:77.4196) beta=13.25
Iter 11000 | Total loss: 62.2293 (MSE:0.0007, Reg:62.2285) beta=12.12
Iter 12000 | Total loss: 40.8199 (MSE:0.0008, Reg:40.8191) beta=11.00
Iter 13000 | Total loss: 22.4680 (MSE:0.0008, Reg:22.4672) beta=9.88
Iter 14000 | Total loss: 10.1457 (MSE:0.0008, Reg:10.1449) beta=8.75
Iter 15000 | Total loss: 4.0853 (MSE:0.0008, Reg:4.0845) beta=7.62
Iter 16000 | Total loss: 1.9807 (MSE:0.0007, Reg:1.9799) beta=6.50
Iter 17000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 96807.8516 (MSE:0.0044, Reg:96807.8438) beta=20.00
Iter  5000 | Total loss: 1147.5497 (MSE:0.0058, Reg:1147.5439) beta=18.88
Iter  6000 | Total loss: 544.2805 (MSE:0.0055, Reg:544.2750) beta=17.75
Iter  7000 | Total loss: 343.4715 (MSE:0.0054, Reg:343.4662) beta=16.62
Iter  8000 | Total loss: 242.1548 (MSE:0.0056, Reg:242.1493) beta=15.50
Iter  9000 | Total loss: 170.2144 (MSE:0.0055, Reg:170.2089) beta=14.38
Iter 10000 | Total loss: 124.4303 (MSE:0.0052, Reg:124.4250) beta=13.25
Iter 11000 | Total loss: 100.5226 (MSE:0.0051, Reg:100.5175) beta=12.12
Iter 12000 | Total loss: 72.4022 (MSE:0.0056, Reg:72.3966) beta=11.00
Iter 13000 | Total loss: 48.4712 (MSE:0.0056, Reg:48.4656) beta=9.88
Iter 14000 | Total loss: 33.1065 (MSE:0.0054, Reg:33.1011) beta=8.75
Iter 15000 | Total loss: 20.4099 (MSE:0.0056, Reg:20.4044) beta=7.62
Iter 16000 | Total loss: 10.4422 (MSE:0.0057, Reg:10.4365) beta=6.50
Iter 17000 | Total loss: 4.9608 (MSE:0.0055, Reg:4.9552) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 137360.6406 (MSE:0.0057, Reg:137360.6406) beta=20.00
Iter  5000 | Total loss: 2538.6543 (MSE:0.0067, Reg:2538.6475) beta=18.88
Iter  6000 | Total loss: 1207.9907 (MSE:0.0066, Reg:1207.9841) beta=17.75
Iter  7000 | Total loss: 780.7238 (MSE:0.0064, Reg:780.7174) beta=16.62
Iter  8000 | Total loss: 587.7729 (MSE:0.0065, Reg:587.7664) beta=15.50
Iter  9000 | Total loss: 437.2449 (MSE:0.0068, Reg:437.2381) beta=14.38
Iter 10000 | Total loss: 325.2939 (MSE:0.0064, Reg:325.2875) beta=13.25
Iter 11000 | Total loss: 246.1339 (MSE:0.0066, Reg:246.1272) beta=12.12
Iter 12000 | Total loss: 191.6431 (MSE:0.0069, Reg:191.6362) beta=11.00
Iter 13000 | Total loss: 132.5703 (MSE:0.0066, Reg:132.5636) beta=9.88
Iter 14000 | Total loss: 85.8493 (MSE:0.0062, Reg:85.8430) beta=8.75
Iter 15000 | Total loss: 51.3470 (MSE:0.0066, Reg:51.3404) beta=7.62
Iter 16000 | Total loss: 21.9568 (MSE:0.0066, Reg:21.9502) beta=6.50
Iter 17000 | Total loss: 5.3758 (MSE:0.0068, Reg:5.3690) beta=5.38
Iter 18000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 215526.5625 (MSE:0.0049, Reg:215526.5625) beta=20.00
Iter  5000 | Total loss: 667.9739 (MSE:0.0056, Reg:667.9683) beta=18.88
Iter  6000 | Total loss: 340.1331 (MSE:0.0059, Reg:340.1272) beta=17.75
Iter  7000 | Total loss: 228.9598 (MSE:0.0059, Reg:228.9539) beta=16.62
Iter  8000 | Total loss: 169.3299 (MSE:0.0057, Reg:169.3242) beta=15.50
Iter  9000 | Total loss: 110.9929 (MSE:0.0057, Reg:110.9872) beta=14.38
Iter 10000 | Total loss: 67.3930 (MSE:0.0060, Reg:67.3871) beta=13.25
Iter 11000 | Total loss: 52.0840 (MSE:0.0059, Reg:52.0781) beta=12.12
Iter 12000 | Total loss: 28.3549 (MSE:0.0058, Reg:28.3491) beta=11.00
Iter 13000 | Total loss: 19.9070 (MSE:0.0057, Reg:19.9012) beta=9.88
Iter 14000 | Total loss: 8.9296 (MSE:0.0060, Reg:8.9236) beta=8.75
Iter 15000 | Total loss: 4.0057 (MSE:0.0057, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 3.0057 (MSE:0.0057, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 2.1540 (MSE:0.0058, Reg:2.1482) beta=5.38
Iter 18000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0185 (MSE:0.0185, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0182 (MSE:0.0182, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0168 (MSE:0.0168, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0176 (MSE:0.0176, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 447098.9062 (MSE:0.0166, Reg:447098.8750) beta=20.00
Iter  5000 | Total loss: 5852.9961 (MSE:0.0196, Reg:5852.9766) beta=18.88
Iter  6000 | Total loss: 3172.5945 (MSE:0.0188, Reg:3172.5757) beta=17.75
Iter  7000 | Total loss: 2103.7092 (MSE:0.0203, Reg:2103.6890) beta=16.62
Iter  8000 | Total loss: 1537.5232 (MSE:0.0191, Reg:1537.5042) beta=15.50
Iter  9000 | Total loss: 1176.1938 (MSE:0.0189, Reg:1176.1750) beta=14.38
Iter 10000 | Total loss: 887.2919 (MSE:0.0189, Reg:887.2729) beta=13.25
Iter 11000 | Total loss: 659.6586 (MSE:0.0188, Reg:659.6398) beta=12.12
Iter 12000 | Total loss: 482.3137 (MSE:0.0197, Reg:482.2941) beta=11.00
Iter 13000 | Total loss: 358.7146 (MSE:0.0195, Reg:358.6951) beta=9.88
Iter 14000 | Total loss: 232.0101 (MSE:0.0187, Reg:231.9914) beta=8.75
Iter 15000 | Total loss: 125.2267 (MSE:0.0191, Reg:125.2077) beta=7.62
Iter 16000 | Total loss: 59.0821 (MSE:0.0191, Reg:59.0629) beta=6.50
Iter 17000 | Total loss: 9.8404 (MSE:0.0186, Reg:9.8218) beta=5.38
Iter 18000 | Total loss: 0.0194 (MSE:0.0194, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0192 (MSE:0.0192, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0196 (MSE:0.0196, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 37661.3906 (MSE:0.0051, Reg:37661.3867) beta=20.00
Iter  5000 | Total loss: 1849.4144 (MSE:0.0060, Reg:1849.4084) beta=18.88
Iter  6000 | Total loss: 1005.2094 (MSE:0.0056, Reg:1005.2038) beta=17.75
Iter  7000 | Total loss: 682.0258 (MSE:0.0057, Reg:682.0201) beta=16.62
Iter  8000 | Total loss: 526.9283 (MSE:0.0059, Reg:526.9224) beta=15.50
Iter  9000 | Total loss: 420.5952 (MSE:0.0056, Reg:420.5895) beta=14.38
Iter 10000 | Total loss: 339.4215 (MSE:0.0060, Reg:339.4156) beta=13.25
Iter 11000 | Total loss: 278.2980 (MSE:0.0059, Reg:278.2921) beta=12.12
Iter 12000 | Total loss: 214.1756 (MSE:0.0057, Reg:214.1699) beta=11.00
Iter 13000 | Total loss: 151.4286 (MSE:0.0056, Reg:151.4230) beta=9.88
Iter 14000 | Total loss: 84.9512 (MSE:0.0058, Reg:84.9454) beta=8.75
Iter 15000 | Total loss: 30.2380 (MSE:0.0063, Reg:30.2317) beta=7.62
Iter 16000 | Total loss: 5.2186 (MSE:0.0056, Reg:5.2130) beta=6.50
Iter 17000 | Total loss: 2.6705 (MSE:0.0058, Reg:2.6647) beta=5.38
Iter 18000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 477063.6875 (MSE:0.0083, Reg:477063.6875) beta=20.00
Iter  5000 | Total loss: 560.4725 (MSE:0.0084, Reg:560.4642) beta=18.88
Iter  6000 | Total loss: 287.8562 (MSE:0.0094, Reg:287.8468) beta=17.75
Iter  7000 | Total loss: 199.1318 (MSE:0.0095, Reg:199.1223) beta=16.62
Iter  8000 | Total loss: 151.3973 (MSE:0.0092, Reg:151.3881) beta=15.50
Iter  9000 | Total loss: 112.7961 (MSE:0.0096, Reg:112.7865) beta=14.38
Iter 10000 | Total loss: 87.0098 (MSE:0.0099, Reg:86.9998) beta=13.25
Iter 11000 | Total loss: 67.3301 (MSE:0.0097, Reg:67.3204) beta=12.12
Iter 12000 | Total loss: 50.6935 (MSE:0.0090, Reg:50.6845) beta=11.00
Iter 13000 | Total loss: 35.4888 (MSE:0.0094, Reg:35.4795) beta=9.88
Iter 14000 | Total loss: 23.7764 (MSE:0.0088, Reg:23.7676) beta=8.75
Iter 15000 | Total loss: 13.2103 (MSE:0.0091, Reg:13.2013) beta=7.62
Iter 16000 | Total loss: 4.5071 (MSE:0.0088, Reg:4.4983) beta=6.50
Iter 17000 | Total loss: 1.0101 (MSE:0.0101, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0091 (MSE:0.0091, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0102 (MSE:0.0102, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.6283 (MSE:0.6283, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.6022 (MSE:0.6022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.5884 (MSE:0.5884, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.5483 (MSE:0.5483, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 363708.3125 (MSE:0.5114, Reg:363707.8125) beta=20.00
Iter  5000 | Total loss: 25128.9980 (MSE:0.5735, Reg:25128.4238) beta=18.88
Iter  6000 | Total loss: 14782.4951 (MSE:0.5535, Reg:14781.9414) beta=17.75
Iter  7000 | Total loss: 10155.2471 (MSE:0.5889, Reg:10154.6582) beta=16.62
Iter  8000 | Total loss: 7055.4668 (MSE:0.5626, Reg:7054.9043) beta=15.50
Iter  9000 | Total loss: 5467.0688 (MSE:0.6120, Reg:5466.4570) beta=14.38
Iter 10000 | Total loss: 4452.7690 (MSE:0.5595, Reg:4452.2095) beta=13.25
Iter 11000 | Total loss: 3570.9062 (MSE:0.5668, Reg:3570.3394) beta=12.12
Iter 12000 | Total loss: 2829.9119 (MSE:0.5927, Reg:2829.3191) beta=11.00
Iter 13000 | Total loss: 2144.6807 (MSE:0.5237, Reg:2144.1570) beta=9.88
Iter 14000 | Total loss: 1503.3854 (MSE:0.6030, Reg:1502.7825) beta=8.75
Iter 15000 | Total loss: 939.7201 (MSE:0.5632, Reg:939.1569) beta=7.62
Iter 16000 | Total loss: 473.1273 (MSE:0.5690, Reg:472.5583) beta=6.50
Iter 17000 | Total loss: 139.3455 (MSE:0.5843, Reg:138.7611) beta=5.38
Iter 18000 | Total loss: 10.2307 (MSE:0.5957, Reg:9.6350) beta=4.25
Iter 19000 | Total loss: 0.5588 (MSE:0.5588, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.5303 (MSE:0.5303, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.4875 (MSE:0.4875, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4100 (MSE:0.4100, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.3838 (MSE:0.3838, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4382 (MSE:0.4382, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 88218.4844 (MSE:0.4101, Reg:88218.0781) beta=20.00
Iter  5000 | Total loss: 2277.1709 (MSE:0.5079, Reg:2276.6631) beta=18.88
Iter  6000 | Total loss: 1017.4141 (MSE:0.5358, Reg:1016.8784) beta=17.75
Iter  7000 | Total loss: 701.7812 (MSE:0.5694, Reg:701.2118) beta=16.62
Iter  8000 | Total loss: 484.4586 (MSE:0.5119, Reg:483.9467) beta=15.50
Iter  9000 | Total loss: 349.4210 (MSE:0.5766, Reg:348.8444) beta=14.38
Iter 10000 | Total loss: 272.5587 (MSE:0.4957, Reg:272.0630) beta=13.25
Iter 11000 | Total loss: 218.8280 (MSE:0.4853, Reg:218.3427) beta=12.12
Iter 12000 | Total loss: 171.3735 (MSE:0.4865, Reg:170.8870) beta=11.00
Iter 13000 | Total loss: 130.0796 (MSE:0.5184, Reg:129.5612) beta=9.88
Iter 14000 | Total loss: 95.1752 (MSE:0.4603, Reg:94.7149) beta=8.75
Iter 15000 | Total loss: 65.7517 (MSE:0.4984, Reg:65.2532) beta=7.62
Iter 16000 | Total loss: 35.3018 (MSE:0.5034, Reg:34.7984) beta=6.50
Iter 17000 | Total loss: 14.5829 (MSE:0.4562, Reg:14.1267) beta=5.38
Iter 18000 | Total loss: 3.7930 (MSE:0.4765, Reg:3.3165) beta=4.25
Iter 19000 | Total loss: 0.4884 (MSE:0.4884, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.5468 (MSE:0.5468, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 67.190%
Total time: 872.93 sec


Case: [ resnet18_AdaRound_NormQuantizer_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 873.9561 (MSE:0.0003, Reg:873.9558) beta=20.00
Iter  5000 | Total loss: 39.0003 (MSE:0.0003, Reg:39.0000) beta=18.88
Iter  6000 | Total loss: 30.0003 (MSE:0.0003, Reg:30.0000) beta=17.75
Iter  7000 | Total loss: 23.5225 (MSE:0.0003, Reg:23.5221) beta=16.62
Iter  8000 | Total loss: 21.0003 (MSE:0.0003, Reg:21.0000) beta=15.50
Iter  9000 | Total loss: 17.0003 (MSE:0.0003, Reg:17.0000) beta=14.38
Iter 10000 | Total loss: 12.0004 (MSE:0.0004, Reg:12.0000) beta=13.25
Iter 11000 | Total loss: 10.0004 (MSE:0.0004, Reg:10.0000) beta=12.12
Iter 12000 | Total loss: 6.0004 (MSE:0.0004, Reg:6.0000) beta=11.00
Iter 13000 | Total loss: 4.0003 (MSE:0.0003, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1904.6429 (MSE:0.0006, Reg:1904.6423) beta=20.00
Iter  5000 | Total loss: 66.0005 (MSE:0.0005, Reg:66.0000) beta=18.88
Iter  6000 | Total loss: 50.0001 (MSE:0.0006, Reg:49.9995) beta=17.75
Iter  7000 | Total loss: 38.7737 (MSE:0.0007, Reg:38.7730) beta=16.62
Iter  8000 | Total loss: 30.0007 (MSE:0.0007, Reg:30.0000) beta=15.50
Iter  9000 | Total loss: 27.0007 (MSE:0.0007, Reg:27.0000) beta=14.38
Iter 10000 | Total loss: 15.0005 (MSE:0.0005, Reg:15.0000) beta=13.25
Iter 11000 | Total loss: 12.0006 (MSE:0.0006, Reg:12.0000) beta=12.12
Iter 12000 | Total loss: 7.0007 (MSE:0.0007, Reg:7.0000) beta=11.00
Iter 13000 | Total loss: 4.0006 (MSE:0.0006, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 1.0006 (MSE:0.0006, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2561.7258 (MSE:0.0012, Reg:2561.7246) beta=20.00
Iter  5000 | Total loss: 261.0431 (MSE:0.0011, Reg:261.0420) beta=18.88
Iter  6000 | Total loss: 162.6498 (MSE:0.0011, Reg:162.6488) beta=17.75
Iter  7000 | Total loss: 117.3616 (MSE:0.0012, Reg:117.3604) beta=16.62
Iter  8000 | Total loss: 100.0012 (MSE:0.0012, Reg:100.0000) beta=15.50
Iter  9000 | Total loss: 71.4875 (MSE:0.0012, Reg:71.4863) beta=14.38
Iter 10000 | Total loss: 47.0012 (MSE:0.0012, Reg:47.0000) beta=13.25
Iter 11000 | Total loss: 27.1091 (MSE:0.0010, Reg:27.1081) beta=12.12
Iter 12000 | Total loss: 19.0012 (MSE:0.0012, Reg:19.0000) beta=11.00
Iter 13000 | Total loss: 8.0010 (MSE:0.0010, Reg:8.0000) beta=9.88
Iter 14000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2603.1465 (MSE:0.0013, Reg:2603.1453) beta=20.00
Iter  5000 | Total loss: 233.2582 (MSE:0.0014, Reg:233.2568) beta=18.88
Iter  6000 | Total loss: 183.9377 (MSE:0.0016, Reg:183.9361) beta=17.75
Iter  7000 | Total loss: 132.1866 (MSE:0.0012, Reg:132.1854) beta=16.62
Iter  8000 | Total loss: 92.0014 (MSE:0.0014, Reg:92.0000) beta=15.50
Iter  9000 | Total loss: 72.0011 (MSE:0.0011, Reg:72.0000) beta=14.38
Iter 10000 | Total loss: 45.0011 (MSE:0.0015, Reg:44.9996) beta=13.25
Iter 11000 | Total loss: 26.0014 (MSE:0.0013, Reg:26.0000) beta=12.12
Iter 12000 | Total loss: 13.0013 (MSE:0.0013, Reg:13.0000) beta=11.00
Iter 13000 | Total loss: 7.0013 (MSE:0.0013, Reg:7.0000) beta=9.88
Iter 14000 | Total loss: 2.0013 (MSE:0.0013, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4815.8564 (MSE:0.0040, Reg:4815.8525) beta=20.00
Iter  5000 | Total loss: 610.3909 (MSE:0.0041, Reg:610.3868) beta=18.88
Iter  6000 | Total loss: 472.0040 (MSE:0.0040, Reg:472.0000) beta=17.75
Iter  7000 | Total loss: 402.0011 (MSE:0.0045, Reg:401.9966) beta=16.62
Iter  8000 | Total loss: 339.0027 (MSE:0.0039, Reg:338.9988) beta=15.50
Iter  9000 | Total loss: 259.9436 (MSE:0.0038, Reg:259.9398) beta=14.38
Iter 10000 | Total loss: 179.8175 (MSE:0.0044, Reg:179.8132) beta=13.25
Iter 11000 | Total loss: 111.0039 (MSE:0.0040, Reg:111.0000) beta=12.12
Iter 12000 | Total loss: 68.0042 (MSE:0.0042, Reg:68.0000) beta=11.00
Iter 13000 | Total loss: 19.0043 (MSE:0.0043, Reg:19.0000) beta=9.88
Iter 14000 | Total loss: 9.0045 (MSE:0.0045, Reg:9.0000) beta=8.75
Iter 15000 | Total loss: 1.0043 (MSE:0.0043, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5444.2866 (MSE:0.0020, Reg:5444.2847) beta=20.00
Iter  5000 | Total loss: 395.2198 (MSE:0.0020, Reg:395.2178) beta=18.88
Iter  6000 | Total loss: 263.0019 (MSE:0.0019, Reg:263.0000) beta=17.75
Iter  7000 | Total loss: 235.0019 (MSE:0.0019, Reg:235.0000) beta=16.62
Iter  8000 | Total loss: 172.0020 (MSE:0.0020, Reg:172.0000) beta=15.50
Iter  9000 | Total loss: 140.0003 (MSE:0.0019, Reg:139.9984) beta=14.38
Iter 10000 | Total loss: 102.0003 (MSE:0.0021, Reg:101.9982) beta=13.25
Iter 11000 | Total loss: 71.9800 (MSE:0.0020, Reg:71.9780) beta=12.12
Iter 12000 | Total loss: 50.1350 (MSE:0.0022, Reg:50.1328) beta=11.00
Iter 13000 | Total loss: 20.0020 (MSE:0.0020, Reg:20.0000) beta=9.88
Iter 14000 | Total loss: 7.0020 (MSE:0.0020, Reg:7.0000) beta=8.75
Iter 15000 | Total loss: 2.0021 (MSE:0.0021, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 17919.2559 (MSE:0.0033, Reg:17919.2520) beta=20.00
Iter  5000 | Total loss: 1396.5646 (MSE:0.0033, Reg:1396.5613) beta=18.88
Iter  6000 | Total loss: 1082.0029 (MSE:0.0031, Reg:1081.9999) beta=17.75
Iter  7000 | Total loss: 895.3181 (MSE:0.0030, Reg:895.3151) beta=16.62
Iter  8000 | Total loss: 707.9954 (MSE:0.0032, Reg:707.9922) beta=15.50
Iter  9000 | Total loss: 592.0344 (MSE:0.0031, Reg:592.0314) beta=14.38
Iter 10000 | Total loss: 441.5971 (MSE:0.0034, Reg:441.5937) beta=13.25
Iter 11000 | Total loss: 284.9921 (MSE:0.0029, Reg:284.9891) beta=12.12
Iter 12000 | Total loss: 119.0463 (MSE:0.0031, Reg:119.0432) beta=11.00
Iter 13000 | Total loss: 27.0264 (MSE:0.0030, Reg:27.0234) beta=9.88
Iter 14000 | Total loss: 7.0032 (MSE:0.0032, Reg:7.0000) beta=8.75
Iter 15000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2029.5602 (MSE:0.0012, Reg:2029.5591) beta=20.00
Iter  5000 | Total loss: 130.0013 (MSE:0.0013, Reg:130.0000) beta=18.88
Iter  6000 | Total loss: 121.0013 (MSE:0.0013, Reg:121.0000) beta=17.75
Iter  7000 | Total loss: 107.0013 (MSE:0.0013, Reg:107.0000) beta=16.62
Iter  8000 | Total loss: 104.0014 (MSE:0.0014, Reg:104.0000) beta=15.50
Iter  9000 | Total loss: 83.9626 (MSE:0.0012, Reg:83.9614) beta=14.38
Iter 10000 | Total loss: 65.6641 (MSE:0.0014, Reg:65.6627) beta=13.25
Iter 11000 | Total loss: 48.0013 (MSE:0.0013, Reg:48.0000) beta=12.12
Iter 12000 | Total loss: 37.0014 (MSE:0.0014, Reg:37.0000) beta=11.00
Iter 13000 | Total loss: 18.0013 (MSE:0.0013, Reg:18.0000) beta=9.88
Iter 14000 | Total loss: 9.0012 (MSE:0.0012, Reg:9.0000) beta=8.75
Iter 15000 | Total loss: 5.0014 (MSE:0.0014, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 14580.6592 (MSE:0.0026, Reg:14580.6562) beta=20.00
Iter  5000 | Total loss: 1366.9846 (MSE:0.0027, Reg:1366.9819) beta=18.88
Iter  6000 | Total loss: 1055.4885 (MSE:0.0024, Reg:1055.4861) beta=17.75
Iter  7000 | Total loss: 864.1505 (MSE:0.0025, Reg:864.1480) beta=16.62
Iter  8000 | Total loss: 693.7189 (MSE:0.0025, Reg:693.7164) beta=15.50
Iter  9000 | Total loss: 550.8508 (MSE:0.0025, Reg:550.8483) beta=14.38
Iter 10000 | Total loss: 385.3277 (MSE:0.0025, Reg:385.3252) beta=13.25
Iter 11000 | Total loss: 187.9962 (MSE:0.0030, Reg:187.9933) beta=12.12
Iter 12000 | Total loss: 83.8255 (MSE:0.0024, Reg:83.8231) beta=11.00
Iter 13000 | Total loss: 41.0244 (MSE:0.0027, Reg:41.0217) beta=9.88
Iter 14000 | Total loss: 16.0020 (MSE:0.0026, Reg:15.9994) beta=8.75
Iter 15000 | Total loss: 1.0028 (MSE:0.0028, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30449.2676 (MSE:0.0030, Reg:30449.2637) beta=20.00
Iter  5000 | Total loss: 2990.6260 (MSE:0.0031, Reg:2990.6228) beta=18.88
Iter  6000 | Total loss: 2300.3552 (MSE:0.0027, Reg:2300.3525) beta=17.75
Iter  7000 | Total loss: 1887.6598 (MSE:0.0028, Reg:1887.6570) beta=16.62
Iter  8000 | Total loss: 1508.5038 (MSE:0.0028, Reg:1508.5010) beta=15.50
Iter  9000 | Total loss: 1144.7350 (MSE:0.0033, Reg:1144.7317) beta=14.38
Iter 10000 | Total loss: 800.5803 (MSE:0.0030, Reg:800.5773) beta=13.25
Iter 11000 | Total loss: 515.5556 (MSE:0.0028, Reg:515.5527) beta=12.12
Iter 12000 | Total loss: 301.1711 (MSE:0.0030, Reg:301.1680) beta=11.00
Iter 13000 | Total loss: 128.6887 (MSE:0.0030, Reg:128.6857) beta=9.88
Iter 14000 | Total loss: 37.0028 (MSE:0.0028, Reg:37.0000) beta=8.75
Iter 15000 | Total loss: 5.0030 (MSE:0.0030, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 31457.8926 (MSE:0.0023, Reg:31457.8906) beta=20.00
Iter  5000 | Total loss: 2557.4473 (MSE:0.0023, Reg:2557.4451) beta=18.88
Iter  6000 | Total loss: 1893.5896 (MSE:0.0022, Reg:1893.5874) beta=17.75
Iter  7000 | Total loss: 1487.6632 (MSE:0.0023, Reg:1487.6609) beta=16.62
Iter  8000 | Total loss: 1183.1732 (MSE:0.0023, Reg:1183.1709) beta=15.50
Iter  9000 | Total loss: 952.0903 (MSE:0.0024, Reg:952.0880) beta=14.38
Iter 10000 | Total loss: 672.5666 (MSE:0.0024, Reg:672.5642) beta=13.25
Iter 11000 | Total loss: 407.0940 (MSE:0.0025, Reg:407.0915) beta=12.12
Iter 12000 | Total loss: 207.0003 (MSE:0.0024, Reg:206.9979) beta=11.00
Iter 13000 | Total loss: 88.7927 (MSE:0.0024, Reg:88.7903) beta=9.88
Iter 14000 | Total loss: 23.0024 (MSE:0.0024, Reg:23.0000) beta=8.75
Iter 15000 | Total loss: 3.7704 (MSE:0.0023, Reg:3.7681) beta=7.62
Iter 16000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 69830.5000 (MSE:0.0026, Reg:69830.5000) beta=20.00
Iter  5000 | Total loss: 5221.5869 (MSE:0.0028, Reg:5221.5840) beta=18.88
Iter  6000 | Total loss: 3678.8240 (MSE:0.0024, Reg:3678.8215) beta=17.75
Iter  7000 | Total loss: 2766.6733 (MSE:0.0027, Reg:2766.6707) beta=16.62
Iter  8000 | Total loss: 2198.5930 (MSE:0.0027, Reg:2198.5903) beta=15.50
Iter  9000 | Total loss: 1622.9229 (MSE:0.0027, Reg:1622.9202) beta=14.38
Iter 10000 | Total loss: 1113.5895 (MSE:0.0027, Reg:1113.5867) beta=13.25
Iter 11000 | Total loss: 726.2449 (MSE:0.0027, Reg:726.2422) beta=12.12
Iter 12000 | Total loss: 369.9343 (MSE:0.0027, Reg:369.9316) beta=11.00
Iter 13000 | Total loss: 162.1947 (MSE:0.0026, Reg:162.1921) beta=9.88
Iter 14000 | Total loss: 43.9284 (MSE:0.0026, Reg:43.9257) beta=8.75
Iter 15000 | Total loss: 7.0024 (MSE:0.0024, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9893.2129 (MSE:0.0003, Reg:9893.2129) beta=20.00
Iter  5000 | Total loss: 669.5402 (MSE:0.0003, Reg:669.5399) beta=18.88
Iter  6000 | Total loss: 543.7994 (MSE:0.0003, Reg:543.7991) beta=17.75
Iter  7000 | Total loss: 474.9903 (MSE:0.0003, Reg:474.9900) beta=16.62
Iter  8000 | Total loss: 373.0940 (MSE:0.0003, Reg:373.0937) beta=15.50
Iter  9000 | Total loss: 258.7628 (MSE:0.0003, Reg:258.7625) beta=14.38
Iter 10000 | Total loss: 183.8379 (MSE:0.0003, Reg:183.8376) beta=13.25
Iter 11000 | Total loss: 121.2507 (MSE:0.0003, Reg:121.2504) beta=12.12
Iter 12000 | Total loss: 85.7050 (MSE:0.0003, Reg:85.7046) beta=11.00
Iter 13000 | Total loss: 55.0003 (MSE:0.0003, Reg:55.0000) beta=9.88
Iter 14000 | Total loss: 22.1968 (MSE:0.0003, Reg:22.1965) beta=8.75
Iter 15000 | Total loss: 7.0003 (MSE:0.0003, Reg:7.0000) beta=7.62
Iter 16000 | Total loss: 2.0003 (MSE:0.0003, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.9133 (MSE:0.0003, Reg:0.9130) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 56657.7344 (MSE:0.0016, Reg:56657.7344) beta=20.00
Iter  5000 | Total loss: 3051.5447 (MSE:0.0018, Reg:3051.5430) beta=18.88
Iter  6000 | Total loss: 1979.6949 (MSE:0.0016, Reg:1979.6934) beta=17.75
Iter  7000 | Total loss: 1510.9999 (MSE:0.0016, Reg:1510.9983) beta=16.62
Iter  8000 | Total loss: 1175.3635 (MSE:0.0017, Reg:1175.3618) beta=15.50
Iter  9000 | Total loss: 865.2768 (MSE:0.0017, Reg:865.2751) beta=14.38
Iter 10000 | Total loss: 635.7314 (MSE:0.0016, Reg:635.7298) beta=13.25
Iter 11000 | Total loss: 418.9313 (MSE:0.0016, Reg:418.9297) beta=12.12
Iter 12000 | Total loss: 230.7705 (MSE:0.0017, Reg:230.7688) beta=11.00
Iter 13000 | Total loss: 108.9914 (MSE:0.0017, Reg:108.9898) beta=9.88
Iter 14000 | Total loss: 39.0421 (MSE:0.0016, Reg:39.0405) beta=8.75
Iter 15000 | Total loss: 8.0007 (MSE:0.0017, Reg:7.9990) beta=7.62
Iter 16000 | Total loss: 1.0011 (MSE:0.0017, Reg:0.9993) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 104413.6641 (MSE:0.0020, Reg:104413.6641) beta=20.00
Iter  5000 | Total loss: 4984.0615 (MSE:0.0021, Reg:4984.0596) beta=18.88
Iter  6000 | Total loss: 3211.1887 (MSE:0.0021, Reg:3211.1868) beta=17.75
Iter  7000 | Total loss: 2416.4509 (MSE:0.0021, Reg:2416.4487) beta=16.62
Iter  8000 | Total loss: 1879.7555 (MSE:0.0020, Reg:1879.7534) beta=15.50
Iter  9000 | Total loss: 1409.7195 (MSE:0.0021, Reg:1409.7174) beta=14.38
Iter 10000 | Total loss: 1043.3212 (MSE:0.0020, Reg:1043.3192) beta=13.25
Iter 11000 | Total loss: 710.9225 (MSE:0.0021, Reg:710.9203) beta=12.12
Iter 12000 | Total loss: 429.9155 (MSE:0.0022, Reg:429.9133) beta=11.00
Iter 13000 | Total loss: 200.8559 (MSE:0.0021, Reg:200.8538) beta=9.88
Iter 14000 | Total loss: 67.3753 (MSE:0.0020, Reg:67.3734) beta=8.75
Iter 15000 | Total loss: 14.0021 (MSE:0.0021, Reg:14.0000) beta=7.62
Iter 16000 | Total loss: 3.0021 (MSE:0.0021, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 136261.3281 (MSE:0.0016, Reg:136261.3281) beta=20.00
Iter  5000 | Total loss: 3365.9509 (MSE:0.0017, Reg:3365.9492) beta=18.88
Iter  6000 | Total loss: 1947.7161 (MSE:0.0018, Reg:1947.7144) beta=17.75
Iter  7000 | Total loss: 1379.5955 (MSE:0.0018, Reg:1379.5938) beta=16.62
Iter  8000 | Total loss: 1003.6168 (MSE:0.0017, Reg:1003.6151) beta=15.50
Iter  9000 | Total loss: 746.9604 (MSE:0.0017, Reg:746.9587) beta=14.38
Iter 10000 | Total loss: 527.6805 (MSE:0.0018, Reg:527.6788) beta=13.25
Iter 11000 | Total loss: 345.5407 (MSE:0.0018, Reg:345.5390) beta=12.12
Iter 12000 | Total loss: 213.8128 (MSE:0.0017, Reg:213.8111) beta=11.00
Iter 13000 | Total loss: 114.9428 (MSE:0.0017, Reg:114.9411) beta=9.88
Iter 14000 | Total loss: 35.3122 (MSE:0.0018, Reg:35.3104) beta=8.75
Iter 15000 | Total loss: 6.8099 (MSE:0.0017, Reg:6.8083) beta=7.62
Iter 16000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 260989.2188 (MSE:0.0054, Reg:260989.2188) beta=20.00
Iter  5000 | Total loss: 14123.0107 (MSE:0.0057, Reg:14123.0049) beta=18.88
Iter  6000 | Total loss: 9459.6025 (MSE:0.0055, Reg:9459.5967) beta=17.75
Iter  7000 | Total loss: 6913.0903 (MSE:0.0057, Reg:6913.0845) beta=16.62
Iter  8000 | Total loss: 5261.1875 (MSE:0.0057, Reg:5261.1816) beta=15.50
Iter  9000 | Total loss: 3909.9568 (MSE:0.0054, Reg:3909.9514) beta=14.38
Iter 10000 | Total loss: 2750.8870 (MSE:0.0056, Reg:2750.8813) beta=13.25
Iter 11000 | Total loss: 1768.4946 (MSE:0.0056, Reg:1768.4890) beta=12.12
Iter 12000 | Total loss: 921.9617 (MSE:0.0056, Reg:921.9561) beta=11.00
Iter 13000 | Total loss: 358.9388 (MSE:0.0057, Reg:358.9330) beta=9.88
Iter 14000 | Total loss: 88.7305 (MSE:0.0054, Reg:88.7251) beta=8.75
Iter 15000 | Total loss: 8.0016 (MSE:0.0055, Reg:7.9960) beta=7.62
Iter 16000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30086.3145 (MSE:0.0019, Reg:30086.3125) beta=20.00
Iter  5000 | Total loss: 2111.8054 (MSE:0.0023, Reg:2111.8032) beta=18.88
Iter  6000 | Total loss: 1709.2411 (MSE:0.0021, Reg:1709.2390) beta=17.75
Iter  7000 | Total loss: 1463.1531 (MSE:0.0022, Reg:1463.1509) beta=16.62
Iter  8000 | Total loss: 1202.0891 (MSE:0.0022, Reg:1202.0869) beta=15.50
Iter  9000 | Total loss: 963.6704 (MSE:0.0021, Reg:963.6683) beta=14.38
Iter 10000 | Total loss: 692.0858 (MSE:0.0023, Reg:692.0835) beta=13.25
Iter 11000 | Total loss: 441.4505 (MSE:0.0022, Reg:441.4483) beta=12.12
Iter 12000 | Total loss: 224.3560 (MSE:0.0021, Reg:224.3538) beta=11.00
Iter 13000 | Total loss: 88.5942 (MSE:0.0021, Reg:88.5920) beta=9.88
Iter 14000 | Total loss: 31.4064 (MSE:0.0021, Reg:31.4042) beta=8.75
Iter 15000 | Total loss: 3.0024 (MSE:0.0024, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 324564.3438 (MSE:0.0027, Reg:324564.3438) beta=20.00
Iter  5000 | Total loss: 1754.8235 (MSE:0.0027, Reg:1754.8208) beta=18.88
Iter  6000 | Total loss: 1066.2939 (MSE:0.0030, Reg:1066.2910) beta=17.75
Iter  7000 | Total loss: 793.5673 (MSE:0.0028, Reg:793.5645) beta=16.62
Iter  8000 | Total loss: 572.9863 (MSE:0.0028, Reg:572.9835) beta=15.50
Iter  9000 | Total loss: 423.8482 (MSE:0.0029, Reg:423.8453) beta=14.38
Iter 10000 | Total loss: 298.5541 (MSE:0.0029, Reg:298.5512) beta=13.25
Iter 11000 | Total loss: 211.9990 (MSE:0.0029, Reg:211.9961) beta=12.12
Iter 12000 | Total loss: 135.5508 (MSE:0.0028, Reg:135.5479) beta=11.00
Iter 13000 | Total loss: 72.7894 (MSE:0.0028, Reg:72.7867) beta=9.88
Iter 14000 | Total loss: 23.8137 (MSE:0.0027, Reg:23.8110) beta=8.75
Iter 15000 | Total loss: 3.5158 (MSE:0.0029, Reg:3.5129) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2179 (MSE:0.2179, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1889 (MSE:0.1889, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1863 (MSE:0.1863, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1734 (MSE:0.1734, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 207413.4844 (MSE:0.1748, Reg:207413.3125) beta=20.00
Iter  5000 | Total loss: 38205.6836 (MSE:0.1770, Reg:38205.5078) beta=18.88
Iter  6000 | Total loss: 26010.9531 (MSE:0.1772, Reg:26010.7754) beta=17.75
Iter  7000 | Total loss: 17572.7832 (MSE:0.1860, Reg:17572.5977) beta=16.62
Iter  8000 | Total loss: 11518.6680 (MSE:0.1719, Reg:11518.4961) beta=15.50
Iter  9000 | Total loss: 6835.6313 (MSE:0.1780, Reg:6835.4531) beta=14.38
Iter 10000 | Total loss: 3380.2234 (MSE:0.1729, Reg:3380.0505) beta=13.25
Iter 11000 | Total loss: 1269.5781 (MSE:0.1759, Reg:1269.4022) beta=12.12
Iter 12000 | Total loss: 339.0636 (MSE:0.1864, Reg:338.8772) beta=11.00
Iter 13000 | Total loss: 51.6676 (MSE:0.1717, Reg:51.4959) beta=9.88
Iter 14000 | Total loss: 3.1839 (MSE:0.1839, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.1782 (MSE:0.1782, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1777 (MSE:0.1777, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1780 (MSE:0.1780, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1849 (MSE:0.1849, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1779 (MSE:0.1779, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1777 (MSE:0.1777, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1791 (MSE:0.1791, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1242 (MSE:0.1242, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1263 (MSE:0.1263, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1222 (MSE:0.1222, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 34227.5859 (MSE:0.1190, Reg:34227.4688) beta=20.00
Iter  5000 | Total loss: 6746.7051 (MSE:0.1278, Reg:6746.5771) beta=18.88
Iter  6000 | Total loss: 5010.4722 (MSE:0.1396, Reg:5010.3325) beta=17.75
Iter  7000 | Total loss: 3674.0981 (MSE:0.1360, Reg:3673.9622) beta=16.62
Iter  8000 | Total loss: 2622.6609 (MSE:0.1344, Reg:2622.5266) beta=15.50
Iter  9000 | Total loss: 1704.2251 (MSE:0.1265, Reg:1704.0986) beta=14.38
Iter 10000 | Total loss: 961.6129 (MSE:0.1148, Reg:961.4980) beta=13.25
Iter 11000 | Total loss: 431.3720 (MSE:0.1210, Reg:431.2510) beta=12.12
Iter 12000 | Total loss: 143.9608 (MSE:0.1230, Reg:143.8378) beta=11.00
Iter 13000 | Total loss: 29.1355 (MSE:0.1355, Reg:29.0000) beta=9.88
Iter 14000 | Total loss: 7.1186 (MSE:0.1187, Reg:6.9998) beta=8.75
Iter 15000 | Total loss: 0.1320 (MSE:0.1320, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1218 (MSE:0.1218, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1259 (MSE:0.1259, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1208 (MSE:0.1208, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1270 (MSE:0.1270, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1320 (MSE:0.1320, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 68.994%
Total time: 877.33 sec


Case: [ resnet18_AdaRound_NormQuantizer_CH_W4A4_BNFold_p2.4_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - AdaRound: True

- activation params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: False
    - p: 2.4

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 914.4314 (MSE:0.0013, Reg:914.4301) beta=20.00
Iter  5000 | Total loss: 36.0004 (MSE:0.0011, Reg:35.9992) beta=18.88
Iter  6000 | Total loss: 22.0015 (MSE:0.0015, Reg:22.0000) beta=17.75
Iter  7000 | Total loss: 21.9443 (MSE:0.0013, Reg:21.9430) beta=16.62
Iter  8000 | Total loss: 15.0012 (MSE:0.0012, Reg:15.0000) beta=15.50
Iter  9000 | Total loss: 15.0013 (MSE:0.0013, Reg:15.0000) beta=14.38
Iter 10000 | Total loss: 11.0012 (MSE:0.0012, Reg:11.0000) beta=13.25
Iter 11000 | Total loss: 8.0012 (MSE:0.0012, Reg:8.0000) beta=12.12
Iter 12000 | Total loss: 6.0014 (MSE:0.0014, Reg:6.0000) beta=11.00
Iter 13000 | Total loss: 5.0012 (MSE:0.0012, Reg:5.0000) beta=9.88
Iter 14000 | Total loss: 2.0013 (MSE:0.0013, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 1.0013 (MSE:0.0013, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2055.3215 (MSE:0.0010, Reg:2055.3206) beta=20.00
Iter  5000 | Total loss: 85.0011 (MSE:0.0011, Reg:85.0000) beta=18.88
Iter  6000 | Total loss: 51.0012 (MSE:0.0012, Reg:50.9999) beta=17.75
Iter  7000 | Total loss: 40.9993 (MSE:0.0010, Reg:40.9983) beta=16.62
Iter  8000 | Total loss: 29.0010 (MSE:0.0010, Reg:29.0000) beta=15.50
Iter  9000 | Total loss: 17.0010 (MSE:0.0010, Reg:17.0000) beta=14.38
Iter 10000 | Total loss: 11.0010 (MSE:0.0010, Reg:11.0000) beta=13.25
Iter 11000 | Total loss: 9.0014 (MSE:0.0014, Reg:9.0000) beta=12.12
Iter 12000 | Total loss: 9.0012 (MSE:0.0012, Reg:9.0000) beta=11.00
Iter 13000 | Total loss: 7.0011 (MSE:0.0011, Reg:7.0000) beta=9.88
Iter 14000 | Total loss: 4.0010 (MSE:0.0010, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 1.0011 (MSE:0.0011, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0091 (MSE:0.0091, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2335.9180 (MSE:0.0084, Reg:2335.9097) beta=20.00
Iter  5000 | Total loss: 277.8068 (MSE:0.0090, Reg:277.7979) beta=18.88
Iter  6000 | Total loss: 184.0077 (MSE:0.0079, Reg:183.9998) beta=17.75
Iter  7000 | Total loss: 142.0064 (MSE:0.0090, Reg:141.9973) beta=16.62
Iter  8000 | Total loss: 104.0052 (MSE:0.0081, Reg:103.9972) beta=15.50
Iter  9000 | Total loss: 59.0094 (MSE:0.0094, Reg:59.0000) beta=14.38
Iter 10000 | Total loss: 33.0084 (MSE:0.0084, Reg:33.0000) beta=13.25
Iter 11000 | Total loss: 22.0083 (MSE:0.0083, Reg:22.0000) beta=12.12
Iter 12000 | Total loss: 13.0083 (MSE:0.0083, Reg:13.0000) beta=11.00
Iter 13000 | Total loss: 7.8999 (MSE:0.0087, Reg:7.8912) beta=9.88
Iter 14000 | Total loss: 3.0088 (MSE:0.0088, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2231.9546 (MSE:0.0024, Reg:2231.9521) beta=20.00
Iter  5000 | Total loss: 217.3184 (MSE:0.0028, Reg:217.3156) beta=18.88
Iter  6000 | Total loss: 148.1533 (MSE:0.0027, Reg:148.1506) beta=17.75
Iter  7000 | Total loss: 113.0026 (MSE:0.0026, Reg:113.0000) beta=16.62
Iter  8000 | Total loss: 89.0000 (MSE:0.0027, Reg:88.9972) beta=15.50
Iter  9000 | Total loss: 65.9631 (MSE:0.0026, Reg:65.9605) beta=14.38
Iter 10000 | Total loss: 37.7294 (MSE:0.0028, Reg:37.7267) beta=13.25
Iter 11000 | Total loss: 21.0026 (MSE:0.0026, Reg:21.0000) beta=12.12
Iter 12000 | Total loss: 9.0025 (MSE:0.0025, Reg:9.0000) beta=11.00
Iter 13000 | Total loss: 6.0027 (MSE:0.0027, Reg:6.0000) beta=9.88
Iter 14000 | Total loss: 1.0026 (MSE:0.0026, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0318 (MSE:0.0318, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0304 (MSE:0.0304, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0266 (MSE:0.0266, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0253 (MSE:0.0253, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4310.9634 (MSE:0.0279, Reg:4310.9355) beta=20.00
Iter  5000 | Total loss: 743.2046 (MSE:0.0284, Reg:743.1761) beta=18.88
Iter  6000 | Total loss: 568.6288 (MSE:0.0284, Reg:568.6004) beta=17.75
Iter  7000 | Total loss: 457.0179 (MSE:0.0268, Reg:456.9911) beta=16.62
Iter  8000 | Total loss: 357.7193 (MSE:0.0284, Reg:357.6909) beta=15.50
Iter  9000 | Total loss: 263.7815 (MSE:0.0272, Reg:263.7543) beta=14.38
Iter 10000 | Total loss: 162.3849 (MSE:0.0287, Reg:162.3563) beta=13.25
Iter 11000 | Total loss: 81.7673 (MSE:0.0281, Reg:81.7392) beta=12.12
Iter 12000 | Total loss: 37.0275 (MSE:0.0275, Reg:37.0000) beta=11.00
Iter 13000 | Total loss: 19.0268 (MSE:0.0279, Reg:18.9989) beta=9.88
Iter 14000 | Total loss: 4.0308 (MSE:0.0308, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.0280 (MSE:0.0280, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0273 (MSE:0.0273, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0280 (MSE:0.0280, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0302 (MSE:0.0302, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0274 (MSE:0.0274, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0283 (MSE:0.0283, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5072.2432 (MSE:0.0045, Reg:5072.2388) beta=20.00
Iter  5000 | Total loss: 658.7311 (MSE:0.0044, Reg:658.7267) beta=18.88
Iter  6000 | Total loss: 462.0038 (MSE:0.0045, Reg:461.9993) beta=17.75
Iter  7000 | Total loss: 360.7675 (MSE:0.0045, Reg:360.7630) beta=16.62
Iter  8000 | Total loss: 259.8718 (MSE:0.0042, Reg:259.8676) beta=15.50
Iter  9000 | Total loss: 154.9464 (MSE:0.0045, Reg:154.9418) beta=14.38
Iter 10000 | Total loss: 93.8876 (MSE:0.0046, Reg:93.8831) beta=13.25
Iter 11000 | Total loss: 53.7998 (MSE:0.0041, Reg:53.7957) beta=12.12
Iter 12000 | Total loss: 24.0045 (MSE:0.0045, Reg:24.0000) beta=11.00
Iter 13000 | Total loss: 9.0045 (MSE:0.0045, Reg:9.0000) beta=9.88
Iter 14000 | Total loss: 1.0041 (MSE:0.0041, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 1.0027 (MSE:0.0044, Reg:0.9982) beta=7.62
Iter 16000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0193 (MSE:0.0193, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0161 (MSE:0.0161, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0163 (MSE:0.0163, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15014.0703 (MSE:0.0168, Reg:15014.0537) beta=20.00
Iter  5000 | Total loss: 1656.5787 (MSE:0.0157, Reg:1656.5630) beta=18.88
Iter  6000 | Total loss: 1269.2648 (MSE:0.0161, Reg:1269.2487) beta=17.75
Iter  7000 | Total loss: 1004.3348 (MSE:0.0164, Reg:1004.3184) beta=16.62
Iter  8000 | Total loss: 813.9567 (MSE:0.0167, Reg:813.9400) beta=15.50
Iter  9000 | Total loss: 650.8128 (MSE:0.0160, Reg:650.7968) beta=14.38
Iter 10000 | Total loss: 406.9025 (MSE:0.0171, Reg:406.8853) beta=13.25
Iter 11000 | Total loss: 208.5431 (MSE:0.0159, Reg:208.5272) beta=12.12
Iter 12000 | Total loss: 93.2147 (MSE:0.0164, Reg:93.1983) beta=11.00
Iter 13000 | Total loss: 18.0178 (MSE:0.0178, Reg:18.0000) beta=9.88
Iter 14000 | Total loss: 4.0157 (MSE:0.0157, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0170 (MSE:0.0170, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0165 (MSE:0.0165, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0170 (MSE:0.0170, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0091 (MSE:0.0091, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1718.6851 (MSE:0.0076, Reg:1718.6775) beta=20.00
Iter  5000 | Total loss: 184.9243 (MSE:0.0073, Reg:184.9171) beta=18.88
Iter  6000 | Total loss: 153.0083 (MSE:0.0083, Reg:153.0000) beta=17.75
Iter  7000 | Total loss: 130.0076 (MSE:0.0076, Reg:130.0000) beta=16.62
Iter  8000 | Total loss: 103.0079 (MSE:0.0079, Reg:103.0000) beta=15.50
Iter  9000 | Total loss: 78.0079 (MSE:0.0079, Reg:78.0000) beta=14.38
Iter 10000 | Total loss: 59.9987 (MSE:0.0087, Reg:59.9900) beta=13.25
Iter 11000 | Total loss: 39.0076 (MSE:0.0076, Reg:39.0000) beta=12.12
Iter 12000 | Total loss: 23.0080 (MSE:0.0080, Reg:23.0000) beta=11.00
Iter 13000 | Total loss: 10.0077 (MSE:0.0077, Reg:10.0000) beta=9.88
Iter 14000 | Total loss: 7.0082 (MSE:0.0082, Reg:7.0000) beta=8.75
Iter 15000 | Total loss: 1.0079 (MSE:0.0079, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15951.5830 (MSE:0.0029, Reg:15951.5801) beta=20.00
Iter  5000 | Total loss: 1954.0098 (MSE:0.0027, Reg:1954.0071) beta=18.88
Iter  6000 | Total loss: 1356.9054 (MSE:0.0028, Reg:1356.9026) beta=17.75
Iter  7000 | Total loss: 995.5207 (MSE:0.0026, Reg:995.5181) beta=16.62
Iter  8000 | Total loss: 737.5916 (MSE:0.0026, Reg:737.5890) beta=15.50
Iter  9000 | Total loss: 533.4791 (MSE:0.0027, Reg:533.4764) beta=14.38
Iter 10000 | Total loss: 326.7954 (MSE:0.0029, Reg:326.7925) beta=13.25
Iter 11000 | Total loss: 184.7324 (MSE:0.0027, Reg:184.7297) beta=12.12
Iter 12000 | Total loss: 71.8493 (MSE:0.0029, Reg:71.8464) beta=11.00
Iter 13000 | Total loss: 20.0027 (MSE:0.0027, Reg:19.9999) beta=9.88
Iter 14000 | Total loss: 2.0027 (MSE:0.0028, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0171 (MSE:0.0171, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0152 (MSE:0.0152, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0160 (MSE:0.0160, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0154 (MSE:0.0154, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 23488.8906 (MSE:0.0157, Reg:23488.8750) beta=20.00
Iter  5000 | Total loss: 3001.0984 (MSE:0.0154, Reg:3001.0830) beta=18.88
Iter  6000 | Total loss: 2206.0498 (MSE:0.0168, Reg:2206.0330) beta=17.75
Iter  7000 | Total loss: 1733.1569 (MSE:0.0164, Reg:1733.1405) beta=16.62
Iter  8000 | Total loss: 1356.4312 (MSE:0.0157, Reg:1356.4154) beta=15.50
Iter  9000 | Total loss: 1028.8601 (MSE:0.0151, Reg:1028.8450) beta=14.38
Iter 10000 | Total loss: 678.5135 (MSE:0.0161, Reg:678.4974) beta=13.25
Iter 11000 | Total loss: 386.1936 (MSE:0.0160, Reg:386.1776) beta=12.12
Iter 12000 | Total loss: 184.5640 (MSE:0.0164, Reg:184.5476) beta=11.00
Iter 13000 | Total loss: 62.0162 (MSE:0.0162, Reg:62.0000) beta=9.88
Iter 14000 | Total loss: 19.0178 (MSE:0.0151, Reg:19.0027) beta=8.75
Iter 15000 | Total loss: 3.0152 (MSE:0.0152, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 1.0144 (MSE:0.0144, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0170 (MSE:0.0170, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 28324.5977 (MSE:0.0040, Reg:28324.5938) beta=20.00
Iter  5000 | Total loss: 3351.9563 (MSE:0.0039, Reg:3351.9524) beta=18.88
Iter  6000 | Total loss: 2360.2588 (MSE:0.0040, Reg:2360.2549) beta=17.75
Iter  7000 | Total loss: 1744.7904 (MSE:0.0038, Reg:1744.7866) beta=16.62
Iter  8000 | Total loss: 1276.6997 (MSE:0.0039, Reg:1276.6958) beta=15.50
Iter  9000 | Total loss: 918.8249 (MSE:0.0039, Reg:918.8210) beta=14.38
Iter 10000 | Total loss: 601.3531 (MSE:0.0038, Reg:601.3492) beta=13.25
Iter 11000 | Total loss: 329.5287 (MSE:0.0040, Reg:329.5247) beta=12.12
Iter 12000 | Total loss: 160.6797 (MSE:0.0039, Reg:160.6758) beta=11.00
Iter 13000 | Total loss: 56.0896 (MSE:0.0040, Reg:56.0856) beta=9.88
Iter 14000 | Total loss: 12.7005 (MSE:0.0040, Reg:12.6965) beta=8.75
Iter 15000 | Total loss: 1.0038 (MSE:0.0038, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0152 (MSE:0.0152, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0129 (MSE:0.0129, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0143 (MSE:0.0143, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 60251.0703 (MSE:0.0136, Reg:60251.0586) beta=20.00
Iter  5000 | Total loss: 7026.0391 (MSE:0.0135, Reg:7026.0254) beta=18.88
Iter  6000 | Total loss: 4855.0229 (MSE:0.0142, Reg:4855.0088) beta=17.75
Iter  7000 | Total loss: 3554.7803 (MSE:0.0142, Reg:3554.7661) beta=16.62
Iter  8000 | Total loss: 2596.6213 (MSE:0.0138, Reg:2596.6077) beta=15.50
Iter  9000 | Total loss: 1754.3383 (MSE:0.0131, Reg:1754.3251) beta=14.38
Iter 10000 | Total loss: 1095.1184 (MSE:0.0127, Reg:1095.1057) beta=13.25
Iter 11000 | Total loss: 579.6652 (MSE:0.0130, Reg:579.6522) beta=12.12
Iter 12000 | Total loss: 229.8777 (MSE:0.0133, Reg:229.8644) beta=11.00
Iter 13000 | Total loss: 62.2835 (MSE:0.0138, Reg:62.2697) beta=9.88
Iter 14000 | Total loss: 12.0106 (MSE:0.0138, Reg:11.9968) beta=8.75
Iter 15000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0143 (MSE:0.0143, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0133 (MSE:0.0133, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0140 (MSE:0.0140, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0143 (MSE:0.0143, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8290.9521 (MSE:0.0016, Reg:8290.9502) beta=20.00
Iter  5000 | Total loss: 777.8335 (MSE:0.0017, Reg:777.8317) beta=18.88
Iter  6000 | Total loss: 631.2133 (MSE:0.0017, Reg:631.2115) beta=17.75
Iter  7000 | Total loss: 526.4481 (MSE:0.0017, Reg:526.4464) beta=16.62
Iter  8000 | Total loss: 455.9659 (MSE:0.0017, Reg:455.9642) beta=15.50
Iter  9000 | Total loss: 345.1088 (MSE:0.0017, Reg:345.1071) beta=14.38
Iter 10000 | Total loss: 258.9667 (MSE:0.0017, Reg:258.9650) beta=13.25
Iter 11000 | Total loss: 170.9938 (MSE:0.0016, Reg:170.9922) beta=12.12
Iter 12000 | Total loss: 103.4426 (MSE:0.0017, Reg:103.4409) beta=11.00
Iter 13000 | Total loss: 49.0018 (MSE:0.0018, Reg:49.0000) beta=9.88
Iter 14000 | Total loss: 24.0015 (MSE:0.0017, Reg:23.9999) beta=8.75
Iter 15000 | Total loss: 5.0017 (MSE:0.0017, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 1.0016 (MSE:0.0016, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 67977.3438 (MSE:0.0015, Reg:67977.3438) beta=20.00
Iter  5000 | Total loss: 2932.8149 (MSE:0.0014, Reg:2932.8135) beta=18.88
Iter  6000 | Total loss: 1598.1700 (MSE:0.0016, Reg:1598.1685) beta=17.75
Iter  7000 | Total loss: 1024.8356 (MSE:0.0014, Reg:1024.8341) beta=16.62
Iter  8000 | Total loss: 712.1451 (MSE:0.0013, Reg:712.1439) beta=15.50
Iter  9000 | Total loss: 498.8442 (MSE:0.0014, Reg:498.8428) beta=14.38
Iter 10000 | Total loss: 338.8063 (MSE:0.0015, Reg:338.8049) beta=13.25
Iter 11000 | Total loss: 201.9815 (MSE:0.0012, Reg:201.9803) beta=12.12
Iter 12000 | Total loss: 117.3894 (MSE:0.0015, Reg:117.3879) beta=11.00
Iter 13000 | Total loss: 46.0920 (MSE:0.0015, Reg:46.0906) beta=9.88
Iter 14000 | Total loss: 12.0014 (MSE:0.0014, Reg:12.0000) beta=8.75
Iter 15000 | Total loss: 1.0013 (MSE:0.0013, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 1.0014 (MSE:0.0014, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0119 (MSE:0.0119, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 88803.3984 (MSE:0.0122, Reg:88803.3828) beta=20.00
Iter  5000 | Total loss: 8701.3418 (MSE:0.0120, Reg:8701.3301) beta=18.88
Iter  6000 | Total loss: 5638.4087 (MSE:0.0114, Reg:5638.3975) beta=17.75
Iter  7000 | Total loss: 3977.8674 (MSE:0.0118, Reg:3977.8557) beta=16.62
Iter  8000 | Total loss: 2818.2515 (MSE:0.0127, Reg:2818.2388) beta=15.50
Iter  9000 | Total loss: 1916.4563 (MSE:0.0120, Reg:1916.4443) beta=14.38
Iter 10000 | Total loss: 1247.6874 (MSE:0.0123, Reg:1247.6750) beta=13.25
Iter 11000 | Total loss: 729.1967 (MSE:0.0113, Reg:729.1854) beta=12.12
Iter 12000 | Total loss: 377.6045 (MSE:0.0127, Reg:377.5918) beta=11.00
Iter 13000 | Total loss: 155.1300 (MSE:0.0123, Reg:155.1177) beta=9.88
Iter 14000 | Total loss: 45.0123 (MSE:0.0123, Reg:45.0000) beta=8.75
Iter 15000 | Total loss: 6.9898 (MSE:0.0123, Reg:6.9775) beta=7.62
Iter 16000 | Total loss: 0.6452 (MSE:0.0110, Reg:0.6343) beta=6.50
Iter 17000 | Total loss: 0.0127 (MSE:0.0127, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0112 (MSE:0.0112, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0122 (MSE:0.0122, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 171072.8125 (MSE:0.0014, Reg:171072.8125) beta=20.00
Iter  5000 | Total loss: 3060.1548 (MSE:0.0017, Reg:3060.1531) beta=18.88
Iter  6000 | Total loss: 1314.5919 (MSE:0.0016, Reg:1314.5903) beta=17.75
Iter  7000 | Total loss: 778.0444 (MSE:0.0015, Reg:778.0429) beta=16.62
Iter  8000 | Total loss: 521.9986 (MSE:0.0015, Reg:521.9971) beta=15.50
Iter  9000 | Total loss: 354.3580 (MSE:0.0013, Reg:354.3566) beta=14.38
Iter 10000 | Total loss: 235.8139 (MSE:0.0014, Reg:235.8125) beta=13.25
Iter 11000 | Total loss: 136.9703 (MSE:0.0015, Reg:136.9688) beta=12.12
Iter 12000 | Total loss: 88.4426 (MSE:0.0015, Reg:88.4411) beta=11.00
Iter 13000 | Total loss: 49.4466 (MSE:0.0015, Reg:49.4451) beta=9.88
Iter 14000 | Total loss: 19.4535 (MSE:0.0015, Reg:19.4520) beta=8.75
Iter 15000 | Total loss: 6.0012 (MSE:0.0015, Reg:5.9997) beta=7.62
Iter 16000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0299 (MSE:0.0299, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0272 (MSE:0.0272, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0301 (MSE:0.0301, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0273 (MSE:0.0273, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 202806.3438 (MSE:0.0284, Reg:202806.3125) beta=20.00
Iter  5000 | Total loss: 21626.3711 (MSE:0.0255, Reg:21626.3457) beta=18.88
Iter  6000 | Total loss: 14631.0283 (MSE:0.0268, Reg:14631.0020) beta=17.75
Iter  7000 | Total loss: 10195.8682 (MSE:0.0265, Reg:10195.8418) beta=16.62
Iter  8000 | Total loss: 7013.3022 (MSE:0.0290, Reg:7013.2734) beta=15.50
Iter  9000 | Total loss: 4644.0815 (MSE:0.0305, Reg:4644.0513) beta=14.38
Iter 10000 | Total loss: 2839.6802 (MSE:0.0291, Reg:2839.6511) beta=13.25
Iter 11000 | Total loss: 1498.6573 (MSE:0.0247, Reg:1498.6327) beta=12.12
Iter 12000 | Total loss: 602.9899 (MSE:0.0275, Reg:602.9624) beta=11.00
Iter 13000 | Total loss: 164.0614 (MSE:0.0272, Reg:164.0342) beta=9.88
Iter 14000 | Total loss: 28.6661 (MSE:0.0271, Reg:28.6390) beta=8.75
Iter 15000 | Total loss: 1.0262 (MSE:0.0262, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0249 (MSE:0.0249, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0267 (MSE:0.0267, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0261 (MSE:0.0261, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0292 (MSE:0.0292, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0257 (MSE:0.0257, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0122 (MSE:0.0122, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0116 (MSE:0.0116, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 23548.7656 (MSE:0.0117, Reg:23548.7539) beta=20.00
Iter  5000 | Total loss: 3447.6582 (MSE:0.0122, Reg:3447.6460) beta=18.88
Iter  6000 | Total loss: 2603.1599 (MSE:0.0125, Reg:2603.1475) beta=17.75
Iter  7000 | Total loss: 2077.1785 (MSE:0.0118, Reg:2077.1667) beta=16.62
Iter  8000 | Total loss: 1584.5522 (MSE:0.0132, Reg:1584.5391) beta=15.50
Iter  9000 | Total loss: 1114.1129 (MSE:0.0119, Reg:1114.1011) beta=14.38
Iter 10000 | Total loss: 717.8470 (MSE:0.0113, Reg:717.8358) beta=13.25
Iter 11000 | Total loss: 400.8521 (MSE:0.0123, Reg:400.8398) beta=12.12
Iter 12000 | Total loss: 154.9913 (MSE:0.0119, Reg:154.9794) beta=11.00
Iter 13000 | Total loss: 62.2367 (MSE:0.0110, Reg:62.2258) beta=9.88
Iter 14000 | Total loss: 9.0120 (MSE:0.0120, Reg:9.0000) beta=8.75
Iter 15000 | Total loss: 0.0110 (MSE:0.0110, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0120 (MSE:0.0120, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0110 (MSE:0.0110, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 467119.9062 (MSE:0.0019, Reg:467119.9062) beta=20.00
Iter  5000 | Total loss: 3689.5220 (MSE:0.0018, Reg:3689.5200) beta=18.88
Iter  6000 | Total loss: 1224.5560 (MSE:0.0021, Reg:1224.5540) beta=17.75
Iter  7000 | Total loss: 648.2676 (MSE:0.0018, Reg:648.2659) beta=16.62
Iter  8000 | Total loss: 393.2036 (MSE:0.0019, Reg:393.2017) beta=15.50
Iter  9000 | Total loss: 261.9542 (MSE:0.0018, Reg:261.9523) beta=14.38
Iter 10000 | Total loss: 164.9964 (MSE:0.0019, Reg:164.9945) beta=13.25
Iter 11000 | Total loss: 112.0183 (MSE:0.0018, Reg:112.0166) beta=12.12
Iter 12000 | Total loss: 82.0018 (MSE:0.0018, Reg:82.0000) beta=11.00
Iter 13000 | Total loss: 41.8909 (MSE:0.0020, Reg:41.8890) beta=9.88
Iter 14000 | Total loss: 14.9699 (MSE:0.0020, Reg:14.9679) beta=8.75
Iter 15000 | Total loss: 2.0018 (MSE:0.0018, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.7815 (MSE:0.7815, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.8054 (MSE:0.8054, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.7596 (MSE:0.7596, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.7317 (MSE:0.7317, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 168528.5156 (MSE:0.8219, Reg:168527.6875) beta=20.00
Iter  5000 | Total loss: 29225.2656 (MSE:0.8048, Reg:29224.4609) beta=18.88
Iter  6000 | Total loss: 19132.8281 (MSE:0.8642, Reg:19131.9648) beta=17.75
Iter  7000 | Total loss: 12166.8125 (MSE:0.8393, Reg:12165.9736) beta=16.62
Iter  8000 | Total loss: 7468.0708 (MSE:0.7933, Reg:7467.2773) beta=15.50
Iter  9000 | Total loss: 4279.6318 (MSE:0.8291, Reg:4278.8027) beta=14.38
Iter 10000 | Total loss: 2043.1108 (MSE:0.7812, Reg:2042.3296) beta=13.25
Iter 11000 | Total loss: 770.5546 (MSE:0.7459, Reg:769.8087) beta=12.12
Iter 12000 | Total loss: 196.8733 (MSE:0.7615, Reg:196.1118) beta=11.00
Iter 13000 | Total loss: 26.7352 (MSE:0.7863, Reg:25.9489) beta=9.88
Iter 14000 | Total loss: 2.7225 (MSE:0.7225, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.7862 (MSE:0.7862, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.8088 (MSE:0.8088, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.7654 (MSE:0.7654, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.7558 (MSE:0.7558, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.7563 (MSE:0.7563, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.7655 (MSE:0.7655, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.8398 (MSE:0.8398, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.6701 (MSE:0.6701, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.6955 (MSE:0.6955, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.6781 (MSE:0.6781, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30097.4219 (MSE:0.7006, Reg:30096.7207) beta=20.00
Iter  5000 | Total loss: 5849.0181 (MSE:0.6998, Reg:5848.3184) beta=18.88
Iter  6000 | Total loss: 4019.2043 (MSE:0.5809, Reg:4018.6235) beta=17.75
Iter  7000 | Total loss: 2764.5413 (MSE:0.6468, Reg:2763.8945) beta=16.62
Iter  8000 | Total loss: 1828.7572 (MSE:0.7194, Reg:1828.0378) beta=15.50
Iter  9000 | Total loss: 1101.4434 (MSE:0.6745, Reg:1100.7688) beta=14.38
Iter 10000 | Total loss: 569.6911 (MSE:0.6318, Reg:569.0593) beta=13.25
Iter 11000 | Total loss: 268.2867 (MSE:0.5902, Reg:267.6964) beta=12.12
Iter 12000 | Total loss: 97.6176 (MSE:0.6891, Reg:96.9285) beta=11.00
Iter 13000 | Total loss: 26.6571 (MSE:0.6578, Reg:25.9993) beta=9.88
Iter 14000 | Total loss: 3.6418 (MSE:0.6418, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.7188 (MSE:0.7188, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.6440 (MSE:0.6440, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.6279 (MSE:0.6279, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.6198 (MSE:0.6198, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.6380 (MSE:0.6380, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.7052 (MSE:0.7052, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 39.374%
Total time: 19305.37 sec

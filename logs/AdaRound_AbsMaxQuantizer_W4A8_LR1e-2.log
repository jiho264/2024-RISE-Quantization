
Case: [ resnet18_AdaRound_AbsMaxQuantizer_CH_W4A8_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT8
    - per_channel: False

Replace to QuantModule
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1155.5774 (MSE:0.0007, Reg:1155.5767) beta=20.00
Iter  5000 | Total loss: 63.9349 (MSE:0.0006, Reg:63.9343) beta=18.88
Iter  6000 | Total loss: 44.0011 (MSE:0.0012, Reg:43.9998) beta=17.75
Iter  7000 | Total loss: 35.9674 (MSE:0.0014, Reg:35.9659) beta=16.62
Iter  8000 | Total loss: 20.0007 (MSE:0.0007, Reg:20.0000) beta=15.50
Iter  9000 | Total loss: 19.0008 (MSE:0.0008, Reg:19.0000) beta=14.38
Iter 10000 | Total loss: 11.0007 (MSE:0.0007, Reg:11.0000) beta=13.25
Iter 11000 | Total loss: 6.0010 (MSE:0.0010, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 2.0008 (MSE:0.0008, Reg:2.0000) beta=11.00
Iter 13000 | Total loss: 1.0012 (MSE:0.0012, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5399.2451 (MSE:0.0003, Reg:5399.2446) beta=20.00
Iter  5000 | Total loss: 251.3832 (MSE:0.0004, Reg:251.3828) beta=18.88
Iter  6000 | Total loss: 139.9930 (MSE:0.0006, Reg:139.9924) beta=17.75
Iter  7000 | Total loss: 102.9998 (MSE:0.0006, Reg:102.9992) beta=16.62
Iter  8000 | Total loss: 76.0004 (MSE:0.0004, Reg:76.0000) beta=15.50
Iter  9000 | Total loss: 54.0004 (MSE:0.0004, Reg:54.0000) beta=14.38
Iter 10000 | Total loss: 46.0004 (MSE:0.0004, Reg:46.0000) beta=13.25
Iter 11000 | Total loss: 35.0008 (MSE:0.0008, Reg:35.0000) beta=12.12
Iter 12000 | Total loss: 17.8348 (MSE:0.0004, Reg:17.8344) beta=11.00
Iter 13000 | Total loss: 13.0004 (MSE:0.0004, Reg:13.0000) beta=9.88
Iter 14000 | Total loss: 3.0009 (MSE:0.0009, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 2.0009 (MSE:0.0009, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10026.9170 (MSE:0.0012, Reg:10026.9160) beta=20.00
Iter  5000 | Total loss: 836.6054 (MSE:0.0025, Reg:836.6030) beta=18.88
Iter  6000 | Total loss: 628.2764 (MSE:0.0017, Reg:628.2746) beta=17.75
Iter  7000 | Total loss: 499.9815 (MSE:0.0016, Reg:499.9799) beta=16.62
Iter  8000 | Total loss: 390.4467 (MSE:0.0016, Reg:390.4451) beta=15.50
Iter  9000 | Total loss: 305.8747 (MSE:0.0016, Reg:305.8731) beta=14.38
Iter 10000 | Total loss: 217.3794 (MSE:0.0023, Reg:217.3771) beta=13.25
Iter 11000 | Total loss: 156.8209 (MSE:0.0018, Reg:156.8191) beta=12.12
Iter 12000 | Total loss: 113.0016 (MSE:0.0016, Reg:113.0000) beta=11.00
Iter 13000 | Total loss: 73.3397 (MSE:0.0016, Reg:73.3381) beta=9.88
Iter 14000 | Total loss: 27.0017 (MSE:0.0017, Reg:27.0000) beta=8.75
Iter 15000 | Total loss: 4.0024 (MSE:0.0024, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10584.8447 (MSE:0.0007, Reg:10584.8438) beta=20.00
Iter  5000 | Total loss: 765.5884 (MSE:0.0008, Reg:765.5876) beta=18.88
Iter  6000 | Total loss: 509.7466 (MSE:0.0007, Reg:509.7459) beta=17.75
Iter  7000 | Total loss: 424.0006 (MSE:0.0007, Reg:423.9999) beta=16.62
Iter  8000 | Total loss: 326.9161 (MSE:0.0007, Reg:326.9154) beta=15.50
Iter  9000 | Total loss: 264.0853 (MSE:0.0006, Reg:264.0847) beta=14.38
Iter 10000 | Total loss: 186.9439 (MSE:0.0006, Reg:186.9433) beta=13.25
Iter 11000 | Total loss: 129.0006 (MSE:0.0006, Reg:129.0000) beta=12.12
Iter 12000 | Total loss: 70.0007 (MSE:0.0007, Reg:70.0000) beta=11.00
Iter 13000 | Total loss: 40.8385 (MSE:0.0006, Reg:40.8379) beta=9.88
Iter 14000 | Total loss: 15.0007 (MSE:0.0007, Reg:15.0000) beta=8.75
Iter 15000 | Total loss: 3.4182 (MSE:0.0006, Reg:3.4176) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15717.2939 (MSE:0.0041, Reg:15717.2900) beta=20.00
Iter  5000 | Total loss: 1623.3761 (MSE:0.0056, Reg:1623.3705) beta=18.88
Iter  6000 | Total loss: 1286.9054 (MSE:0.0054, Reg:1286.9000) beta=17.75
Iter  7000 | Total loss: 1041.5583 (MSE:0.0057, Reg:1041.5527) beta=16.62
Iter  8000 | Total loss: 802.9473 (MSE:0.0053, Reg:802.9420) beta=15.50
Iter  9000 | Total loss: 651.2338 (MSE:0.0049, Reg:651.2289) beta=14.38
Iter 10000 | Total loss: 440.9590 (MSE:0.0050, Reg:440.9540) beta=13.25
Iter 11000 | Total loss: 318.0044 (MSE:0.0053, Reg:317.9990) beta=12.12
Iter 12000 | Total loss: 234.0037 (MSE:0.0053, Reg:233.9984) beta=11.00
Iter 13000 | Total loss: 115.9655 (MSE:0.0050, Reg:115.9605) beta=9.88
Iter 14000 | Total loss: 42.0422 (MSE:0.0059, Reg:42.0363) beta=8.75
Iter 15000 | Total loss: 12.0056 (MSE:0.0056, Reg:12.0000) beta=7.62
Iter 16000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 27895.0781 (MSE:0.0009, Reg:27895.0781) beta=20.00
Iter  5000 | Total loss: 2232.0974 (MSE:0.0008, Reg:2232.0967) beta=18.88
Iter  6000 | Total loss: 1479.8522 (MSE:0.0009, Reg:1479.8513) beta=17.75
Iter  7000 | Total loss: 1169.1173 (MSE:0.0008, Reg:1169.1165) beta=16.62
Iter  8000 | Total loss: 915.2309 (MSE:0.0009, Reg:915.2300) beta=15.50
Iter  9000 | Total loss: 706.0001 (MSE:0.0008, Reg:705.9992) beta=14.38
Iter 10000 | Total loss: 530.0579 (MSE:0.0009, Reg:530.0570) beta=13.25
Iter 11000 | Total loss: 370.7491 (MSE:0.0008, Reg:370.7483) beta=12.12
Iter 12000 | Total loss: 231.9702 (MSE:0.0010, Reg:231.9693) beta=11.00
Iter 13000 | Total loss: 131.4915 (MSE:0.0010, Reg:131.4905) beta=9.88
Iter 14000 | Total loss: 50.5317 (MSE:0.0009, Reg:50.5308) beta=8.75
Iter 15000 | Total loss: 12.0009 (MSE:0.0009, Reg:12.0000) beta=7.62
Iter 16000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 72486.4688 (MSE:0.0030, Reg:72486.4688) beta=20.00
Iter  5000 | Total loss: 6441.5977 (MSE:0.0037, Reg:6441.5938) beta=18.88
Iter  6000 | Total loss: 5139.6426 (MSE:0.0044, Reg:5139.6382) beta=17.75
Iter  7000 | Total loss: 4283.0156 (MSE:0.0047, Reg:4283.0107) beta=16.62
Iter  8000 | Total loss: 3355.4456 (MSE:0.0042, Reg:3355.4414) beta=15.50
Iter  9000 | Total loss: 2551.3699 (MSE:0.0053, Reg:2551.3645) beta=14.38
Iter 10000 | Total loss: 1862.9446 (MSE:0.0052, Reg:1862.9393) beta=13.25
Iter 11000 | Total loss: 1260.0077 (MSE:0.0041, Reg:1260.0037) beta=12.12
Iter 12000 | Total loss: 777.9170 (MSE:0.0040, Reg:777.9130) beta=11.00
Iter 13000 | Total loss: 430.5048 (MSE:0.0043, Reg:430.5005) beta=9.88
Iter 14000 | Total loss: 174.9483 (MSE:0.0040, Reg:174.9443) beta=8.75
Iter 15000 | Total loss: 43.8004 (MSE:0.0037, Reg:43.7967) beta=7.62
Iter 16000 | Total loss: 2.0039 (MSE:0.0039, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5370.7529 (MSE:0.0011, Reg:5370.7520) beta=20.00
Iter  5000 | Total loss: 960.6929 (MSE:0.0016, Reg:960.6913) beta=18.88
Iter  6000 | Total loss: 852.6070 (MSE:0.0018, Reg:852.6052) beta=17.75
Iter  7000 | Total loss: 745.7976 (MSE:0.0015, Reg:745.7961) beta=16.62
Iter  8000 | Total loss: 633.8657 (MSE:0.0015, Reg:633.8642) beta=15.50
Iter  9000 | Total loss: 528.0016 (MSE:0.0016, Reg:528.0000) beta=14.38
Iter 10000 | Total loss: 403.8435 (MSE:0.0025, Reg:403.8410) beta=13.25
Iter 11000 | Total loss: 302.0025 (MSE:0.0025, Reg:302.0000) beta=12.12
Iter 12000 | Total loss: 206.2900 (MSE:0.0017, Reg:206.2883) beta=11.00
Iter 13000 | Total loss: 120.4815 (MSE:0.0017, Reg:120.4798) beta=9.88
Iter 14000 | Total loss: 57.0018 (MSE:0.0018, Reg:57.0000) beta=8.75
Iter 15000 | Total loss: 25.8775 (MSE:0.0018, Reg:25.8757) beta=7.62
Iter 16000 | Total loss: 3.0018 (MSE:0.0018, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 50871.4883 (MSE:0.0006, Reg:50871.4883) beta=20.00
Iter  5000 | Total loss: 4554.9888 (MSE:0.0007, Reg:4554.9883) beta=18.88
Iter  6000 | Total loss: 2993.3396 (MSE:0.0007, Reg:2993.3389) beta=17.75
Iter  7000 | Total loss: 2215.5374 (MSE:0.0008, Reg:2215.5366) beta=16.62
Iter  8000 | Total loss: 1687.7015 (MSE:0.0008, Reg:1687.7008) beta=15.50
Iter  9000 | Total loss: 1234.2487 (MSE:0.0007, Reg:1234.2479) beta=14.38
Iter 10000 | Total loss: 866.3652 (MSE:0.0008, Reg:866.3644) beta=13.25
Iter 11000 | Total loss: 597.3605 (MSE:0.0007, Reg:597.3597) beta=12.12
Iter 12000 | Total loss: 362.8877 (MSE:0.0008, Reg:362.8870) beta=11.00
Iter 13000 | Total loss: 190.5912 (MSE:0.0007, Reg:190.5905) beta=9.88
Iter 14000 | Total loss: 74.9995 (MSE:0.0007, Reg:74.9987) beta=8.75
Iter 15000 | Total loss: 18.9932 (MSE:0.0007, Reg:18.9925) beta=7.62
Iter 16000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 68215.2734 (MSE:0.0032, Reg:68215.2734) beta=20.00
Iter  5000 | Total loss: 7577.5283 (MSE:0.0035, Reg:7577.5249) beta=18.88
Iter  6000 | Total loss: 5876.1763 (MSE:0.0037, Reg:5876.1724) beta=17.75
Iter  7000 | Total loss: 4883.0356 (MSE:0.0037, Reg:4883.0317) beta=16.62
Iter  8000 | Total loss: 3999.6038 (MSE:0.0036, Reg:3999.6001) beta=15.50
Iter  9000 | Total loss: 3091.6509 (MSE:0.0038, Reg:3091.6470) beta=14.38
Iter 10000 | Total loss: 2284.9409 (MSE:0.0036, Reg:2284.9373) beta=13.25
Iter 11000 | Total loss: 1460.3912 (MSE:0.0039, Reg:1460.3873) beta=12.12
Iter 12000 | Total loss: 812.0052 (MSE:0.0037, Reg:812.0015) beta=11.00
Iter 13000 | Total loss: 416.0904 (MSE:0.0037, Reg:416.0866) beta=9.88
Iter 14000 | Total loss: 180.0464 (MSE:0.0037, Reg:180.0427) beta=8.75
Iter 15000 | Total loss: 34.7876 (MSE:0.0040, Reg:34.7836) beta=7.62
Iter 16000 | Total loss: 1.0041 (MSE:0.0041, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 106000.2734 (MSE:0.0010, Reg:106000.2734) beta=20.00
Iter  5000 | Total loss: 10367.9756 (MSE:0.0011, Reg:10367.9746) beta=18.88
Iter  6000 | Total loss: 6738.3389 (MSE:0.0011, Reg:6738.3379) beta=17.75
Iter  7000 | Total loss: 5019.5991 (MSE:0.0013, Reg:5019.5977) beta=16.62
Iter  8000 | Total loss: 3899.6663 (MSE:0.0011, Reg:3899.6653) beta=15.50
Iter  9000 | Total loss: 2913.2026 (MSE:0.0012, Reg:2913.2014) beta=14.38
Iter 10000 | Total loss: 2125.4561 (MSE:0.0011, Reg:2125.4551) beta=13.25
Iter 11000 | Total loss: 1385.6392 (MSE:0.0011, Reg:1385.6381) beta=12.12
Iter 12000 | Total loss: 858.1505 (MSE:0.0012, Reg:858.1493) beta=11.00
Iter 13000 | Total loss: 458.5748 (MSE:0.0012, Reg:458.5736) beta=9.88
Iter 14000 | Total loss: 165.2570 (MSE:0.0010, Reg:165.2560) beta=8.75
Iter 15000 | Total loss: 22.7680 (MSE:0.0011, Reg:22.7669) beta=7.62
Iter 16000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 205262.3281 (MSE:0.0033, Reg:205262.3281) beta=20.00
Iter  5000 | Total loss: 17762.1680 (MSE:0.0035, Reg:17762.1641) beta=18.88
Iter  6000 | Total loss: 12095.9102 (MSE:0.0037, Reg:12095.9062) beta=17.75
Iter  7000 | Total loss: 9225.8779 (MSE:0.0038, Reg:9225.8740) beta=16.62
Iter  8000 | Total loss: 7088.0146 (MSE:0.0037, Reg:7088.0107) beta=15.50
Iter  9000 | Total loss: 5251.8579 (MSE:0.0044, Reg:5251.8535) beta=14.38
Iter 10000 | Total loss: 3719.6814 (MSE:0.0042, Reg:3719.6772) beta=13.25
Iter 11000 | Total loss: 2397.4824 (MSE:0.0039, Reg:2397.4785) beta=12.12
Iter 12000 | Total loss: 1326.6921 (MSE:0.0040, Reg:1326.6881) beta=11.00
Iter 13000 | Total loss: 587.2156 (MSE:0.0038, Reg:587.2118) beta=9.88
Iter 14000 | Total loss: 176.0965 (MSE:0.0037, Reg:176.0928) beta=8.75
Iter 15000 | Total loss: 31.0040 (MSE:0.0040, Reg:31.0000) beta=7.62
Iter 16000 | Total loss: 1.0040 (MSE:0.0040, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 20843.3105 (MSE:0.0003, Reg:20843.3105) beta=20.00
Iter  5000 | Total loss: 3194.6160 (MSE:0.0004, Reg:3194.6157) beta=18.88
Iter  6000 | Total loss: 2478.2048 (MSE:0.0004, Reg:2478.2046) beta=17.75
Iter  7000 | Total loss: 2083.3679 (MSE:0.0004, Reg:2083.3674) beta=16.62
Iter  8000 | Total loss: 1723.2058 (MSE:0.0004, Reg:1723.2054) beta=15.50
Iter  9000 | Total loss: 1416.3967 (MSE:0.0004, Reg:1416.3964) beta=14.38
Iter 10000 | Total loss: 1107.2970 (MSE:0.0004, Reg:1107.2966) beta=13.25
Iter 11000 | Total loss: 741.0301 (MSE:0.0004, Reg:741.0297) beta=12.12
Iter 12000 | Total loss: 477.9570 (MSE:0.0004, Reg:477.9566) beta=11.00
Iter 13000 | Total loss: 284.2138 (MSE:0.0005, Reg:284.2133) beta=9.88
Iter 14000 | Total loss: 130.3867 (MSE:0.0004, Reg:130.3863) beta=8.75
Iter 15000 | Total loss: 49.0004 (MSE:0.0004, Reg:49.0000) beta=7.62
Iter 16000 | Total loss: 7.6701 (MSE:0.0004, Reg:7.6697) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 149852.9688 (MSE:0.0004, Reg:149852.9688) beta=20.00
Iter  5000 | Total loss: 6149.3735 (MSE:0.0004, Reg:6149.3730) beta=18.88
Iter  6000 | Total loss: 3365.7334 (MSE:0.0005, Reg:3365.7329) beta=17.75
Iter  7000 | Total loss: 2311.9585 (MSE:0.0005, Reg:2311.9580) beta=16.62
Iter  8000 | Total loss: 1708.0706 (MSE:0.0004, Reg:1708.0701) beta=15.50
Iter  9000 | Total loss: 1267.2905 (MSE:0.0005, Reg:1267.2900) beta=14.38
Iter 10000 | Total loss: 958.0836 (MSE:0.0005, Reg:958.0831) beta=13.25
Iter 11000 | Total loss: 683.6124 (MSE:0.0005, Reg:683.6119) beta=12.12
Iter 12000 | Total loss: 435.2990 (MSE:0.0004, Reg:435.2986) beta=11.00
Iter 13000 | Total loss: 251.0748 (MSE:0.0005, Reg:251.0743) beta=9.88
Iter 14000 | Total loss: 110.3856 (MSE:0.0004, Reg:110.3852) beta=8.75
Iter 15000 | Total loss: 31.0004 (MSE:0.0004, Reg:31.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 199971.4219 (MSE:0.0029, Reg:199971.4219) beta=20.00
Iter  5000 | Total loss: 16827.4062 (MSE:0.0029, Reg:16827.4043) beta=18.88
Iter  6000 | Total loss: 11200.6455 (MSE:0.0032, Reg:11200.6426) beta=17.75
Iter  7000 | Total loss: 8514.4541 (MSE:0.0030, Reg:8514.4512) beta=16.62
Iter  8000 | Total loss: 6549.6089 (MSE:0.0031, Reg:6549.6060) beta=15.50
Iter  9000 | Total loss: 4954.1172 (MSE:0.0029, Reg:4954.1143) beta=14.38
Iter 10000 | Total loss: 3556.6978 (MSE:0.0031, Reg:3556.6948) beta=13.25
Iter 11000 | Total loss: 2343.9678 (MSE:0.0034, Reg:2343.9644) beta=12.12
Iter 12000 | Total loss: 1331.0995 (MSE:0.0031, Reg:1331.0964) beta=11.00
Iter 13000 | Total loss: 643.4763 (MSE:0.0032, Reg:643.4730) beta=9.88
Iter 14000 | Total loss: 233.6926 (MSE:0.0033, Reg:233.6893) beta=8.75
Iter 15000 | Total loss: 53.5879 (MSE:0.0034, Reg:53.5845) beta=7.62
Iter 16000 | Total loss: 3.9953 (MSE:0.0032, Reg:3.9921) beta=6.50
Iter 17000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 269752.1875 (MSE:0.0005, Reg:269752.1875) beta=20.00
Iter  5000 | Total loss: 4850.7700 (MSE:0.0005, Reg:4850.7695) beta=18.88
Iter  6000 | Total loss: 1972.1774 (MSE:0.0005, Reg:1972.1769) beta=17.75
Iter  7000 | Total loss: 1207.6090 (MSE:0.0005, Reg:1207.6085) beta=16.62
Iter  8000 | Total loss: 858.9469 (MSE:0.0005, Reg:858.9464) beta=15.50
Iter  9000 | Total loss: 632.6993 (MSE:0.0006, Reg:632.6986) beta=14.38
Iter 10000 | Total loss: 481.7114 (MSE:0.0005, Reg:481.7109) beta=13.25
Iter 11000 | Total loss: 346.9706 (MSE:0.0005, Reg:346.9701) beta=12.12
Iter 12000 | Total loss: 243.0005 (MSE:0.0005, Reg:243.0000) beta=11.00
Iter 13000 | Total loss: 152.2804 (MSE:0.0005, Reg:152.2799) beta=9.88
Iter 14000 | Total loss: 84.9669 (MSE:0.0005, Reg:84.9664) beta=8.75
Iter 15000 | Total loss: 23.6592 (MSE:0.0005, Reg:23.6587) beta=7.62
Iter 16000 | Total loss: 4.0005 (MSE:0.0005, Reg:4.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 621050.9375 (MSE:0.0079, Reg:621050.9375) beta=20.00
Iter  5000 | Total loss: 79629.1562 (MSE:0.0088, Reg:79629.1484) beta=18.88
Iter  6000 | Total loss: 54509.9922 (MSE:0.0079, Reg:54509.9844) beta=17.75
Iter  7000 | Total loss: 40330.6797 (MSE:0.0090, Reg:40330.6719) beta=16.62
Iter  8000 | Total loss: 30179.6387 (MSE:0.0096, Reg:30179.6289) beta=15.50
Iter  9000 | Total loss: 21769.5879 (MSE:0.0098, Reg:21769.5781) beta=14.38
Iter 10000 | Total loss: 14703.5098 (MSE:0.0084, Reg:14703.5010) beta=13.25
Iter 11000 | Total loss: 8860.7480 (MSE:0.0098, Reg:8860.7383) beta=12.12
Iter 12000 | Total loss: 4425.2397 (MSE:0.0082, Reg:4425.2314) beta=11.00
Iter 13000 | Total loss: 1645.7289 (MSE:0.0085, Reg:1645.7205) beta=9.88
Iter 14000 | Total loss: 339.1937 (MSE:0.0089, Reg:339.1848) beta=8.75
Iter 15000 | Total loss: 18.9722 (MSE:0.0089, Reg:18.9634) beta=7.62
Iter 16000 | Total loss: 0.9827 (MSE:0.0098, Reg:0.9729) beta=6.50
Iter 17000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 66365.2812 (MSE:0.0024, Reg:66365.2812) beta=20.00
Iter  5000 | Total loss: 11024.4912 (MSE:0.0031, Reg:11024.4883) beta=18.88
Iter  6000 | Total loss: 8847.7373 (MSE:0.0030, Reg:8847.7344) beta=17.75
Iter  7000 | Total loss: 7387.9551 (MSE:0.0038, Reg:7387.9512) beta=16.62
Iter  8000 | Total loss: 6119.7949 (MSE:0.0031, Reg:6119.7920) beta=15.50
Iter  9000 | Total loss: 4806.2847 (MSE:0.0032, Reg:4806.2812) beta=14.38
Iter 10000 | Total loss: 3632.4993 (MSE:0.0038, Reg:3632.4956) beta=13.25
Iter 11000 | Total loss: 2384.7651 (MSE:0.0031, Reg:2384.7620) beta=12.12
Iter 12000 | Total loss: 1311.1071 (MSE:0.0034, Reg:1311.1036) beta=11.00
Iter 13000 | Total loss: 609.4979 (MSE:0.0034, Reg:609.4945) beta=9.88
Iter 14000 | Total loss: 190.0809 (MSE:0.0033, Reg:190.0775) beta=8.75
Iter 15000 | Total loss: 27.4833 (MSE:0.0035, Reg:27.4798) beta=7.62
Iter 16000 | Total loss: 2.0035 (MSE:0.0035, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 464476.3750 (MSE:0.0007, Reg:464476.3750) beta=20.00
Iter  5000 | Total loss: 7326.4565 (MSE:0.0007, Reg:7326.4561) beta=18.88
Iter  6000 | Total loss: 2159.8252 (MSE:0.0007, Reg:2159.8245) beta=17.75
Iter  7000 | Total loss: 1234.0585 (MSE:0.0007, Reg:1234.0579) beta=16.62
Iter  8000 | Total loss: 827.7859 (MSE:0.0007, Reg:827.7852) beta=15.50
Iter  9000 | Total loss: 575.2268 (MSE:0.0007, Reg:575.2261) beta=14.38
Iter 10000 | Total loss: 417.4833 (MSE:0.0007, Reg:417.4826) beta=13.25
Iter 11000 | Total loss: 274.8746 (MSE:0.0006, Reg:274.8740) beta=12.12
Iter 12000 | Total loss: 166.9595 (MSE:0.0007, Reg:166.9588) beta=11.00
Iter 13000 | Total loss: 89.9839 (MSE:0.0007, Reg:89.9832) beta=9.88
Iter 14000 | Total loss: 37.4885 (MSE:0.0007, Reg:37.4878) beta=8.75
Iter 15000 | Total loss: 5.0007 (MSE:0.0007, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.3605 (MSE:0.3605, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3023 (MSE:0.3023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2870 (MSE:0.2870, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2918 (MSE:0.2918, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 434634.4688 (MSE:0.3076, Reg:434634.1562) beta=20.00
Iter  5000 | Total loss: 90026.4844 (MSE:0.3110, Reg:90026.1719) beta=18.88
Iter  6000 | Total loss: 61968.5000 (MSE:0.3157, Reg:61968.1836) beta=17.75
Iter  7000 | Total loss: 43049.0312 (MSE:0.3366, Reg:43048.6953) beta=16.62
Iter  8000 | Total loss: 29027.1680 (MSE:0.3265, Reg:29026.8418) beta=15.50
Iter  9000 | Total loss: 18476.2695 (MSE:0.2970, Reg:18475.9727) beta=14.38
Iter 10000 | Total loss: 10000.5889 (MSE:0.2982, Reg:10000.2910) beta=13.25
Iter 11000 | Total loss: 4469.8018 (MSE:0.3063, Reg:4469.4956) beta=12.12
Iter 12000 | Total loss: 1487.6738 (MSE:0.2996, Reg:1487.3743) beta=11.00
Iter 13000 | Total loss: 277.0916 (MSE:0.3149, Reg:276.7766) beta=9.88
Iter 14000 | Total loss: 17.3700 (MSE:0.2876, Reg:17.0824) beta=8.75
Iter 15000 | Total loss: 0.3073 (MSE:0.3073, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.3287 (MSE:0.3287, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.3006 (MSE:0.3006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2917 (MSE:0.2917, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2885 (MSE:0.2885, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2959 (MSE:0.2959, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.3481 (MSE:0.3481, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1491 (MSE:0.1491, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1441 (MSE:0.1441, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1478 (MSE:0.1478, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 40733.4688 (MSE:0.1512, Reg:40733.3164) beta=20.00
Iter  5000 | Total loss: 8113.6670 (MSE:0.1384, Reg:8113.5283) beta=18.88
Iter  6000 | Total loss: 6195.1938 (MSE:0.1436, Reg:6195.0503) beta=17.75
Iter  7000 | Total loss: 4775.3477 (MSE:0.1496, Reg:4775.1982) beta=16.62
Iter  8000 | Total loss: 3586.3835 (MSE:0.1498, Reg:3586.2339) beta=15.50
Iter  9000 | Total loss: 2511.8193 (MSE:0.1435, Reg:2511.6758) beta=14.38
Iter 10000 | Total loss: 1577.2457 (MSE:0.1549, Reg:1577.0908) beta=13.25
Iter 11000 | Total loss: 819.5828 (MSE:0.1365, Reg:819.4463) beta=12.12
Iter 12000 | Total loss: 314.1830 (MSE:0.1461, Reg:314.0368) beta=11.00
Iter 13000 | Total loss: 79.1675 (MSE:0.1561, Reg:79.0115) beta=9.88
Iter 14000 | Total loss: 23.1442 (MSE:0.1442, Reg:23.0000) beta=8.75
Iter 15000 | Total loss: 8.2488 (MSE:0.1497, Reg:8.0991) beta=7.62
Iter 16000 | Total loss: 0.1533 (MSE:0.1533, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1527 (MSE:0.1527, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1425 (MSE:0.1425, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1487 (MSE:0.1487, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1544 (MSE:0.1544, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 68.834%
Total time: 1300.83 sec

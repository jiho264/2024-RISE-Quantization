Case: [ resnet18_AdaRoundQuantizer_CH_W4A32 ]
    - {'arch': 'resnet18', 'batch_size': 128, 'num_samples': 1024, 'batch_size_AdaRound': 32, 'lr': 0.01}
    - weight params: {'scheme': 'AdaRoundQuantizer', 'per_channel': 'True', 'dstDtype': 'INT4', 'BaseScheme': 'AbsMaxQuantizer'}
    - activation params: {}
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Parent class is AbsMaxQuantizer
Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.4589 (MSE:0.4589, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0263 (MSE:0.0263, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1007.9041 (MSE:0.0233, Reg:1007.8809) beta=20.00
Iter  6000 | Total loss: 109.0205 (MSE:0.0273, Reg:108.9932) beta=17.75
Iter  8000 | Total loss: 69.0359 (MSE:0.0359, Reg:69.0000) beta=15.50
Iter 10000 | Total loss: 39.3889 (MSE:0.0298, Reg:39.3591) beta=13.25
Iter 12000 | Total loss: 18.0312 (MSE:0.0312, Reg:18.0000) beta=11.00
Iter 14000 | Total loss: 9.0306 (MSE:0.0306, Reg:9.0000) beta=8.75
Iter 16000 | Total loss: 3.0345 (MSE:0.0345, Reg:3.0000) beta=6.50
Iter 18000 | Total loss: 0.0291 (MSE:0.0291, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0260 (MSE:0.0260, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2245 (MSE:0.2245, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2623.0430 (MSE:0.0118, Reg:2623.0312) beta=20.00
Iter  6000 | Total loss: 232.7566 (MSE:0.0134, Reg:232.7432) beta=17.75
Iter  8000 | Total loss: 149.8953 (MSE:0.0135, Reg:149.8817) beta=15.50
Iter 10000 | Total loss: 59.0022 (MSE:0.0115, Reg:58.9907) beta=13.25
Iter 12000 | Total loss: 20.0143 (MSE:0.0143, Reg:20.0000) beta=11.00
Iter 14000 | Total loss: 5.0110 (MSE:0.0110, Reg:5.0000) beta=8.75
Iter 16000 | Total loss: 1.0124 (MSE:0.0124, Reg:1.0000) beta=6.50
Iter 18000 | Total loss: 0.0113 (MSE:0.0113, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0118 (MSE:0.0118, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0099 (MSE:0.0099, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2647.4600 (MSE:0.0040, Reg:2647.4561) beta=20.00
Iter  6000 | Total loss: 281.4313 (MSE:0.0035, Reg:281.4278) beta=17.75
Iter  8000 | Total loss: 156.0016 (MSE:0.0040, Reg:155.9976) beta=15.50
Iter 10000 | Total loss: 78.0431 (MSE:0.0039, Reg:78.0393) beta=13.25
Iter 12000 | Total loss: 42.0037 (MSE:0.0038, Reg:41.9999) beta=11.00
Iter 14000 | Total loss: 4.0037 (MSE:0.0037, Reg:4.0000) beta=8.75
Iter 16000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1286 (MSE:0.1286, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0287 (MSE:0.0287, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2428.7747 (MSE:0.0263, Reg:2428.7483) beta=20.00
Iter  6000 | Total loss: 333.9858 (MSE:0.0331, Reg:333.9527) beta=17.75
Iter  8000 | Total loss: 233.0280 (MSE:0.0280, Reg:233.0000) beta=15.50
Iter 10000 | Total loss: 146.7638 (MSE:0.0302, Reg:146.7336) beta=13.25
Iter 12000 | Total loss: 48.0021 (MSE:0.0276, Reg:47.9745) beta=11.00
Iter 14000 | Total loss: 14.0268 (MSE:0.0268, Reg:14.0000) beta=8.75
Iter 16000 | Total loss: 0.0265 (MSE:0.0265, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0314 (MSE:0.0314, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0269 (MSE:0.0269, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0089 (MSE:0.0089, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5033.2441 (MSE:0.0051, Reg:5033.2393) beta=20.00
Iter  6000 | Total loss: 594.9980 (MSE:0.0051, Reg:594.9929) beta=17.75
Iter  8000 | Total loss: 412.9247 (MSE:0.0051, Reg:412.9196) beta=15.50
Iter 10000 | Total loss: 172.4781 (MSE:0.0056, Reg:172.4725) beta=13.25
Iter 12000 | Total loss: 68.4727 (MSE:0.0054, Reg:68.4673) beta=11.00
Iter 14000 | Total loss: 3.9994 (MSE:0.0058, Reg:3.9936) beta=8.75
Iter 16000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0816 (MSE:0.0816, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0452 (MSE:0.0452, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5360.8237 (MSE:0.0499, Reg:5360.7739) beta=20.00
Iter  6000 | Total loss: 691.1452 (MSE:0.0462, Reg:691.0990) beta=17.75
Iter  8000 | Total loss: 481.6562 (MSE:0.0487, Reg:481.6075) beta=15.50
Iter 10000 | Total loss: 276.4952 (MSE:0.0519, Reg:276.4432) beta=13.25
Iter 12000 | Total loss: 97.8557 (MSE:0.0537, Reg:97.8020) beta=11.00
Iter 14000 | Total loss: 23.0251 (MSE:0.0483, Reg:22.9768) beta=8.75
Iter 16000 | Total loss: 0.0484 (MSE:0.0484, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0512 (MSE:0.0512, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0529 (MSE:0.0529, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0165 (MSE:0.0165, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 18210.2539 (MSE:0.0098, Reg:18210.2441) beta=20.00
Iter  6000 | Total loss: 1328.8945 (MSE:0.0094, Reg:1328.8851) beta=17.75
Iter  8000 | Total loss: 943.3846 (MSE:0.0099, Reg:943.3748) beta=15.50
Iter 10000 | Total loss: 560.1345 (MSE:0.0104, Reg:560.1240) beta=13.25
Iter 12000 | Total loss: 161.5892 (MSE:0.0095, Reg:161.5797) beta=11.00
Iter 14000 | Total loss: 6.1410 (MSE:0.0097, Reg:6.1312) beta=8.75
Iter 16000 | Total loss: 0.0098 (MSE:0.0098, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0106 (MSE:0.0106, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0356 (MSE:0.0356, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0093 (MSE:0.0093, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2043.0983 (MSE:0.0088, Reg:2043.0895) beta=20.00
Iter  6000 | Total loss: 247.0119 (MSE:0.0119, Reg:247.0000) beta=17.75
Iter  8000 | Total loss: 218.9807 (MSE:0.0122, Reg:218.9684) beta=15.50
Iter 10000 | Total loss: 158.9566 (MSE:0.0119, Reg:158.9446) beta=13.25
Iter 12000 | Total loss: 87.0122 (MSE:0.0122, Reg:87.0000) beta=11.00
Iter 14000 | Total loss: 29.0112 (MSE:0.0112, Reg:29.0000) beta=8.75
Iter 16000 | Total loss: 1.0115 (MSE:0.0115, Reg:1.0000) beta=6.50
Iter 18000 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0476 (MSE:0.0476, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0269 (MSE:0.0269, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15203.6201 (MSE:0.0271, Reg:15203.5928) beta=20.00
Iter  6000 | Total loss: 1723.3103 (MSE:0.0251, Reg:1723.2852) beta=17.75
Iter  8000 | Total loss: 1252.7721 (MSE:0.0264, Reg:1252.7457) beta=15.50
Iter 10000 | Total loss: 765.1447 (MSE:0.0268, Reg:765.1179) beta=13.25
Iter 12000 | Total loss: 234.7847 (MSE:0.0260, Reg:234.7587) beta=11.00
Iter 14000 | Total loss: 21.6728 (MSE:0.0276, Reg:21.6452) beta=8.75
Iter 16000 | Total loss: 0.0276 (MSE:0.0276, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0269 (MSE:0.0269, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0273 (MSE:0.0273, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30106.8965 (MSE:0.0054, Reg:30106.8906) beta=20.00
Iter  6000 | Total loss: 2058.1111 (MSE:0.0052, Reg:2058.1060) beta=17.75
Iter  8000 | Total loss: 1443.2577 (MSE:0.0054, Reg:1443.2523) beta=15.50
Iter 10000 | Total loss: 825.9575 (MSE:0.0058, Reg:825.9517) beta=13.25
Iter 12000 | Total loss: 252.6655 (MSE:0.0058, Reg:252.6597) beta=11.00
Iter 14000 | Total loss: 28.9469 (MSE:0.0055, Reg:28.9415) beta=8.75
Iter 16000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0495 (MSE:0.0495, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0320 (MSE:0.0320, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 34081.6953 (MSE:0.0321, Reg:34081.6641) beta=20.00
Iter  6000 | Total loss: 3250.3586 (MSE:0.0305, Reg:3250.3281) beta=17.75
Iter  8000 | Total loss: 2176.5664 (MSE:0.0329, Reg:2176.5334) beta=15.50
Iter 10000 | Total loss: 1167.4427 (MSE:0.0326, Reg:1167.4102) beta=13.25
Iter 12000 | Total loss: 302.3394 (MSE:0.0345, Reg:302.3049) beta=11.00
Iter 14000 | Total loss: 26.9431 (MSE:0.0333, Reg:26.9098) beta=8.75
Iter 16000 | Total loss: 0.0336 (MSE:0.0336, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0327 (MSE:0.0327, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0362 (MSE:0.0362, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 65970.2891 (MSE:0.0124, Reg:65970.2734) beta=20.00
Iter  6000 | Total loss: 4413.6934 (MSE:0.0118, Reg:4413.6816) beta=17.75
Iter  8000 | Total loss: 2802.0662 (MSE:0.0133, Reg:2802.0530) beta=15.50
Iter 10000 | Total loss: 1362.0543 (MSE:0.0136, Reg:1362.0408) beta=13.25
Iter 12000 | Total loss: 324.1317 (MSE:0.0133, Reg:324.1183) beta=11.00
Iter 14000 | Total loss: 9.0131 (MSE:0.0131, Reg:9.0000) beta=8.75
Iter 16000 | Total loss: 0.0128 (MSE:0.0128, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0137 (MSE:0.0137, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9496.3926 (MSE:0.0022, Reg:9496.3906) beta=20.00
Iter  6000 | Total loss: 791.9572 (MSE:0.0027, Reg:791.9546) beta=17.75
Iter  8000 | Total loss: 584.9803 (MSE:0.0025, Reg:584.9778) beta=15.50
Iter 10000 | Total loss: 329.3786 (MSE:0.0025, Reg:329.3760) beta=13.25
Iter 12000 | Total loss: 136.0590 (MSE:0.0027, Reg:136.0564) beta=11.00
Iter 14000 | Total loss: 17.0023 (MSE:0.0026, Reg:16.9997) beta=8.75
Iter 16000 | Total loss: 1.0026 (MSE:0.0026, Reg:1.0000) beta=6.50
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0302 (MSE:0.0302, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 48666.1758 (MSE:0.0162, Reg:48666.1602) beta=20.00
Iter  6000 | Total loss: 3298.1182 (MSE:0.0172, Reg:3298.1011) beta=17.75
Iter  8000 | Total loss: 2013.9578 (MSE:0.0176, Reg:2013.9402) beta=15.50
Iter 10000 | Total loss: 972.9744 (MSE:0.0163, Reg:972.9581) beta=13.25
Iter 12000 | Total loss: 250.9809 (MSE:0.0173, Reg:250.9635) beta=11.00
Iter 14000 | Total loss: 6.0051 (MSE:0.0163, Reg:5.9888) beta=8.75
Iter 16000 | Total loss: 0.0182 (MSE:0.0182, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0185 (MSE:0.0185, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 96678.5000 (MSE:0.0032, Reg:96678.5000) beta=20.00
Iter  6000 | Total loss: 3970.3123 (MSE:0.0035, Reg:3970.3088) beta=17.75
Iter  8000 | Total loss: 2625.8772 (MSE:0.0036, Reg:2625.8735) beta=15.50
Iter 10000 | Total loss: 1452.9766 (MSE:0.0035, Reg:1452.9730) beta=13.25
Iter 12000 | Total loss: 434.3546 (MSE:0.0039, Reg:434.3507) beta=11.00
Iter 14000 | Total loss: 52.7671 (MSE:0.0035, Reg:52.7637) beta=8.75
Iter 16000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0636 (MSE:0.0636, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 103509.8516 (MSE:0.0229, Reg:103509.8281) beta=20.00
Iter  6000 | Total loss: 5186.1479 (MSE:0.0249, Reg:5186.1230) beta=17.75
Iter  8000 | Total loss: 3178.0974 (MSE:0.0243, Reg:3178.0730) beta=15.50
Iter 10000 | Total loss: 1545.8359 (MSE:0.0251, Reg:1545.8108) beta=13.25
Iter 12000 | Total loss: 337.4073 (MSE:0.0238, Reg:337.3835) beta=11.00
Iter 14000 | Total loss: 9.0254 (MSE:0.0254, Reg:9.0000) beta=8.75
Iter 16000 | Total loss: 0.0245 (MSE:0.0245, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0242 (MSE:0.0242, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0240 (MSE:0.0240, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0130 (MSE:0.0130, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 243680.7812 (MSE:0.0056, Reg:243680.7812) beta=20.00
Iter  6000 | Total loss: 6827.8779 (MSE:0.0061, Reg:6827.8721) beta=17.75
Iter  8000 | Total loss: 4305.8804 (MSE:0.0061, Reg:4305.8740) beta=15.50
Iter 10000 | Total loss: 2501.8411 (MSE:0.0062, Reg:2501.8350) beta=13.25
Iter 12000 | Total loss: 948.5400 (MSE:0.0062, Reg:948.5338) beta=11.00
Iter 14000 | Total loss: 59.7785 (MSE:0.0059, Reg:59.7725) beta=8.75
Iter 16000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 28549.5078 (MSE:0.0051, Reg:28549.5020) beta=20.00
Iter  6000 | Total loss: 1110.6831 (MSE:0.0066, Reg:1110.6765) beta=17.75
Iter  8000 | Total loss: 865.3580 (MSE:0.0072, Reg:865.3508) beta=15.50
Iter 10000 | Total loss: 498.9337 (MSE:0.0072, Reg:498.9265) beta=13.25
Iter 12000 | Total loss: 181.2990 (MSE:0.0069, Reg:181.2921) beta=11.00
Iter 14000 | Total loss: 32.0069 (MSE:0.0069, Reg:32.0000) beta=8.75
Iter 16000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0825 (MSE:0.0825, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0336 (MSE:0.0336, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 231100.1250 (MSE:0.0344, Reg:231100.0938) beta=20.00
Iter  6000 | Total loss: 20940.1035 (MSE:0.0351, Reg:20940.0684) beta=17.75
Iter  8000 | Total loss: 10707.9229 (MSE:0.0351, Reg:10707.8877) beta=15.50
Iter 10000 | Total loss: 3794.7036 (MSE:0.0354, Reg:3794.6682) beta=13.25
Iter 12000 | Total loss: 505.1368 (MSE:0.0357, Reg:505.1011) beta=11.00
Iter 14000 | Total loss: 2.2567 (MSE:0.0333, Reg:2.2235) beta=8.75
Iter 16000 | Total loss: 0.0316 (MSE:0.0316, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0345 (MSE:0.0345, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0346 (MSE:0.0346, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0104 (MSE:0.0104, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 283141.5938 (MSE:0.0060, Reg:283141.5938) beta=20.00
Iter  6000 | Total loss: 7362.5410 (MSE:0.0061, Reg:7362.5347) beta=17.75
Iter  8000 | Total loss: 3655.6938 (MSE:0.0059, Reg:3655.6880) beta=15.50
Iter 10000 | Total loss: 1647.4861 (MSE:0.0061, Reg:1647.4800) beta=13.25
Iter 12000 | Total loss: 430.3381 (MSE:0.0065, Reg:430.3315) beta=11.00
Iter 14000 | Total loss: 20.8654 (MSE:0.0062, Reg:20.8591) beta=8.75
Iter 16000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 1.7014 (MSE:1.7014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 1.1744 (MSE:1.1744, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 28216.9062 (MSE:1.3612, Reg:28215.5449) beta=20.00
Iter  6000 | Total loss: 5065.6436 (MSE:1.2870, Reg:5064.3564) beta=17.75
Iter  8000 | Total loss: 2967.8003 (MSE:1.3511, Reg:2966.4492) beta=15.50
Iter 10000 | Total loss: 1348.1914 (MSE:1.1927, Reg:1346.9988) beta=13.25
Iter 12000 | Total loss: 302.0571 (MSE:1.3792, Reg:300.6780) beta=11.00
Iter 14000 | Total loss: 19.1988 (MSE:1.1988, Reg:18.0000) beta=8.75
Iter 16000 | Total loss: 1.2725 (MSE:1.2725, Reg:0.0000) beta=6.50
Iter 18000 | Total loss: 1.3598 (MSE:1.3598, Reg:0.0000) beta=4.25
Iter 20000 | Total loss: 1.4251 (MSE:1.4251, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 56.63%
Total time: 852.23 sec

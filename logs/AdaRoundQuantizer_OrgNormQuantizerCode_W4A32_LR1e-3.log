
Case: [ resnet18_AdaRound_OrgNormQuantizerCode_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: OrgNormQuantizerCode
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is OrgNormQuantizerCode
    Initiated the V
    Parent class is OrgNormQuantizerCode
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2019.8429 (MSE:0.0002, Reg:2019.8427) beta=20.00
Iter  5000 | Total loss: 35.5806 (MSE:0.0017, Reg:35.5789) beta=18.88
Iter  6000 | Total loss: 25.9955 (MSE:0.0013, Reg:25.9942) beta=17.75
Iter  7000 | Total loss: 21.0006 (MSE:0.0013, Reg:20.9993) beta=16.62
Iter  8000 | Total loss: 18.0014 (MSE:0.0014, Reg:18.0000) beta=15.50
Iter  9000 | Total loss: 11.8329 (MSE:0.0013, Reg:11.8315) beta=14.38
Iter 10000 | Total loss: 5.0014 (MSE:0.0014, Reg:5.0000) beta=13.25
Iter 11000 | Total loss: 1.0012 (MSE:0.0012, Reg:1.0000) beta=12.12
Iter 12000 | Total loss: 1.0015 (MSE:0.0015, Reg:1.0000) beta=11.00
Iter 13000 | Total loss: 1.0012 (MSE:0.0012, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0013 (MSE:0.0013, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.7565 (MSE:0.0014, Reg:0.7551) beta=7.62
Iter 16000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5617.8208 (MSE:0.0008, Reg:5617.8198) beta=20.00
Iter  5000 | Total loss: 289.5690 (MSE:0.0028, Reg:289.5662) beta=18.88
Iter  6000 | Total loss: 144.9947 (MSE:0.0030, Reg:144.9917) beta=17.75
Iter  7000 | Total loss: 102.0349 (MSE:0.0030, Reg:102.0319) beta=16.62
Iter  8000 | Total loss: 66.1721 (MSE:0.0030, Reg:66.1691) beta=15.50
Iter  9000 | Total loss: 37.0349 (MSE:0.0030, Reg:37.0318) beta=14.38
Iter 10000 | Total loss: 17.9992 (MSE:0.0027, Reg:17.9964) beta=13.25
Iter 11000 | Total loss: 11.9586 (MSE:0.0028, Reg:11.9557) beta=12.12
Iter 12000 | Total loss: 4.0031 (MSE:0.0031, Reg:4.0000) beta=11.00
Iter 13000 | Total loss: 1.0030 (MSE:0.0030, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0015 (MSE:0.0027, Reg:0.9987) beta=8.75
Iter 15000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7788.1831 (MSE:0.0023, Reg:7788.1807) beta=20.00
Iter  5000 | Total loss: 824.9556 (MSE:0.0028, Reg:824.9528) beta=18.88
Iter  6000 | Total loss: 526.5451 (MSE:0.0028, Reg:526.5424) beta=17.75
Iter  7000 | Total loss: 380.7415 (MSE:0.0029, Reg:380.7387) beta=16.62
Iter  8000 | Total loss: 266.2104 (MSE:0.0030, Reg:266.2074) beta=15.50
Iter  9000 | Total loss: 190.3796 (MSE:0.0029, Reg:190.3767) beta=14.38
Iter 10000 | Total loss: 132.2722 (MSE:0.0029, Reg:132.2693) beta=13.25
Iter 11000 | Total loss: 96.8054 (MSE:0.0026, Reg:96.8028) beta=12.12
Iter 12000 | Total loss: 67.9958 (MSE:0.0029, Reg:67.9930) beta=11.00
Iter 13000 | Total loss: 31.8935 (MSE:0.0026, Reg:31.8909) beta=9.88
Iter 14000 | Total loss: 15.8370 (MSE:0.0029, Reg:15.8341) beta=8.75
Iter 15000 | Total loss: 3.7672 (MSE:0.0028, Reg:3.7643) beta=7.62
Iter 16000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5002.4985 (MSE:0.0022, Reg:5002.4966) beta=20.00
Iter  5000 | Total loss: 344.9154 (MSE:0.0028, Reg:344.9125) beta=18.88
Iter  6000 | Total loss: 219.8199 (MSE:0.0032, Reg:219.8168) beta=17.75
Iter  7000 | Total loss: 153.8244 (MSE:0.0026, Reg:153.8218) beta=16.62
Iter  8000 | Total loss: 100.9360 (MSE:0.0027, Reg:100.9333) beta=15.50
Iter  9000 | Total loss: 70.2631 (MSE:0.0024, Reg:70.2607) beta=14.38
Iter 10000 | Total loss: 52.4205 (MSE:0.0029, Reg:52.4177) beta=13.25
Iter 11000 | Total loss: 38.9984 (MSE:0.0028, Reg:38.9956) beta=12.12
Iter 12000 | Total loss: 30.8711 (MSE:0.0026, Reg:30.8685) beta=11.00
Iter 13000 | Total loss: 19.3878 (MSE:0.0027, Reg:19.3852) beta=9.88
Iter 14000 | Total loss: 10.8119 (MSE:0.0026, Reg:10.8093) beta=8.75
Iter 15000 | Total loss: 2.0943 (MSE:0.0029, Reg:2.0914) beta=7.62
Iter 16000 | Total loss: 1.9917 (MSE:0.0026, Reg:1.9891) beta=6.50
Iter 17000 | Total loss: 1.0023 (MSE:0.0028, Reg:0.9996) beta=5.38
Iter 18000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0119 (MSE:0.0119, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0089 (MSE:0.0089, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7414.5244 (MSE:0.0070, Reg:7414.5176) beta=20.00
Iter  5000 | Total loss: 1180.7687 (MSE:0.0074, Reg:1180.7614) beta=18.88
Iter  6000 | Total loss: 791.8312 (MSE:0.0070, Reg:791.8242) beta=17.75
Iter  7000 | Total loss: 596.3152 (MSE:0.0077, Reg:596.3075) beta=16.62
Iter  8000 | Total loss: 466.2033 (MSE:0.0071, Reg:466.1963) beta=15.50
Iter  9000 | Total loss: 378.4174 (MSE:0.0068, Reg:378.4106) beta=14.38
Iter 10000 | Total loss: 284.6145 (MSE:0.0075, Reg:284.6070) beta=13.25
Iter 11000 | Total loss: 209.7072 (MSE:0.0070, Reg:209.7002) beta=12.12
Iter 12000 | Total loss: 154.3213 (MSE:0.0073, Reg:154.3140) beta=11.00
Iter 13000 | Total loss: 104.6553 (MSE:0.0076, Reg:104.6476) beta=9.88
Iter 14000 | Total loss: 47.7348 (MSE:0.0078, Reg:47.7270) beta=8.75
Iter 15000 | Total loss: 18.3611 (MSE:0.0076, Reg:18.3535) beta=7.62
Iter 16000 | Total loss: 7.5219 (MSE:0.0078, Reg:7.5141) beta=6.50
Iter 17000 | Total loss: 4.6658 (MSE:0.0070, Reg:4.6589) beta=5.38
Iter 18000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9966.8613 (MSE:0.0034, Reg:9966.8574) beta=20.00
Iter  5000 | Total loss: 835.0687 (MSE:0.0034, Reg:835.0653) beta=18.88
Iter  6000 | Total loss: 461.2959 (MSE:0.0034, Reg:461.2925) beta=17.75
Iter  7000 | Total loss: 318.4812 (MSE:0.0033, Reg:318.4779) beta=16.62
Iter  8000 | Total loss: 214.9754 (MSE:0.0034, Reg:214.9720) beta=15.50
Iter  9000 | Total loss: 150.0905 (MSE:0.0033, Reg:150.0872) beta=14.38
Iter 10000 | Total loss: 98.4742 (MSE:0.0036, Reg:98.4706) beta=13.25
Iter 11000 | Total loss: 55.7328 (MSE:0.0036, Reg:55.7292) beta=12.12
Iter 12000 | Total loss: 37.7692 (MSE:0.0038, Reg:37.7654) beta=11.00
Iter 13000 | Total loss: 29.3445 (MSE:0.0034, Reg:29.3411) beta=9.88
Iter 14000 | Total loss: 16.7546 (MSE:0.0035, Reg:16.7511) beta=8.75
Iter 15000 | Total loss: 2.3240 (MSE:0.0035, Reg:2.3206) beta=7.62
Iter 16000 | Total loss: 1.0034 (MSE:0.0034, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 26391.0762 (MSE:0.0049, Reg:26391.0703) beta=20.00
Iter  5000 | Total loss: 2129.0305 (MSE:0.0051, Reg:2129.0254) beta=18.88
Iter  6000 | Total loss: 1268.7625 (MSE:0.0048, Reg:1268.7577) beta=17.75
Iter  7000 | Total loss: 789.2936 (MSE:0.0047, Reg:789.2889) beta=16.62
Iter  8000 | Total loss: 589.2139 (MSE:0.0050, Reg:589.2089) beta=15.50
Iter  9000 | Total loss: 473.6633 (MSE:0.0049, Reg:473.6584) beta=14.38
Iter 10000 | Total loss: 366.5213 (MSE:0.0052, Reg:366.5161) beta=13.25
Iter 11000 | Total loss: 271.6531 (MSE:0.0047, Reg:271.6484) beta=12.12
Iter 12000 | Total loss: 196.9730 (MSE:0.0049, Reg:196.9681) beta=11.00
Iter 13000 | Total loss: 126.4245 (MSE:0.0047, Reg:126.4197) beta=9.88
Iter 14000 | Total loss: 58.2941 (MSE:0.0049, Reg:58.2892) beta=8.75
Iter 15000 | Total loss: 23.2817 (MSE:0.0051, Reg:23.2767) beta=7.62
Iter 16000 | Total loss: 8.1275 (MSE:0.0049, Reg:8.1226) beta=6.50
Iter 17000 | Total loss: 1.5234 (MSE:0.0047, Reg:1.5186) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2407.5112 (MSE:0.0021, Reg:2407.5093) beta=20.00
Iter  5000 | Total loss: 298.3021 (MSE:0.0021, Reg:298.3000) beta=18.88
Iter  6000 | Total loss: 194.4461 (MSE:0.0021, Reg:194.4441) beta=17.75
Iter  7000 | Total loss: 141.9351 (MSE:0.0021, Reg:141.9330) beta=16.62
Iter  8000 | Total loss: 110.5321 (MSE:0.0022, Reg:110.5299) beta=15.50
Iter  9000 | Total loss: 90.1459 (MSE:0.0021, Reg:90.1438) beta=14.38
Iter 10000 | Total loss: 74.8522 (MSE:0.0022, Reg:74.8500) beta=13.25
Iter 11000 | Total loss: 62.7080 (MSE:0.0021, Reg:62.7059) beta=12.12
Iter 12000 | Total loss: 48.7146 (MSE:0.0023, Reg:48.7124) beta=11.00
Iter 13000 | Total loss: 29.6342 (MSE:0.0020, Reg:29.6322) beta=9.88
Iter 14000 | Total loss: 18.3822 (MSE:0.0020, Reg:18.3802) beta=8.75
Iter 15000 | Total loss: 14.2089 (MSE:0.0022, Reg:14.2067) beta=7.62
Iter 16000 | Total loss: 4.5879 (MSE:0.0020, Reg:4.5859) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 24221.7129 (MSE:0.0038, Reg:24221.7090) beta=20.00
Iter  5000 | Total loss: 1960.6621 (MSE:0.0041, Reg:1960.6580) beta=18.88
Iter  6000 | Total loss: 990.5799 (MSE:0.0038, Reg:990.5761) beta=17.75
Iter  7000 | Total loss: 630.1172 (MSE:0.0039, Reg:630.1133) beta=16.62
Iter  8000 | Total loss: 429.1701 (MSE:0.0039, Reg:429.1663) beta=15.50
Iter  9000 | Total loss: 309.9277 (MSE:0.0039, Reg:309.9239) beta=14.38
Iter 10000 | Total loss: 213.6822 (MSE:0.0039, Reg:213.6783) beta=13.25
Iter 11000 | Total loss: 148.5279 (MSE:0.0044, Reg:148.5235) beta=12.12
Iter 12000 | Total loss: 103.3321 (MSE:0.0038, Reg:103.3282) beta=11.00
Iter 13000 | Total loss: 71.6022 (MSE:0.0040, Reg:71.5982) beta=9.88
Iter 14000 | Total loss: 45.8297 (MSE:0.0040, Reg:45.8257) beta=8.75
Iter 15000 | Total loss: 18.7686 (MSE:0.0041, Reg:18.7645) beta=7.62
Iter 16000 | Total loss: 4.2293 (MSE:0.0040, Reg:4.2253) beta=6.50
Iter 17000 | Total loss: 1.6359 (MSE:0.0039, Reg:1.6319) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38528.0352 (MSE:0.0045, Reg:38528.0312) beta=20.00
Iter  5000 | Total loss: 2499.7168 (MSE:0.0048, Reg:2499.7119) beta=18.88
Iter  6000 | Total loss: 1294.4572 (MSE:0.0043, Reg:1294.4529) beta=17.75
Iter  7000 | Total loss: 850.7224 (MSE:0.0045, Reg:850.7180) beta=16.62
Iter  8000 | Total loss: 639.2857 (MSE:0.0044, Reg:639.2812) beta=15.50
Iter  9000 | Total loss: 482.6433 (MSE:0.0048, Reg:482.6385) beta=14.38
Iter 10000 | Total loss: 373.7996 (MSE:0.0046, Reg:373.7950) beta=13.25
Iter 11000 | Total loss: 286.8315 (MSE:0.0043, Reg:286.8272) beta=12.12
Iter 12000 | Total loss: 206.4277 (MSE:0.0047, Reg:206.4230) beta=11.00
Iter 13000 | Total loss: 131.5953 (MSE:0.0046, Reg:131.5908) beta=9.88
Iter 14000 | Total loss: 73.5322 (MSE:0.0043, Reg:73.5279) beta=8.75
Iter 15000 | Total loss: 32.9293 (MSE:0.0046, Reg:32.9247) beta=7.62
Iter 16000 | Total loss: 15.9622 (MSE:0.0044, Reg:15.9577) beta=6.50
Iter 17000 | Total loss: 2.2265 (MSE:0.0044, Reg:2.2221) beta=5.38
Iter 18000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 57889.8555 (MSE:0.0036, Reg:57889.8516) beta=20.00
Iter  5000 | Total loss: 2054.7219 (MSE:0.0041, Reg:2054.7178) beta=18.88
Iter  6000 | Total loss: 917.1904 (MSE:0.0039, Reg:917.1865) beta=17.75
Iter  7000 | Total loss: 545.1816 (MSE:0.0040, Reg:545.1776) beta=16.62
Iter  8000 | Total loss: 393.6076 (MSE:0.0042, Reg:393.6034) beta=15.50
Iter  9000 | Total loss: 292.3011 (MSE:0.0043, Reg:292.2969) beta=14.38
Iter 10000 | Total loss: 208.1183 (MSE:0.0040, Reg:208.1143) beta=13.25
Iter 11000 | Total loss: 152.8861 (MSE:0.0043, Reg:152.8819) beta=12.12
Iter 12000 | Total loss: 113.8347 (MSE:0.0042, Reg:113.8305) beta=11.00
Iter 13000 | Total loss: 74.8327 (MSE:0.0044, Reg:74.8283) beta=9.88
Iter 14000 | Total loss: 42.9638 (MSE:0.0041, Reg:42.9597) beta=8.75
Iter 15000 | Total loss: 19.5665 (MSE:0.0041, Reg:19.5624) beta=7.62
Iter 16000 | Total loss: 8.8917 (MSE:0.0044, Reg:8.8873) beta=6.50
Iter 17000 | Total loss: 2.0041 (MSE:0.0041, Reg:2.0000) beta=5.38
Iter 18000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 110105.9297 (MSE:0.0042, Reg:110105.9219) beta=20.00
Iter  5000 | Total loss: 2011.2717 (MSE:0.0049, Reg:2011.2668) beta=18.88
Iter  6000 | Total loss: 958.5809 (MSE:0.0044, Reg:958.5765) beta=17.75
Iter  7000 | Total loss: 586.3406 (MSE:0.0050, Reg:586.3357) beta=16.62
Iter  8000 | Total loss: 402.2960 (MSE:0.0047, Reg:402.2913) beta=15.50
Iter  9000 | Total loss: 290.1425 (MSE:0.0051, Reg:290.1375) beta=14.38
Iter 10000 | Total loss: 224.3953 (MSE:0.0049, Reg:224.3904) beta=13.25
Iter 11000 | Total loss: 157.8369 (MSE:0.0046, Reg:157.8323) beta=12.12
Iter 12000 | Total loss: 116.6091 (MSE:0.0050, Reg:116.6041) beta=11.00
Iter 13000 | Total loss: 84.2921 (MSE:0.0045, Reg:84.2876) beta=9.88
Iter 14000 | Total loss: 59.1621 (MSE:0.0047, Reg:59.1574) beta=8.75
Iter 15000 | Total loss: 31.5909 (MSE:0.0046, Reg:31.5863) beta=7.62
Iter 16000 | Total loss: 14.8317 (MSE:0.0048, Reg:14.8269) beta=6.50
Iter 17000 | Total loss: 3.5057 (MSE:0.0049, Reg:3.5009) beta=5.38
Iter 18000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 12061.2559 (MSE:0.0004, Reg:12061.2559) beta=20.00
Iter  5000 | Total loss: 196.8784 (MSE:0.0005, Reg:196.8779) beta=18.88
Iter  6000 | Total loss: 79.7673 (MSE:0.0005, Reg:79.7668) beta=17.75
Iter  7000 | Total loss: 48.5441 (MSE:0.0005, Reg:48.5436) beta=16.62
Iter  8000 | Total loss: 32.5434 (MSE:0.0005, Reg:32.5429) beta=15.50
Iter  9000 | Total loss: 29.9430 (MSE:0.0005, Reg:29.9425) beta=14.38
Iter 10000 | Total loss: 22.6219 (MSE:0.0005, Reg:22.6215) beta=13.25
Iter 11000 | Total loss: 11.8253 (MSE:0.0005, Reg:11.8249) beta=12.12
Iter 12000 | Total loss: 10.0002 (MSE:0.0005, Reg:9.9997) beta=11.00
Iter 13000 | Total loss: 7.0005 (MSE:0.0005, Reg:7.0000) beta=9.88
Iter 14000 | Total loss: 7.0005 (MSE:0.0005, Reg:7.0000) beta=8.75
Iter 15000 | Total loss: 6.0005 (MSE:0.0005, Reg:6.0000) beta=7.62
Iter 16000 | Total loss: 1.9900 (MSE:0.0005, Reg:1.9896) beta=6.50
Iter 17000 | Total loss: 0.9965 (MSE:0.0005, Reg:0.9960) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 108837.0156 (MSE:0.0026, Reg:108837.0156) beta=20.00
Iter  5000 | Total loss: 1028.0117 (MSE:0.0036, Reg:1028.0082) beta=18.88
Iter  6000 | Total loss: 504.0511 (MSE:0.0033, Reg:504.0478) beta=17.75
Iter  7000 | Total loss: 328.9084 (MSE:0.0032, Reg:328.9052) beta=16.62
Iter  8000 | Total loss: 253.5650 (MSE:0.0033, Reg:253.5618) beta=15.50
Iter  9000 | Total loss: 186.0848 (MSE:0.0034, Reg:186.0814) beta=14.38
Iter 10000 | Total loss: 140.4707 (MSE:0.0032, Reg:140.4676) beta=13.25
Iter 11000 | Total loss: 92.4351 (MSE:0.0031, Reg:92.4320) beta=12.12
Iter 12000 | Total loss: 63.3049 (MSE:0.0034, Reg:63.3015) beta=11.00
Iter 13000 | Total loss: 38.9971 (MSE:0.0034, Reg:38.9937) beta=9.88
Iter 14000 | Total loss: 22.9912 (MSE:0.0032, Reg:22.9880) beta=8.75
Iter 15000 | Total loss: 12.6235 (MSE:0.0034, Reg:12.6201) beta=7.62
Iter 16000 | Total loss: 5.4664 (MSE:0.0034, Reg:5.4631) beta=6.50
Iter 17000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 146506.8125 (MSE:0.0034, Reg:146506.8125) beta=20.00
Iter  5000 | Total loss: 2037.5033 (MSE:0.0043, Reg:2037.4990) beta=18.88
Iter  6000 | Total loss: 711.9459 (MSE:0.0040, Reg:711.9420) beta=17.75
Iter  7000 | Total loss: 400.8325 (MSE:0.0039, Reg:400.8286) beta=16.62
Iter  8000 | Total loss: 286.5413 (MSE:0.0040, Reg:286.5373) beta=15.50
Iter  9000 | Total loss: 203.2661 (MSE:0.0041, Reg:203.2620) beta=14.38
Iter 10000 | Total loss: 145.1311 (MSE:0.0041, Reg:145.1270) beta=13.25
Iter 11000 | Total loss: 104.7438 (MSE:0.0041, Reg:104.7397) beta=12.12
Iter 12000 | Total loss: 69.6834 (MSE:0.0045, Reg:69.6789) beta=11.00
Iter 13000 | Total loss: 47.6021 (MSE:0.0041, Reg:47.5980) beta=9.88
Iter 14000 | Total loss: 25.7006 (MSE:0.0037, Reg:25.6969) beta=8.75
Iter 15000 | Total loss: 14.0041 (MSE:0.0041, Reg:13.9999) beta=7.62
Iter 16000 | Total loss: 5.4903 (MSE:0.0041, Reg:5.4863) beta=6.50
Iter 17000 | Total loss: 2.7372 (MSE:0.0043, Reg:2.7329) beta=5.38
Iter 18000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 263646.5938 (MSE:0.0031, Reg:263646.5938) beta=20.00
Iter  5000 | Total loss: 186.9514 (MSE:0.0036, Reg:186.9478) beta=18.88
Iter  6000 | Total loss: 91.0436 (MSE:0.0037, Reg:91.0399) beta=17.75
Iter  7000 | Total loss: 70.9866 (MSE:0.0037, Reg:70.9828) beta=16.62
Iter  8000 | Total loss: 48.4120 (MSE:0.0036, Reg:48.4084) beta=15.50
Iter  9000 | Total loss: 33.4957 (MSE:0.0038, Reg:33.4919) beta=14.38
Iter 10000 | Total loss: 23.8116 (MSE:0.0038, Reg:23.8078) beta=13.25
Iter 11000 | Total loss: 14.0038 (MSE:0.0038, Reg:14.0000) beta=12.12
Iter 12000 | Total loss: 11.8759 (MSE:0.0037, Reg:11.8722) beta=11.00
Iter 13000 | Total loss: 6.0034 (MSE:0.0037, Reg:5.9997) beta=9.88
Iter 14000 | Total loss: 2.8328 (MSE:0.0040, Reg:2.8288) beta=8.75
Iter 15000 | Total loss: 0.7159 (MSE:0.0036, Reg:0.7123) beta=7.62
Iter 16000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0110 (MSE:0.0110, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0112 (MSE:0.0112, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0107 (MSE:0.0107, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 521392.6250 (MSE:0.0101, Reg:521392.6250) beta=20.00
Iter  5000 | Total loss: 3042.6262 (MSE:0.0120, Reg:3042.6143) beta=18.88
Iter  6000 | Total loss: 1327.2836 (MSE:0.0111, Reg:1327.2725) beta=17.75
Iter  7000 | Total loss: 861.6086 (MSE:0.0121, Reg:861.5965) beta=16.62
Iter  8000 | Total loss: 611.3152 (MSE:0.0112, Reg:611.3041) beta=15.50
Iter  9000 | Total loss: 444.4999 (MSE:0.0116, Reg:444.4883) beta=14.38
Iter 10000 | Total loss: 325.0771 (MSE:0.0115, Reg:325.0656) beta=13.25
Iter 11000 | Total loss: 231.7344 (MSE:0.0114, Reg:231.7230) beta=12.12
Iter 12000 | Total loss: 171.1768 (MSE:0.0123, Reg:171.1646) beta=11.00
Iter 13000 | Total loss: 113.3307 (MSE:0.0115, Reg:113.3191) beta=9.88
Iter 14000 | Total loss: 77.6594 (MSE:0.0111, Reg:77.6483) beta=8.75
Iter 15000 | Total loss: 31.1215 (MSE:0.0110, Reg:31.1105) beta=7.62
Iter 16000 | Total loss: 15.5546 (MSE:0.0115, Reg:15.5432) beta=6.50
Iter 17000 | Total loss: 2.0361 (MSE:0.0114, Reg:2.0248) beta=5.38
Iter 18000 | Total loss: 0.0117 (MSE:0.0117, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0117 (MSE:0.0117, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0123 (MSE:0.0123, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38636.5742 (MSE:0.0031, Reg:38636.5703) beta=20.00
Iter  5000 | Total loss: 1776.2521 (MSE:0.0039, Reg:1776.2482) beta=18.88
Iter  6000 | Total loss: 901.1215 (MSE:0.0036, Reg:901.1179) beta=17.75
Iter  7000 | Total loss: 597.3176 (MSE:0.0036, Reg:597.3140) beta=16.62
Iter  8000 | Total loss: 457.8290 (MSE:0.0037, Reg:457.8253) beta=15.50
Iter  9000 | Total loss: 355.4471 (MSE:0.0034, Reg:355.4437) beta=14.38
Iter 10000 | Total loss: 277.3864 (MSE:0.0039, Reg:277.3825) beta=13.25
Iter 11000 | Total loss: 202.3476 (MSE:0.0038, Reg:202.3438) beta=12.12
Iter 12000 | Total loss: 142.3396 (MSE:0.0035, Reg:142.3361) beta=11.00
Iter 13000 | Total loss: 99.7429 (MSE:0.0036, Reg:99.7393) beta=9.88
Iter 14000 | Total loss: 58.7340 (MSE:0.0037, Reg:58.7303) beta=8.75
Iter 15000 | Total loss: 32.8827 (MSE:0.0042, Reg:32.8785) beta=7.62
Iter 16000 | Total loss: 17.3731 (MSE:0.0035, Reg:17.3695) beta=6.50
Iter 17000 | Total loss: 3.9327 (MSE:0.0037, Reg:3.9290) beta=5.38
Iter 18000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 750006.8750 (MSE:0.0051, Reg:750006.8750) beta=20.00
Iter  5000 | Total loss: 316.3504 (MSE:0.0063, Reg:316.3441) beta=18.88
Iter  6000 | Total loss: 17.8337 (MSE:0.0068, Reg:17.8269) beta=17.75
Iter  7000 | Total loss: 10.9478 (MSE:0.0064, Reg:10.9414) beta=16.62
Iter  8000 | Total loss: 6.7592 (MSE:0.0067, Reg:6.7525) beta=15.50
Iter  9000 | Total loss: 5.0062 (MSE:0.0062, Reg:5.0000) beta=14.38
Iter 10000 | Total loss: 1.0067 (MSE:0.0067, Reg:1.0000) beta=13.25
Iter 11000 | Total loss: 1.0070 (MSE:0.0070, Reg:1.0000) beta=12.12
Iter 12000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3499 (MSE:0.3499, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3560 (MSE:0.3560, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.3470 (MSE:0.3470, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3156 (MSE:0.3156, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 375069.3125 (MSE:0.3154, Reg:375069.0000) beta=20.00
Iter  5000 | Total loss: 22960.7520 (MSE:0.3271, Reg:22960.4258) beta=18.88
Iter  6000 | Total loss: 12031.3984 (MSE:0.3347, Reg:12031.0635) beta=17.75
Iter  7000 | Total loss: 8113.3071 (MSE:0.3453, Reg:8112.9619) beta=16.62
Iter  8000 | Total loss: 5863.5693 (MSE:0.3452, Reg:5863.2241) beta=15.50
Iter  9000 | Total loss: 4581.3403 (MSE:0.3515, Reg:4580.9888) beta=14.38
Iter 10000 | Total loss: 3715.1580 (MSE:0.3368, Reg:3714.8213) beta=13.25
Iter 11000 | Total loss: 2986.9023 (MSE:0.3380, Reg:2986.5645) beta=12.12
Iter 12000 | Total loss: 2300.3484 (MSE:0.3735, Reg:2299.9749) beta=11.00
Iter 13000 | Total loss: 1746.3909 (MSE:0.3268, Reg:1746.0641) beta=9.88
Iter 14000 | Total loss: 1201.7263 (MSE:0.3728, Reg:1201.3535) beta=8.75
Iter 15000 | Total loss: 725.3350 (MSE:0.3489, Reg:724.9860) beta=7.62
Iter 16000 | Total loss: 354.6014 (MSE:0.3359, Reg:354.2655) beta=6.50
Iter 17000 | Total loss: 104.1626 (MSE:0.3535, Reg:103.8092) beta=5.38
Iter 18000 | Total loss: 5.9532 (MSE:0.3605, Reg:5.5927) beta=4.25
Iter 19000 | Total loss: 0.3538 (MSE:0.3538, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3266 (MSE:0.3266, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3259 (MSE:0.3259, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2724 (MSE:0.2724, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2725 (MSE:0.2725, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2605 (MSE:0.2605, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 93122.0625 (MSE:0.2645, Reg:93121.7969) beta=20.00
Iter  5000 | Total loss: 2879.1855 (MSE:0.3008, Reg:2878.8848) beta=18.88
Iter  6000 | Total loss: 1179.4896 (MSE:0.3698, Reg:1179.1199) beta=17.75
Iter  7000 | Total loss: 766.8757 (MSE:0.3570, Reg:766.5187) beta=16.62
Iter  8000 | Total loss: 528.1403 (MSE:0.3244, Reg:527.8159) beta=15.50
Iter  9000 | Total loss: 394.4651 (MSE:0.3072, Reg:394.1579) beta=14.38
Iter 10000 | Total loss: 311.9611 (MSE:0.2891, Reg:311.6719) beta=13.25
Iter 11000 | Total loss: 260.0853 (MSE:0.3042, Reg:259.7811) beta=12.12
Iter 12000 | Total loss: 213.0553 (MSE:0.3145, Reg:212.7408) beta=11.00
Iter 13000 | Total loss: 162.2302 (MSE:0.3223, Reg:161.9079) beta=9.88
Iter 14000 | Total loss: 115.2940 (MSE:0.2768, Reg:115.0172) beta=8.75
Iter 15000 | Total loss: 77.0550 (MSE:0.3143, Reg:76.7407) beta=7.62
Iter 16000 | Total loss: 43.8525 (MSE:0.3098, Reg:43.5426) beta=6.50
Iter 17000 | Total loss: 15.8760 (MSE:0.2701, Reg:15.6058) beta=5.38
Iter 18000 | Total loss: 1.1132 (MSE:0.3299, Reg:0.7833) beta=4.25
Iter 19000 | Total loss: 0.3317 (MSE:0.3317, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3093 (MSE:0.3093, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 68.360%
Total time: 886.29 sec

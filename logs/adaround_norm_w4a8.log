
Case: [ resnet18_AdaRound_NormQuantizer_CH_W4A8_p2.4_RoundingLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - AdaRound: True

- activation params:
    - scheme: NormQuantizer
    - dstDtype: INT8
    - per_channel: False
    - p: 2.4

Replace to QuantLayer
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantLayer: conv1, torch.Size([64, 3, 7, 7])
    QuantLayer: layer1.0.conv_bn_relu_1, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.0.conv_bn_2, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.1.conv_bn_relu_1, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.1.conv_bn_2, torch.Size([64, 64, 3, 3])
    QuantLayer: layer2.0.conv_bn_relu_1, torch.Size([128, 64, 3, 3])
    QuantLayer: layer2.0.conv_bn_2, torch.Size([128, 128, 3, 3])
    QuantLayer: layer2.0.conv_bn_down, torch.Size([128, 64, 1, 1])
    QuantLayer: layer2.1.conv_bn_relu_1, torch.Size([128, 128, 3, 3])
    QuantLayer: layer2.1.conv_bn_2, torch.Size([128, 128, 3, 3])
    QuantLayer: layer3.0.conv_bn_relu_1, torch.Size([256, 128, 3, 3])
    QuantLayer: layer3.0.conv_bn_2, torch.Size([256, 256, 3, 3])
    QuantLayer: layer3.0.conv_bn_down, torch.Size([256, 128, 1, 1])
    QuantLayer: layer3.1.conv_bn_relu_1, torch.Size([256, 256, 3, 3])
    QuantLayer: layer3.1.conv_bn_2, torch.Size([256, 256, 3, 3])
    QuantLayer: layer4.0.conv_bn_relu_1, torch.Size([512, 256, 3, 3])
    QuantLayer: layer4.0.conv_bn_2, torch.Size([512, 512, 3, 3])
    QuantLayer: layer4.0.conv_bn_down, torch.Size([512, 256, 1, 1])
    QuantLayer: layer4.1.conv_bn_relu_1, torch.Size([512, 512, 3, 3])
    QuantLayer: layer4.1.conv_bn_2, torch.Size([512, 512, 3, 3])
    QuantLayer: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
conv1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_down <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_down <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_down <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_relu_1 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_2 <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
fc <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    1D search with UINT8
    p = 2.4
Activation quantizer initialized from QuantLayer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1183.7050 (MSE:0.0007, Reg:1183.7042) beta=20.00
Iter  5000 | Total loss: 56.0005 (MSE:0.0005, Reg:56.0000) beta=18.88
Iter  6000 | Total loss: 42.0012 (MSE:0.0012, Reg:42.0000) beta=17.75
Iter  7000 | Total loss: 24.0014 (MSE:0.0014, Reg:24.0000) beta=16.62
Iter  8000 | Total loss: 21.0007 (MSE:0.0007, Reg:21.0000) beta=15.50
Iter  9000 | Total loss: 13.0007 (MSE:0.0007, Reg:13.0000) beta=14.38
Iter 10000 | Total loss: 5.0007 (MSE:0.0007, Reg:5.0000) beta=13.25
Iter 11000 | Total loss: 3.0009 (MSE:0.0009, Reg:3.0000) beta=12.12
Iter 12000 | Total loss: 3.0007 (MSE:0.0007, Reg:3.0000) beta=11.00
Iter 13000 | Total loss: 3.0011 (MSE:0.0011, Reg:3.0000) beta=9.88
Iter 14000 | Total loss: 2.0007 (MSE:0.0007, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3374.9590 (MSE:0.0023, Reg:3374.9568) beta=20.00
Iter  5000 | Total loss: 190.6277 (MSE:0.0023, Reg:190.6254) beta=18.88
Iter  6000 | Total loss: 98.1594 (MSE:0.0022, Reg:98.1572) beta=17.75
Iter  7000 | Total loss: 65.0022 (MSE:0.0022, Reg:65.0000) beta=16.62
Iter  8000 | Total loss: 52.9747 (MSE:0.0021, Reg:52.9726) beta=15.50
Iter  9000 | Total loss: 34.2471 (MSE:0.0021, Reg:34.2450) beta=14.38
Iter 10000 | Total loss: 20.0021 (MSE:0.0021, Reg:20.0000) beta=13.25
Iter 11000 | Total loss: 9.4576 (MSE:0.0019, Reg:9.4557) beta=12.12
Iter 12000 | Total loss: 2.0021 (MSE:0.0021, Reg:2.0000) beta=11.00
Iter 13000 | Total loss: 1.0023 (MSE:0.0023, Reg:1.0000) beta=9.88
Iter 14000 | Total loss: 1.0028 (MSE:0.0028, Reg:1.0000) beta=8.75
Iter 15000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0104 (MSE:0.0104, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6435.4160 (MSE:0.0099, Reg:6435.4062) beta=20.00
Iter  5000 | Total loss: 750.6429 (MSE:0.0086, Reg:750.6344) beta=18.88
Iter  6000 | Total loss: 553.2329 (MSE:0.0102, Reg:553.2227) beta=17.75
Iter  7000 | Total loss: 424.7894 (MSE:0.0095, Reg:424.7798) beta=16.62
Iter  8000 | Total loss: 346.6911 (MSE:0.0098, Reg:346.6813) beta=15.50
Iter  9000 | Total loss: 244.4030 (MSE:0.0080, Reg:244.3950) beta=14.38
Iter 10000 | Total loss: 170.9594 (MSE:0.0087, Reg:170.9507) beta=13.25
Iter 11000 | Total loss: 113.6038 (MSE:0.0099, Reg:113.5939) beta=12.12
Iter 12000 | Total loss: 73.6675 (MSE:0.0101, Reg:73.6574) beta=11.00
Iter 13000 | Total loss: 29.0079 (MSE:0.0079, Reg:29.0000) beta=9.88
Iter 14000 | Total loss: 12.0087 (MSE:0.0087, Reg:12.0000) beta=8.75
Iter 15000 | Total loss: 4.0085 (MSE:0.0085, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6463.2769 (MSE:0.0017, Reg:6463.2754) beta=20.00
Iter  5000 | Total loss: 564.3621 (MSE:0.0017, Reg:564.3604) beta=18.88
Iter  6000 | Total loss: 358.9517 (MSE:0.0018, Reg:358.9498) beta=17.75
Iter  7000 | Total loss: 273.7877 (MSE:0.0022, Reg:273.7855) beta=16.62
Iter  8000 | Total loss: 211.8476 (MSE:0.0017, Reg:211.8459) beta=15.50
Iter  9000 | Total loss: 157.3234 (MSE:0.0018, Reg:157.3217) beta=14.38
Iter 10000 | Total loss: 112.9989 (MSE:0.0017, Reg:112.9972) beta=13.25
Iter 11000 | Total loss: 79.7895 (MSE:0.0017, Reg:79.7878) beta=12.12
Iter 12000 | Total loss: 35.0018 (MSE:0.0019, Reg:35.0000) beta=11.00
Iter 13000 | Total loss: 16.0017 (MSE:0.0017, Reg:16.0000) beta=9.88
Iter 14000 | Total loss: 10.0017 (MSE:0.0017, Reg:10.0000) beta=8.75
Iter 15000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0193 (MSE:0.0193, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0184 (MSE:0.0184, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0194 (MSE:0.0194, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8976.4961 (MSE:0.0175, Reg:8976.4785) beta=20.00
Iter  5000 | Total loss: 1111.4468 (MSE:0.0190, Reg:1111.4277) beta=18.88
Iter  6000 | Total loss: 881.8722 (MSE:0.0165, Reg:881.8557) beta=17.75
Iter  7000 | Total loss: 721.9673 (MSE:0.0166, Reg:721.9507) beta=16.62
Iter  8000 | Total loss: 584.5070 (MSE:0.0169, Reg:584.4901) beta=15.50
Iter  9000 | Total loss: 432.9430 (MSE:0.0188, Reg:432.9243) beta=14.38
Iter 10000 | Total loss: 321.3794 (MSE:0.0197, Reg:321.3597) beta=13.25
Iter 11000 | Total loss: 210.9219 (MSE:0.0193, Reg:210.9026) beta=12.12
Iter 12000 | Total loss: 137.5668 (MSE:0.0188, Reg:137.5479) beta=11.00
Iter 13000 | Total loss: 74.0197 (MSE:0.0197, Reg:74.0000) beta=9.88
Iter 14000 | Total loss: 29.8051 (MSE:0.0185, Reg:29.7866) beta=8.75
Iter 15000 | Total loss: 2.0189 (MSE:0.0189, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0194 (MSE:0.0194, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0207 (MSE:0.0207, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0213 (MSE:0.0213, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0177 (MSE:0.0177, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 16056.3223 (MSE:0.0023, Reg:16056.3203) beta=20.00
Iter  5000 | Total loss: 1684.8092 (MSE:0.0024, Reg:1684.8069) beta=18.88
Iter  6000 | Total loss: 1121.2106 (MSE:0.0025, Reg:1121.2080) beta=17.75
Iter  7000 | Total loss: 830.6663 (MSE:0.0023, Reg:830.6641) beta=16.62
Iter  8000 | Total loss: 629.9979 (MSE:0.0022, Reg:629.9956) beta=15.50
Iter  9000 | Total loss: 471.4440 (MSE:0.0024, Reg:471.4417) beta=14.38
Iter 10000 | Total loss: 347.9629 (MSE:0.0022, Reg:347.9607) beta=13.25
Iter 11000 | Total loss: 258.4403 (MSE:0.0023, Reg:258.4380) beta=12.12
Iter 12000 | Total loss: 149.7761 (MSE:0.0023, Reg:149.7739) beta=11.00
Iter 13000 | Total loss: 89.6665 (MSE:0.0022, Reg:89.6643) beta=9.88
Iter 14000 | Total loss: 26.7011 (MSE:0.0023, Reg:26.6988) beta=8.75
Iter 15000 | Total loss: 3.0023 (MSE:0.0023, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 1.0022 (MSE:0.0022, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0095 (MSE:0.0095, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 39394.1016 (MSE:0.0090, Reg:39394.0938) beta=20.00
Iter  5000 | Total loss: 3852.3862 (MSE:0.0086, Reg:3852.3777) beta=18.88
Iter  6000 | Total loss: 3021.5767 (MSE:0.0077, Reg:3021.5688) beta=17.75
Iter  7000 | Total loss: 2476.1543 (MSE:0.0080, Reg:2476.1462) beta=16.62
Iter  8000 | Total loss: 1996.0183 (MSE:0.0080, Reg:1996.0103) beta=15.50
Iter  9000 | Total loss: 1584.0571 (MSE:0.0083, Reg:1584.0488) beta=14.38
Iter 10000 | Total loss: 1220.8427 (MSE:0.0084, Reg:1220.8342) beta=13.25
Iter 11000 | Total loss: 865.5231 (MSE:0.0080, Reg:865.5151) beta=12.12
Iter 12000 | Total loss: 545.7367 (MSE:0.0080, Reg:545.7288) beta=11.00
Iter 13000 | Total loss: 278.3216 (MSE:0.0086, Reg:278.3130) beta=9.88
Iter 14000 | Total loss: 127.4921 (MSE:0.0076, Reg:127.4845) beta=8.75
Iter 15000 | Total loss: 27.2136 (MSE:0.0086, Reg:27.2050) beta=7.62
Iter 16000 | Total loss: 3.0081 (MSE:0.0081, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: conv_bn_down
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2687.9905 (MSE:0.0051, Reg:2687.9854) beta=20.00
Iter  5000 | Total loss: 301.0053 (MSE:0.0053, Reg:301.0000) beta=18.88
Iter  6000 | Total loss: 272.0049 (MSE:0.0049, Reg:272.0000) beta=17.75
Iter  7000 | Total loss: 235.8879 (MSE:0.0047, Reg:235.8832) beta=16.62
Iter  8000 | Total loss: 210.0048 (MSE:0.0048, Reg:210.0000) beta=15.50
Iter  9000 | Total loss: 156.0051 (MSE:0.0051, Reg:156.0000) beta=14.38
Iter 10000 | Total loss: 134.8781 (MSE:0.0047, Reg:134.8734) beta=13.25
Iter 11000 | Total loss: 113.0051 (MSE:0.0051, Reg:113.0000) beta=12.12
Iter 12000 | Total loss: 86.0049 (MSE:0.0049, Reg:86.0000) beta=11.00
Iter 13000 | Total loss: 52.0050 (MSE:0.0050, Reg:52.0000) beta=9.88
Iter 14000 | Total loss: 32.8444 (MSE:0.0056, Reg:32.8388) beta=8.75
Iter 15000 | Total loss: 6.0046 (MSE:0.0046, Reg:6.0000) beta=7.62
Iter 16000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30473.1426 (MSE:0.0012, Reg:30473.1406) beta=20.00
Iter  5000 | Total loss: 3276.9382 (MSE:0.0012, Reg:3276.9370) beta=18.88
Iter  6000 | Total loss: 2272.6497 (MSE:0.0012, Reg:2272.6484) beta=17.75
Iter  7000 | Total loss: 1704.8270 (MSE:0.0012, Reg:1704.8258) beta=16.62
Iter  8000 | Total loss: 1277.7684 (MSE:0.0012, Reg:1277.7672) beta=15.50
Iter  9000 | Total loss: 942.5502 (MSE:0.0013, Reg:942.5490) beta=14.38
Iter 10000 | Total loss: 640.0181 (MSE:0.0012, Reg:640.0169) beta=13.25
Iter 11000 | Total loss: 440.2569 (MSE:0.0013, Reg:440.2556) beta=12.12
Iter 12000 | Total loss: 278.9984 (MSE:0.0012, Reg:278.9972) beta=11.00
Iter 13000 | Total loss: 151.5863 (MSE:0.0012, Reg:151.5852) beta=9.88
Iter 14000 | Total loss: 53.9983 (MSE:0.0013, Reg:53.9970) beta=8.75
Iter 15000 | Total loss: 11.0011 (MSE:0.0011, Reg:11.0000) beta=7.62
Iter 16000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 40490.4961 (MSE:0.0054, Reg:40490.4922) beta=20.00
Iter  5000 | Total loss: 4911.7031 (MSE:0.0070, Reg:4911.6963) beta=18.88
Iter  6000 | Total loss: 3777.2771 (MSE:0.0057, Reg:3777.2715) beta=17.75
Iter  7000 | Total loss: 3050.7449 (MSE:0.0064, Reg:3050.7385) beta=16.62
Iter  8000 | Total loss: 2454.4868 (MSE:0.0053, Reg:2454.4814) beta=15.50
Iter  9000 | Total loss: 1948.0370 (MSE:0.0051, Reg:1948.0319) beta=14.38
Iter 10000 | Total loss: 1458.5359 (MSE:0.0066, Reg:1458.5293) beta=13.25
Iter 11000 | Total loss: 967.4109 (MSE:0.0059, Reg:967.4050) beta=12.12
Iter 12000 | Total loss: 568.9139 (MSE:0.0053, Reg:568.9086) beta=11.00
Iter 13000 | Total loss: 291.0641 (MSE:0.0056, Reg:291.0586) beta=9.88
Iter 14000 | Total loss: 118.9173 (MSE:0.0062, Reg:118.9111) beta=8.75
Iter 15000 | Total loss: 20.0059 (MSE:0.0059, Reg:20.0000) beta=7.62
Iter 16000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 66555.2656 (MSE:0.0021, Reg:66555.2656) beta=20.00
Iter  5000 | Total loss: 8433.3633 (MSE:0.0023, Reg:8433.3613) beta=18.88
Iter  6000 | Total loss: 5777.3745 (MSE:0.0023, Reg:5777.3721) beta=17.75
Iter  7000 | Total loss: 4233.2905 (MSE:0.0020, Reg:4233.2886) beta=16.62
Iter  8000 | Total loss: 3192.1970 (MSE:0.0021, Reg:3192.1948) beta=15.50
Iter  9000 | Total loss: 2347.3689 (MSE:0.0023, Reg:2347.3667) beta=14.38
Iter 10000 | Total loss: 1674.4814 (MSE:0.0022, Reg:1674.4792) beta=13.25
Iter 11000 | Total loss: 1103.6205 (MSE:0.0020, Reg:1103.6184) beta=12.12
Iter 12000 | Total loss: 661.4307 (MSE:0.0020, Reg:661.4287) beta=11.00
Iter 13000 | Total loss: 347.6086 (MSE:0.0022, Reg:347.6064) beta=9.88
Iter 14000 | Total loss: 135.4047 (MSE:0.0021, Reg:135.4026) beta=8.75
Iter 15000 | Total loss: 23.0020 (MSE:0.0022, Reg:22.9999) beta=7.62
Iter 16000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 127141.3203 (MSE:0.0065, Reg:127141.3125) beta=20.00
Iter  5000 | Total loss: 13483.5166 (MSE:0.0067, Reg:13483.5098) beta=18.88
Iter  6000 | Total loss: 9517.4697 (MSE:0.0073, Reg:9517.4629) beta=17.75
Iter  7000 | Total loss: 7132.8457 (MSE:0.0075, Reg:7132.8384) beta=16.62
Iter  8000 | Total loss: 5473.5259 (MSE:0.0078, Reg:5473.5181) beta=15.50
Iter  9000 | Total loss: 4066.8660 (MSE:0.0071, Reg:4066.8589) beta=14.38
Iter 10000 | Total loss: 2869.6018 (MSE:0.0080, Reg:2869.5938) beta=13.25
Iter 11000 | Total loss: 1903.3405 (MSE:0.0065, Reg:1903.3340) beta=12.12
Iter 12000 | Total loss: 1077.4735 (MSE:0.0071, Reg:1077.4664) beta=11.00
Iter 13000 | Total loss: 494.5284 (MSE:0.0065, Reg:494.5220) beta=9.88
Iter 14000 | Total loss: 147.0203 (MSE:0.0067, Reg:147.0135) beta=8.75
Iter 15000 | Total loss: 28.0874 (MSE:0.0071, Reg:28.0803) beta=7.62
Iter 16000 | Total loss: 0.6966 (MSE:0.0069, Reg:0.6897) beta=6.50
Iter 17000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: conv_bn_down
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10921.7822 (MSE:0.0006, Reg:10921.7812) beta=20.00
Iter  5000 | Total loss: 1241.1534 (MSE:0.0006, Reg:1241.1528) beta=18.88
Iter  6000 | Total loss: 1027.5173 (MSE:0.0007, Reg:1027.5167) beta=17.75
Iter  7000 | Total loss: 856.7148 (MSE:0.0006, Reg:856.7142) beta=16.62
Iter  8000 | Total loss: 727.9223 (MSE:0.0006, Reg:727.9217) beta=15.50
Iter  9000 | Total loss: 613.7980 (MSE:0.0006, Reg:613.7974) beta=14.38
Iter 10000 | Total loss: 491.9974 (MSE:0.0006, Reg:491.9968) beta=13.25
Iter 11000 | Total loss: 378.7680 (MSE:0.0006, Reg:378.7674) beta=12.12
Iter 12000 | Total loss: 256.7975 (MSE:0.0006, Reg:256.7969) beta=11.00
Iter 13000 | Total loss: 160.9710 (MSE:0.0006, Reg:160.9704) beta=9.88
Iter 14000 | Total loss: 87.0969 (MSE:0.0006, Reg:87.0963) beta=8.75
Iter 15000 | Total loss: 33.0007 (MSE:0.0007, Reg:33.0000) beta=7.62
Iter 16000 | Total loss: 5.0006 (MSE:0.0006, Reg:5.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 126332.4375 (MSE:0.0006, Reg:126332.4375) beta=20.00
Iter  5000 | Total loss: 6966.8403 (MSE:0.0006, Reg:6966.8398) beta=18.88
Iter  6000 | Total loss: 3323.6826 (MSE:0.0005, Reg:3323.6821) beta=17.75
Iter  7000 | Total loss: 2073.8940 (MSE:0.0006, Reg:2073.8936) beta=16.62
Iter  8000 | Total loss: 1435.5480 (MSE:0.0005, Reg:1435.5475) beta=15.50
Iter  9000 | Total loss: 1041.2678 (MSE:0.0005, Reg:1041.2673) beta=14.38
Iter 10000 | Total loss: 736.1149 (MSE:0.0006, Reg:736.1143) beta=13.25
Iter 11000 | Total loss: 517.2903 (MSE:0.0005, Reg:517.2898) beta=12.12
Iter 12000 | Total loss: 346.4641 (MSE:0.0006, Reg:346.4635) beta=11.00
Iter 13000 | Total loss: 202.2565 (MSE:0.0006, Reg:202.2560) beta=9.88
Iter 14000 | Total loss: 74.3575 (MSE:0.0006, Reg:74.3569) beta=8.75
Iter 15000 | Total loss: 25.3998 (MSE:0.0005, Reg:25.3993) beta=7.62
Iter 16000 | Total loss: 1.9935 (MSE:0.0006, Reg:1.9930) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 133747.3906 (MSE:0.0056, Reg:133747.3906) beta=20.00
Iter  5000 | Total loss: 9669.7109 (MSE:0.0056, Reg:9669.7051) beta=18.88
Iter  6000 | Total loss: 6212.9692 (MSE:0.0051, Reg:6212.9639) beta=17.75
Iter  7000 | Total loss: 4577.9941 (MSE:0.0049, Reg:4577.9893) beta=16.62
Iter  8000 | Total loss: 3501.1455 (MSE:0.0059, Reg:3501.1396) beta=15.50
Iter  9000 | Total loss: 2672.6470 (MSE:0.0059, Reg:2672.6411) beta=14.38
Iter 10000 | Total loss: 1929.2245 (MSE:0.0067, Reg:1929.2178) beta=13.25
Iter 11000 | Total loss: 1341.1810 (MSE:0.0070, Reg:1341.1741) beta=12.12
Iter 12000 | Total loss: 806.4355 (MSE:0.0054, Reg:806.4301) beta=11.00
Iter 13000 | Total loss: 378.1233 (MSE:0.0054, Reg:378.1179) beta=9.88
Iter 14000 | Total loss: 138.9367 (MSE:0.0047, Reg:138.9320) beta=8.75
Iter 15000 | Total loss: 24.3897 (MSE:0.0050, Reg:24.3848) beta=7.62
Iter 16000 | Total loss: 3.0060 (MSE:0.0060, Reg:3.0000) beta=6.50
Iter 17000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 274195.4375 (MSE:0.0008, Reg:274195.4375) beta=20.00
Iter  5000 | Total loss: 11358.1396 (MSE:0.0009, Reg:11358.1387) beta=18.88
Iter  6000 | Total loss: 3498.7009 (MSE:0.0009, Reg:3498.7002) beta=17.75
Iter  7000 | Total loss: 1885.6152 (MSE:0.0007, Reg:1885.6145) beta=16.62
Iter  8000 | Total loss: 1207.8682 (MSE:0.0008, Reg:1207.8674) beta=15.50
Iter  9000 | Total loss: 844.1760 (MSE:0.0008, Reg:844.1752) beta=14.38
Iter 10000 | Total loss: 593.8739 (MSE:0.0008, Reg:593.8731) beta=13.25
Iter 11000 | Total loss: 404.9960 (MSE:0.0009, Reg:404.9952) beta=12.12
Iter 12000 | Total loss: 276.0007 (MSE:0.0008, Reg:275.9999) beta=11.00
Iter 13000 | Total loss: 166.5856 (MSE:0.0008, Reg:166.5848) beta=9.88
Iter 14000 | Total loss: 75.8268 (MSE:0.0008, Reg:75.8260) beta=8.75
Iter 15000 | Total loss: 26.9621 (MSE:0.0008, Reg:26.9613) beta=7.62
Iter 16000 | Total loss: 1.0008 (MSE:0.0008, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0181 (MSE:0.0181, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0148 (MSE:0.0148, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0151 (MSE:0.0151, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 451140.7812 (MSE:0.0146, Reg:451140.7812) beta=20.00
Iter  5000 | Total loss: 64423.5508 (MSE:0.0149, Reg:64423.5352) beta=18.88
Iter  6000 | Total loss: 44763.3047 (MSE:0.0121, Reg:44763.2930) beta=17.75
Iter  7000 | Total loss: 32646.7363 (MSE:0.0120, Reg:32646.7246) beta=16.62
Iter  8000 | Total loss: 23956.0605 (MSE:0.0144, Reg:23956.0469) beta=15.50
Iter  9000 | Total loss: 17231.8418 (MSE:0.0130, Reg:17231.8281) beta=14.38
Iter 10000 | Total loss: 11678.1436 (MSE:0.0157, Reg:11678.1279) beta=13.25
Iter 11000 | Total loss: 7295.1606 (MSE:0.0157, Reg:7295.1450) beta=12.12
Iter 12000 | Total loss: 3904.0679 (MSE:0.0146, Reg:3904.0532) beta=11.00
Iter 13000 | Total loss: 1599.5685 (MSE:0.0153, Reg:1599.5532) beta=9.88
Iter 14000 | Total loss: 382.0014 (MSE:0.0131, Reg:381.9883) beta=8.75
Iter 15000 | Total loss: 28.0002 (MSE:0.0156, Reg:27.9846) beta=7.62
Iter 16000 | Total loss: 0.0142 (MSE:0.0142, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0141 (MSE:0.0141, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0141 (MSE:0.0141, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0137 (MSE:0.0137, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: conv_bn_down
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 34547.5625 (MSE:0.0064, Reg:34547.5547) beta=20.00
Iter  5000 | Total loss: 4229.6440 (MSE:0.0073, Reg:4229.6367) beta=18.88
Iter  6000 | Total loss: 3521.1970 (MSE:0.0071, Reg:3521.1899) beta=17.75
Iter  7000 | Total loss: 3015.7578 (MSE:0.0064, Reg:3015.7515) beta=16.62
Iter  8000 | Total loss: 2520.1914 (MSE:0.0068, Reg:2520.1846) beta=15.50
Iter  9000 | Total loss: 2025.8201 (MSE:0.0065, Reg:2025.8135) beta=14.38
Iter 10000 | Total loss: 1562.5706 (MSE:0.0059, Reg:1562.5647) beta=13.25
Iter 11000 | Total loss: 1079.4441 (MSE:0.0071, Reg:1079.4370) beta=12.12
Iter 12000 | Total loss: 665.8063 (MSE:0.0064, Reg:665.7999) beta=11.00
Iter 13000 | Total loss: 291.6113 (MSE:0.0063, Reg:291.6050) beta=9.88
Iter 14000 | Total loss: 97.0868 (MSE:0.0069, Reg:97.0799) beta=8.75
Iter 15000 | Total loss: 15.0064 (MSE:0.0064, Reg:15.0000) beta=7.62
Iter 16000 | Total loss: 1.0068 (MSE:0.0068, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv_bn_relu_1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 444431.1250 (MSE:0.0010, Reg:444431.1250) beta=20.00
Iter  5000 | Total loss: 9210.4736 (MSE:0.0010, Reg:9210.4727) beta=18.88
Iter  6000 | Total loss: 1135.0421 (MSE:0.0011, Reg:1135.0410) beta=17.75
Iter  7000 | Total loss: 455.7200 (MSE:0.0009, Reg:455.7191) beta=16.62
Iter  8000 | Total loss: 266.1995 (MSE:0.0011, Reg:266.1984) beta=15.50
Iter  9000 | Total loss: 181.8612 (MSE:0.0010, Reg:181.8602) beta=14.38
Iter 10000 | Total loss: 125.7372 (MSE:0.0009, Reg:125.7363) beta=13.25
Iter 11000 | Total loss: 77.8838 (MSE:0.0009, Reg:77.8829) beta=12.12
Iter 12000 | Total loss: 48.9980 (MSE:0.0011, Reg:48.9970) beta=11.00
Iter 13000 | Total loss: 30.0010 (MSE:0.0010, Reg:30.0000) beta=9.88
Iter 14000 | Total loss: 17.0011 (MSE:0.0011, Reg:17.0000) beta=8.75
Iter 15000 | Total loss: 6.0010 (MSE:0.0010, Reg:6.0000) beta=7.62
Iter 16000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv_bn_2
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.4792 (MSE:0.4792, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3994 (MSE:0.3994, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4182 (MSE:0.4182, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4351 (MSE:0.4351, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 285071.3750 (MSE:0.4322, Reg:285070.9375) beta=20.00
Iter  5000 | Total loss: 54409.3750 (MSE:0.4150, Reg:54408.9609) beta=18.88
Iter  6000 | Total loss: 35953.8359 (MSE:0.4381, Reg:35953.3984) beta=17.75
Iter  7000 | Total loss: 23478.8320 (MSE:0.4296, Reg:23478.4023) beta=16.62
Iter  8000 | Total loss: 14731.8213 (MSE:0.4706, Reg:14731.3506) beta=15.50
Iter  9000 | Total loss: 8433.1826 (MSE:0.4068, Reg:8432.7754) beta=14.38
Iter 10000 | Total loss: 4107.8257 (MSE:0.4117, Reg:4107.4141) beta=13.25
Iter 11000 | Total loss: 1610.2971 (MSE:0.4438, Reg:1609.8534) beta=12.12
Iter 12000 | Total loss: 437.5766 (MSE:0.4360, Reg:437.1406) beta=11.00
Iter 13000 | Total loss: 63.6122 (MSE:0.4202, Reg:63.1920) beta=9.88
Iter 14000 | Total loss: 4.4199 (MSE:0.4199, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.4311 (MSE:0.4311, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.4184 (MSE:0.4184, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.4141 (MSE:0.4141, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.4104 (MSE:0.4104, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.4216 (MSE:0.4216, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4372 (MSE:0.4372, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
 <- Commas indicate the INT inference.
    2D search with INT8
    p = 2.4
Activation quantizer initialized from QuantLayer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 4e-05
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.5517 (MSE:0.5517, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2410 (MSE:0.2410, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2209 (MSE:0.2209, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2302 (MSE:0.2302, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 43070.9219 (MSE:0.2263, Reg:43070.6953) beta=20.00
Iter  5000 | Total loss: 8752.2041 (MSE:0.2355, Reg:8751.9688) beta=18.88
Iter  6000 | Total loss: 6412.2642 (MSE:0.2253, Reg:6412.0391) beta=17.75
Iter  7000 | Total loss: 4639.3110 (MSE:0.2205, Reg:4639.0903) beta=16.62
Iter  8000 | Total loss: 3251.6155 (MSE:0.2383, Reg:3251.3772) beta=15.50
Iter  9000 | Total loss: 2109.7727 (MSE:0.2434, Reg:2109.5293) beta=14.38
Iter 10000 | Total loss: 1178.0583 (MSE:0.2204, Reg:1177.8380) beta=13.25
Iter 11000 | Total loss: 523.1729 (MSE:0.2320, Reg:522.9409) beta=12.12
Iter 12000 | Total loss: 188.6673 (MSE:0.2418, Reg:188.4255) beta=11.00
Iter 13000 | Total loss: 45.0704 (MSE:0.2477, Reg:44.8227) beta=9.88
Iter 14000 | Total loss: 6.2343 (MSE:0.2343, Reg:6.0000) beta=8.75
Iter 15000 | Total loss: 0.2175 (MSE:0.2175, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2161 (MSE:0.2161, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2596 (MSE:0.2596, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2291 (MSE:0.2291, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2488 (MSE:0.2488, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2364 (MSE:0.2364, Reg:0.0000) beta=2.00
AdaRound values computing done!

    Quantized model Evaluation accuracy on 50000 images, 64.272%
Total time: 1162.21 sec

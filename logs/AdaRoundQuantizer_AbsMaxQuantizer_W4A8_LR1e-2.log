
Case: [ resnet18_AdaRound_AbsMaxQuantizer_CH_W4A8_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT8
    - per_channel: False

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0097 (MSE:0.0097, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0098 (MSE:0.0098, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0105 (MSE:0.0105, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0107 (MSE:0.0107, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0088 (MSE:0.0088, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0091 (MSE:0.0091, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0092 (MSE:0.0092, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0108 (MSE:0.0108, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0095 (MSE:0.0095, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0181 (MSE:0.0181, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0249 (MSE:0.0249, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0242 (MSE:0.0242, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0223 (MSE:0.0223, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0253 (MSE:0.0253, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0269 (MSE:0.0269, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0247 (MSE:0.0247, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0228 (MSE:0.0228, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0240 (MSE:0.0240, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0334 (MSE:0.0334, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0278 (MSE:0.0278, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0237 (MSE:0.0237, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0252 (MSE:0.0252, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0246 (MSE:0.0246, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0248 (MSE:0.0248, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0241 (MSE:0.0241, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0270 (MSE:0.0270, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0347 (MSE:0.0347, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0437 (MSE:0.0437, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0441 (MSE:0.0441, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0445 (MSE:0.0445, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0443 (MSE:0.0443, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0471 (MSE:0.0471, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0411 (MSE:0.0411, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0436 (MSE:0.0436, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0402 (MSE:0.0402, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0467 (MSE:0.0467, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0434 (MSE:0.0434, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0421 (MSE:0.0421, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0423 (MSE:0.0423, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0433 (MSE:0.0433, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0447 (MSE:0.0447, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0441 (MSE:0.0441, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0428 (MSE:0.0428, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0390 (MSE:0.0390, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0494 (MSE:0.0494, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0439 (MSE:0.0439, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0443 (MSE:0.0443, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0324 (MSE:0.0324, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0350 (MSE:0.0350, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0382 (MSE:0.0382, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0365 (MSE:0.0365, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0372 (MSE:0.0372, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0413 (MSE:0.0413, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0401 (MSE:0.0401, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0410 (MSE:0.0410, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0419 (MSE:0.0419, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0352 (MSE:0.0352, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0374 (MSE:0.0374, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0386 (MSE:0.0386, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0373 (MSE:0.0373, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0377 (MSE:0.0377, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0360 (MSE:0.0360, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0391 (MSE:0.0391, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0431 (MSE:0.0431, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0363 (MSE:0.0363, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0411 (MSE:0.0411, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0333 (MSE:0.0333, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0404 (MSE:0.0404, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0889 (MSE:0.0889, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1115 (MSE:0.1115, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0870 (MSE:0.0870, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0880 (MSE:0.0880, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0953 (MSE:0.0953, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1057 (MSE:0.1057, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1015 (MSE:0.1015, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0875 (MSE:0.0875, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0931 (MSE:0.0931, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0924 (MSE:0.0924, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1006 (MSE:0.1006, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1018 (MSE:0.1018, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0944 (MSE:0.0944, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0949 (MSE:0.0949, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1096 (MSE:0.1096, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0920 (MSE:0.0920, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0927 (MSE:0.0927, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1015 (MSE:0.1015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1011 (MSE:0.1011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1003 (MSE:0.1003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1012 (MSE:0.1012, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0360 (MSE:0.0360, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0360 (MSE:0.0360, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0360 (MSE:0.0360, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0357 (MSE:0.0357, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0352 (MSE:0.0352, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0369 (MSE:0.0369, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0397 (MSE:0.0397, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0356 (MSE:0.0356, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0358 (MSE:0.0358, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0388 (MSE:0.0388, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0365 (MSE:0.0365, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0372 (MSE:0.0372, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0386 (MSE:0.0386, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0405 (MSE:0.0405, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0362 (MSE:0.0362, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0390 (MSE:0.0390, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0389 (MSE:0.0389, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0389 (MSE:0.0389, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0400 (MSE:0.0400, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0335 (MSE:0.0335, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0398 (MSE:0.0398, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0518 (MSE:0.0518, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0495 (MSE:0.0495, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0548 (MSE:0.0548, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0549 (MSE:0.0549, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0553 (MSE:0.0553, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0524 (MSE:0.0524, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0567 (MSE:0.0567, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0577 (MSE:0.0577, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0547 (MSE:0.0547, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0526 (MSE:0.0526, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0576 (MSE:0.0576, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0518 (MSE:0.0518, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0524 (MSE:0.0524, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0579 (MSE:0.0579, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0538 (MSE:0.0538, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0534 (MSE:0.0534, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0553 (MSE:0.0553, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0541 (MSE:0.0541, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0608 (MSE:0.0608, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0573 (MSE:0.0573, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0540 (MSE:0.0540, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0225 (MSE:0.0225, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0242 (MSE:0.0242, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0237 (MSE:0.0237, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0269 (MSE:0.0269, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0259 (MSE:0.0259, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0274 (MSE:0.0274, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0246 (MSE:0.0246, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0240 (MSE:0.0240, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0250 (MSE:0.0250, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0237 (MSE:0.0237, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0255 (MSE:0.0255, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0237 (MSE:0.0237, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0241 (MSE:0.0241, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0402 (MSE:0.0402, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0466 (MSE:0.0466, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0503 (MSE:0.0503, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0432 (MSE:0.0432, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0462 (MSE:0.0462, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0489 (MSE:0.0489, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0472 (MSE:0.0472, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0487 (MSE:0.0487, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0432 (MSE:0.0432, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0481 (MSE:0.0481, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0446 (MSE:0.0446, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0473 (MSE:0.0473, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0473 (MSE:0.0473, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0482 (MSE:0.0482, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0465 (MSE:0.0465, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0474 (MSE:0.0474, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0473 (MSE:0.0473, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0491 (MSE:0.0491, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0551 (MSE:0.0551, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0528 (MSE:0.0528, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0518 (MSE:0.0518, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0555 (MSE:0.0555, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0525 (MSE:0.0525, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0509 (MSE:0.0509, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0516 (MSE:0.0516, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0543 (MSE:0.0543, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0539 (MSE:0.0539, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0545 (MSE:0.0545, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0558 (MSE:0.0558, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0507 (MSE:0.0507, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0499 (MSE:0.0499, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0478 (MSE:0.0478, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0505 (MSE:0.0505, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0529 (MSE:0.0529, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0532 (MSE:0.0532, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0600 (MSE:0.0600, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0538 (MSE:0.0538, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0547 (MSE:0.0547, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0496 (MSE:0.0496, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0500 (MSE:0.0500, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0520 (MSE:0.0520, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0527 (MSE:0.0527, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0516 (MSE:0.0516, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0514 (MSE:0.0514, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0502 (MSE:0.0502, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0499 (MSE:0.0499, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0512 (MSE:0.0512, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0518 (MSE:0.0518, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0495 (MSE:0.0495, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0503 (MSE:0.0503, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0519 (MSE:0.0519, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0474 (MSE:0.0474, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0538 (MSE:0.0538, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0499 (MSE:0.0499, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0494 (MSE:0.0494, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0613 (MSE:0.0613, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0608 (MSE:0.0608, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0623 (MSE:0.0623, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0646 (MSE:0.0646, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0631 (MSE:0.0631, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0590 (MSE:0.0590, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0608 (MSE:0.0608, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0635 (MSE:0.0635, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0625 (MSE:0.0625, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0599 (MSE:0.0599, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0611 (MSE:0.0611, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0623 (MSE:0.0623, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0625 (MSE:0.0625, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0617 (MSE:0.0617, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0612 (MSE:0.0612, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0624 (MSE:0.0624, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0657 (MSE:0.0657, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0609 (MSE:0.0609, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0621 (MSE:0.0621, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0662 (MSE:0.0662, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0680 (MSE:0.0680, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0532 (MSE:0.0532, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0665 (MSE:0.0665, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0657 (MSE:0.0657, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0654 (MSE:0.0654, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0699 (MSE:0.0699, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0678 (MSE:0.0678, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0703 (MSE:0.0703, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0632 (MSE:0.0632, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0636 (MSE:0.0636, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0741 (MSE:0.0741, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0621 (MSE:0.0621, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0701 (MSE:0.0701, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0659 (MSE:0.0659, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0642 (MSE:0.0642, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0662 (MSE:0.0662, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0655 (MSE:0.0655, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0701 (MSE:0.0701, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0666 (MSE:0.0666, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0679 (MSE:0.0679, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0709 (MSE:0.0709, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0996 (MSE:0.0996, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0973 (MSE:0.0973, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1003 (MSE:0.1003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1054 (MSE:0.1054, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1009 (MSE:0.1009, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0937 (MSE:0.0937, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0900 (MSE:0.0900, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0952 (MSE:0.0952, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0996 (MSE:0.0996, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0930 (MSE:0.0930, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0972 (MSE:0.0972, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0901 (MSE:0.0901, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1006 (MSE:0.1006, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0996 (MSE:0.0996, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0949 (MSE:0.0949, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1039 (MSE:0.1039, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0896 (MSE:0.0896, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0969 (MSE:0.0969, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0912 (MSE:0.0912, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0994 (MSE:0.0994, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1021 (MSE:0.1021, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0498 (MSE:0.0498, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0512 (MSE:0.0512, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0540 (MSE:0.0540, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0499 (MSE:0.0499, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0506 (MSE:0.0506, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0561 (MSE:0.0561, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0564 (MSE:0.0564, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0512 (MSE:0.0512, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0468 (MSE:0.0468, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0494 (MSE:0.0494, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0528 (MSE:0.0528, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0501 (MSE:0.0501, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0496 (MSE:0.0496, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0516 (MSE:0.0516, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0522 (MSE:0.0522, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0515 (MSE:0.0515, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0512 (MSE:0.0512, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0561 (MSE:0.0561, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1719 (MSE:0.1719, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1810 (MSE:0.1810, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2085 (MSE:0.2085, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1815 (MSE:0.1815, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1737 (MSE:0.1737, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1711 (MSE:0.1711, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1870 (MSE:0.1870, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1687 (MSE:0.1687, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.2139 (MSE:0.2139, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.2042 (MSE:0.2042, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1972 (MSE:0.1972, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1633 (MSE:0.1633, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1957 (MSE:0.1957, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1851 (MSE:0.1851, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1981 (MSE:0.1981, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1887 (MSE:0.1887, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1672 (MSE:0.1672, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1881 (MSE:0.1881, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1826 (MSE:0.1826, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2073 (MSE:0.2073, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1877 (MSE:0.1877, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0503 (MSE:0.0503, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0494 (MSE:0.0494, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0514 (MSE:0.0514, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0500 (MSE:0.0500, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0505 (MSE:0.0505, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0514 (MSE:0.0514, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0478 (MSE:0.0478, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0569 (MSE:0.0569, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0475 (MSE:0.0475, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0503 (MSE:0.0503, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0500 (MSE:0.0500, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0461 (MSE:0.0461, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0492 (MSE:0.0492, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0458 (MSE:0.0458, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0569 (MSE:0.0569, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0557 (MSE:0.0557, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0877 (MSE:0.0877, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0971 (MSE:0.0971, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1139 (MSE:0.1139, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0959 (MSE:0.0959, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1072 (MSE:0.1072, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1109 (MSE:0.1109, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1141 (MSE:0.1141, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1058 (MSE:0.1058, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0982 (MSE:0.0982, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.1024 (MSE:0.1024, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1025 (MSE:0.1025, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1128 (MSE:0.1128, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1078 (MSE:0.1078, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1113 (MSE:0.1113, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1237 (MSE:0.1237, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1065 (MSE:0.1065, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1071 (MSE:0.1071, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1027 (MSE:0.1027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0964 (MSE:0.0964, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1047 (MSE:0.1047, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1021 (MSE:0.1021, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 3.9538 (MSE:3.9538, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 3.8751 (MSE:3.8751, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 3.5735 (MSE:3.5735, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 3.5552 (MSE:3.5552, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3.9507 (MSE:3.9507, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 3.9109 (MSE:3.9109, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 3.7049 (MSE:3.7049, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 3.9386 (MSE:3.9386, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 3.9107 (MSE:3.9107, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 3.8265 (MSE:3.8265, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 3.6430 (MSE:3.6430, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 3.6470 (MSE:3.6470, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 3.6944 (MSE:3.6944, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 3.6919 (MSE:3.6919, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 3.3890 (MSE:3.3890, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 3.6480 (MSE:3.6480, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 3.9163 (MSE:3.9163, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 3.5119 (MSE:3.5119, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 3.6627 (MSE:3.6627, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 3.5587 (MSE:3.5587, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 3.4734 (MSE:3.4734, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 7.6809 (MSE:7.6809, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 6.6426 (MSE:6.6426, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 6.7955 (MSE:6.7955, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 6.9279 (MSE:6.9279, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7.2787 (MSE:7.2787, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 7.6955 (MSE:7.6955, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 6.4900 (MSE:6.4900, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 7.5191 (MSE:7.5191, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 7.2811 (MSE:7.2811, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 6.8267 (MSE:6.8267, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 7.0094 (MSE:7.0094, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 6.9064 (MSE:6.9064, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 6.8433 (MSE:6.8433, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 7.7516 (MSE:7.7516, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 7.6063 (MSE:7.6063, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 7.5126 (MSE:7.5126, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 6.6084 (MSE:6.6084, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 7.1703 (MSE:7.1703, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 7.1067 (MSE:7.1067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 6.9664 (MSE:6.9664, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 8.0428 (MSE:8.0428, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 0.176%
Total time: 1206.24 sec

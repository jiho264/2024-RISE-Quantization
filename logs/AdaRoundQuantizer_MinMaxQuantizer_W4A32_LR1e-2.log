
Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 873.9561 (MSE:0.0003, Reg:873.9558) beta=20.00
Iter  5000 | Total loss: 39.0003 (MSE:0.0003, Reg:39.0000) beta=18.88
Iter  6000 | Total loss: 30.0003 (MSE:0.0003, Reg:30.0000) beta=17.75
Iter  7000 | Total loss: 23.5225 (MSE:0.0003, Reg:23.5221) beta=16.62
Iter  8000 | Total loss: 21.0003 (MSE:0.0003, Reg:21.0000) beta=15.50
Iter  9000 | Total loss: 17.0003 (MSE:0.0003, Reg:17.0000) beta=14.38
Iter 10000 | Total loss: 12.0004 (MSE:0.0004, Reg:12.0000) beta=13.25
Iter 11000 | Total loss: 10.0004 (MSE:0.0004, Reg:10.0000) beta=12.12
Iter 12000 | Total loss: 6.0004 (MSE:0.0004, Reg:6.0000) beta=11.00
Iter 13000 | Total loss: 4.0003 (MSE:0.0003, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1911.1571 (MSE:0.0006, Reg:1911.1565) beta=20.00
Iter  5000 | Total loss: 68.9288 (MSE:0.0005, Reg:68.9283) beta=18.88
Iter  6000 | Total loss: 47.0006 (MSE:0.0007, Reg:46.9999) beta=17.75
Iter  7000 | Total loss: 30.0007 (MSE:0.0007, Reg:30.0000) beta=16.62
Iter  8000 | Total loss: 27.0007 (MSE:0.0007, Reg:27.0000) beta=15.50
Iter  9000 | Total loss: 22.0007 (MSE:0.0007, Reg:22.0000) beta=14.38
Iter 10000 | Total loss: 16.0006 (MSE:0.0006, Reg:16.0000) beta=13.25
Iter 11000 | Total loss: 7.0006 (MSE:0.0006, Reg:7.0000) beta=12.12
Iter 12000 | Total loss: 5.0007 (MSE:0.0007, Reg:5.0000) beta=11.00
Iter 13000 | Total loss: 3.0006 (MSE:0.0006, Reg:3.0000) beta=9.88
Iter 14000 | Total loss: 2.0005 (MSE:0.0005, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3468.7874 (MSE:0.0013, Reg:3468.7861) beta=20.00
Iter  5000 | Total loss: 354.5594 (MSE:0.0011, Reg:354.5582) beta=18.88
Iter  6000 | Total loss: 239.9501 (MSE:0.0011, Reg:239.9490) beta=17.75
Iter  7000 | Total loss: 191.0012 (MSE:0.0012, Reg:191.0000) beta=16.62
Iter  8000 | Total loss: 137.0013 (MSE:0.0013, Reg:137.0000) beta=15.50
Iter  9000 | Total loss: 99.0012 (MSE:0.0012, Reg:99.0000) beta=14.38
Iter 10000 | Total loss: 69.8479 (MSE:0.0012, Reg:69.8466) beta=13.25
Iter 11000 | Total loss: 40.9976 (MSE:0.0011, Reg:40.9965) beta=12.12
Iter 12000 | Total loss: 23.0012 (MSE:0.0012, Reg:23.0000) beta=11.00
Iter 13000 | Total loss: 14.0010 (MSE:0.0010, Reg:14.0000) beta=9.88
Iter 14000 | Total loss: 3.0012 (MSE:0.0012, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2527.5974 (MSE:0.0014, Reg:2527.5959) beta=20.00
Iter  5000 | Total loss: 163.9904 (MSE:0.0015, Reg:163.9889) beta=18.88
Iter  6000 | Total loss: 135.0017 (MSE:0.0017, Reg:135.0000) beta=17.75
Iter  7000 | Total loss: 119.3678 (MSE:0.0014, Reg:119.3664) beta=16.62
Iter  8000 | Total loss: 81.0015 (MSE:0.0015, Reg:81.0000) beta=15.50
Iter  9000 | Total loss: 51.0012 (MSE:0.0012, Reg:51.0000) beta=14.38
Iter 10000 | Total loss: 37.9049 (MSE:0.0016, Reg:37.9033) beta=13.25
Iter 11000 | Total loss: 24.9419 (MSE:0.0015, Reg:24.9404) beta=12.12
Iter 12000 | Total loss: 13.9534 (MSE:0.0014, Reg:13.9520) beta=11.00
Iter 13000 | Total loss: 4.0014 (MSE:0.0014, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 3.0014 (MSE:0.0014, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 1.0015 (MSE:0.0015, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0096 (MSE:0.0096, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5058.9272 (MSE:0.0046, Reg:5058.9229) beta=20.00
Iter  5000 | Total loss: 638.6564 (MSE:0.0048, Reg:638.6516) beta=18.88
Iter  6000 | Total loss: 513.9951 (MSE:0.0047, Reg:513.9904) beta=17.75
Iter  7000 | Total loss: 417.0033 (MSE:0.0052, Reg:416.9981) beta=16.62
Iter  8000 | Total loss: 337.5580 (MSE:0.0046, Reg:337.5534) beta=15.50
Iter  9000 | Total loss: 278.9332 (MSE:0.0045, Reg:278.9287) beta=14.38
Iter 10000 | Total loss: 201.5486 (MSE:0.0051, Reg:201.5435) beta=13.25
Iter 11000 | Total loss: 118.1397 (MSE:0.0046, Reg:118.1350) beta=12.12
Iter 12000 | Total loss: 72.7325 (MSE:0.0049, Reg:72.7276) beta=11.00
Iter 13000 | Total loss: 21.8666 (MSE:0.0052, Reg:21.8615) beta=9.88
Iter 14000 | Total loss: 1.3080 (MSE:0.0052, Reg:1.3028) beta=8.75
Iter 15000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5585.1753 (MSE:0.0023, Reg:5585.1729) beta=20.00
Iter  5000 | Total loss: 452.7102 (MSE:0.0022, Reg:452.7081) beta=18.88
Iter  6000 | Total loss: 357.9644 (MSE:0.0021, Reg:357.9622) beta=17.75
Iter  7000 | Total loss: 295.5795 (MSE:0.0021, Reg:295.5773) beta=16.62
Iter  8000 | Total loss: 207.8920 (MSE:0.0023, Reg:207.8897) beta=15.50
Iter  9000 | Total loss: 151.0021 (MSE:0.0021, Reg:151.0000) beta=14.38
Iter 10000 | Total loss: 94.0017 (MSE:0.0023, Reg:93.9994) beta=13.25
Iter 11000 | Total loss: 60.9990 (MSE:0.0023, Reg:60.9967) beta=12.12
Iter 12000 | Total loss: 26.9612 (MSE:0.0025, Reg:26.9588) beta=11.00
Iter 13000 | Total loss: 14.3329 (MSE:0.0022, Reg:14.3306) beta=9.88
Iter 14000 | Total loss: 4.0022 (MSE:0.0022, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 1.0024 (MSE:0.0024, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 18176.8789 (MSE:0.0041, Reg:18176.8750) beta=20.00
Iter  5000 | Total loss: 1259.7826 (MSE:0.0040, Reg:1259.7786) beta=18.88
Iter  6000 | Total loss: 993.9940 (MSE:0.0038, Reg:993.9901) beta=17.75
Iter  7000 | Total loss: 855.9396 (MSE:0.0037, Reg:855.9359) beta=16.62
Iter  8000 | Total loss: 715.1279 (MSE:0.0040, Reg:715.1239) beta=15.50
Iter  9000 | Total loss: 579.1965 (MSE:0.0038, Reg:579.1926) beta=14.38
Iter 10000 | Total loss: 412.8893 (MSE:0.0042, Reg:412.8850) beta=13.25
Iter 11000 | Total loss: 262.1244 (MSE:0.0036, Reg:262.1207) beta=12.12
Iter 12000 | Total loss: 100.6508 (MSE:0.0039, Reg:100.6469) beta=11.00
Iter 13000 | Total loss: 46.0026 (MSE:0.0037, Reg:45.9989) beta=9.88
Iter 14000 | Total loss: 19.0040 (MSE:0.0040, Reg:19.0000) beta=8.75
Iter 15000 | Total loss: 2.0040 (MSE:0.0040, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2051.7092 (MSE:0.0013, Reg:2051.7080) beta=20.00
Iter  5000 | Total loss: 151.8062 (MSE:0.0013, Reg:151.8049) beta=18.88
Iter  6000 | Total loss: 131.9579 (MSE:0.0014, Reg:131.9565) beta=17.75
Iter  7000 | Total loss: 107.8720 (MSE:0.0013, Reg:107.8707) beta=16.62
Iter  8000 | Total loss: 89.0015 (MSE:0.0015, Reg:89.0000) beta=15.50
Iter  9000 | Total loss: 70.8050 (MSE:0.0013, Reg:70.8037) beta=14.38
Iter 10000 | Total loss: 43.6853 (MSE:0.0014, Reg:43.6838) beta=13.25
Iter 11000 | Total loss: 35.0014 (MSE:0.0014, Reg:35.0000) beta=12.12
Iter 12000 | Total loss: 24.0014 (MSE:0.0014, Reg:24.0000) beta=11.00
Iter 13000 | Total loss: 11.0014 (MSE:0.0014, Reg:11.0000) beta=9.88
Iter 14000 | Total loss: 4.0013 (MSE:0.0013, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15533.4863 (MSE:0.0032, Reg:15533.4834) beta=20.00
Iter  5000 | Total loss: 1163.1727 (MSE:0.0033, Reg:1163.1694) beta=18.88
Iter  6000 | Total loss: 911.9946 (MSE:0.0030, Reg:911.9916) beta=17.75
Iter  7000 | Total loss: 776.9157 (MSE:0.0032, Reg:776.9125) beta=16.62
Iter  8000 | Total loss: 643.0435 (MSE:0.0031, Reg:643.0404) beta=15.50
Iter  9000 | Total loss: 504.9605 (MSE:0.0032, Reg:504.9573) beta=14.38
Iter 10000 | Total loss: 325.5803 (MSE:0.0031, Reg:325.5771) beta=13.25
Iter 11000 | Total loss: 177.0737 (MSE:0.0037, Reg:177.0700) beta=12.12
Iter 12000 | Total loss: 94.0020 (MSE:0.0030, Reg:93.9990) beta=11.00
Iter 13000 | Total loss: 35.8421 (MSE:0.0034, Reg:35.8387) beta=9.88
Iter 14000 | Total loss: 6.0032 (MSE:0.0032, Reg:6.0000) beta=8.75
Iter 15000 | Total loss: 1.0035 (MSE:0.0035, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 31898.2773 (MSE:0.0035, Reg:31898.2734) beta=20.00
Iter  5000 | Total loss: 2548.8486 (MSE:0.0039, Reg:2548.8447) beta=18.88
Iter  6000 | Total loss: 1997.8070 (MSE:0.0034, Reg:1997.8036) beta=17.75
Iter  7000 | Total loss: 1654.9290 (MSE:0.0036, Reg:1654.9254) beta=16.62
Iter  8000 | Total loss: 1350.0825 (MSE:0.0036, Reg:1350.0790) beta=15.50
Iter  9000 | Total loss: 1080.7465 (MSE:0.0040, Reg:1080.7424) beta=14.38
Iter 10000 | Total loss: 754.6381 (MSE:0.0038, Reg:754.6343) beta=13.25
Iter 11000 | Total loss: 464.7580 (MSE:0.0036, Reg:464.7544) beta=12.12
Iter 12000 | Total loss: 236.7227 (MSE:0.0038, Reg:236.7190) beta=11.00
Iter 13000 | Total loss: 87.9135 (MSE:0.0038, Reg:87.9098) beta=9.88
Iter 14000 | Total loss: 18.0035 (MSE:0.0035, Reg:18.0000) beta=8.75
Iter 15000 | Total loss: 2.0038 (MSE:0.0038, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 31991.5332 (MSE:0.0028, Reg:31991.5312) beta=20.00
Iter  5000 | Total loss: 2268.1594 (MSE:0.0027, Reg:2268.1567) beta=18.88
Iter  6000 | Total loss: 1679.2034 (MSE:0.0026, Reg:1679.2007) beta=17.75
Iter  7000 | Total loss: 1381.9308 (MSE:0.0027, Reg:1381.9281) beta=16.62
Iter  8000 | Total loss: 1112.4548 (MSE:0.0028, Reg:1112.4520) beta=15.50
Iter  9000 | Total loss: 859.7495 (MSE:0.0028, Reg:859.7467) beta=14.38
Iter 10000 | Total loss: 610.9188 (MSE:0.0028, Reg:610.9159) beta=13.25
Iter 11000 | Total loss: 364.9133 (MSE:0.0030, Reg:364.9103) beta=12.12
Iter 12000 | Total loss: 171.4119 (MSE:0.0029, Reg:171.4089) beta=11.00
Iter 13000 | Total loss: 61.2293 (MSE:0.0029, Reg:61.2264) beta=9.88
Iter 14000 | Total loss: 18.0028 (MSE:0.0028, Reg:18.0000) beta=8.75
Iter 15000 | Total loss: 2.2526 (MSE:0.0028, Reg:2.2498) beta=7.62
Iter 16000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 70221.9531 (MSE:0.0033, Reg:70221.9531) beta=20.00
Iter  5000 | Total loss: 4784.1128 (MSE:0.0036, Reg:4784.1094) beta=18.88
Iter  6000 | Total loss: 3388.7358 (MSE:0.0032, Reg:3388.7327) beta=17.75
Iter  7000 | Total loss: 2640.5500 (MSE:0.0035, Reg:2640.5466) beta=16.62
Iter  8000 | Total loss: 2131.2241 (MSE:0.0035, Reg:2131.2207) beta=15.50
Iter  9000 | Total loss: 1618.4088 (MSE:0.0036, Reg:1618.4053) beta=14.38
Iter 10000 | Total loss: 1131.0427 (MSE:0.0036, Reg:1131.0391) beta=13.25
Iter 11000 | Total loss: 718.6316 (MSE:0.0035, Reg:718.6281) beta=12.12
Iter 12000 | Total loss: 368.8526 (MSE:0.0035, Reg:368.8491) beta=11.00
Iter 13000 | Total loss: 129.6395 (MSE:0.0034, Reg:129.6361) beta=9.88
Iter 14000 | Total loss: 27.9981 (MSE:0.0035, Reg:27.9947) beta=8.75
Iter 15000 | Total loss: 1.0031 (MSE:0.0031, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 10067.4238 (MSE:0.0003, Reg:10067.4238) beta=20.00
Iter  5000 | Total loss: 708.9063 (MSE:0.0003, Reg:708.9060) beta=18.88
Iter  6000 | Total loss: 583.6230 (MSE:0.0004, Reg:583.6226) beta=17.75
Iter  7000 | Total loss: 483.1445 (MSE:0.0004, Reg:483.1442) beta=16.62
Iter  8000 | Total loss: 402.9995 (MSE:0.0003, Reg:402.9992) beta=15.50
Iter  9000 | Total loss: 317.9565 (MSE:0.0003, Reg:317.9561) beta=14.38
Iter 10000 | Total loss: 218.9386 (MSE:0.0003, Reg:218.9383) beta=13.25
Iter 11000 | Total loss: 132.3642 (MSE:0.0003, Reg:132.3639) beta=12.12
Iter 12000 | Total loss: 71.7541 (MSE:0.0004, Reg:71.7537) beta=11.00
Iter 13000 | Total loss: 46.0004 (MSE:0.0004, Reg:46.0000) beta=9.88
Iter 14000 | Total loss: 23.0003 (MSE:0.0003, Reg:23.0000) beta=8.75
Iter 15000 | Total loss: 7.2444 (MSE:0.0004, Reg:7.2440) beta=7.62
Iter 16000 | Total loss: 1.0003 (MSE:0.0003, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 56846.1836 (MSE:0.0020, Reg:56846.1797) beta=20.00
Iter  5000 | Total loss: 3447.0066 (MSE:0.0023, Reg:3447.0044) beta=18.88
Iter  6000 | Total loss: 2304.8855 (MSE:0.0021, Reg:2304.8833) beta=17.75
Iter  7000 | Total loss: 1718.8755 (MSE:0.0021, Reg:1718.8734) beta=16.62
Iter  8000 | Total loss: 1326.5035 (MSE:0.0022, Reg:1326.5013) beta=15.50
Iter  9000 | Total loss: 994.5872 (MSE:0.0021, Reg:994.5851) beta=14.38
Iter 10000 | Total loss: 680.7015 (MSE:0.0020, Reg:680.6995) beta=13.25
Iter 11000 | Total loss: 452.2258 (MSE:0.0020, Reg:452.2238) beta=12.12
Iter 12000 | Total loss: 242.9099 (MSE:0.0021, Reg:242.9078) beta=11.00
Iter 13000 | Total loss: 91.0021 (MSE:0.0021, Reg:90.9999) beta=9.88
Iter 14000 | Total loss: 28.0020 (MSE:0.0020, Reg:28.0000) beta=8.75
Iter 15000 | Total loss: 2.0022 (MSE:0.0022, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 120154.2344 (MSE:0.0024, Reg:120154.2344) beta=20.00
Iter  5000 | Total loss: 5981.9434 (MSE:0.0026, Reg:5981.9409) beta=18.88
Iter  6000 | Total loss: 3944.0046 (MSE:0.0026, Reg:3944.0020) beta=17.75
Iter  7000 | Total loss: 2982.5188 (MSE:0.0026, Reg:2982.5161) beta=16.62
Iter  8000 | Total loss: 2328.5854 (MSE:0.0026, Reg:2328.5830) beta=15.50
Iter  9000 | Total loss: 1780.0110 (MSE:0.0026, Reg:1780.0083) beta=14.38
Iter 10000 | Total loss: 1293.8661 (MSE:0.0025, Reg:1293.8635) beta=13.25
Iter 11000 | Total loss: 831.6564 (MSE:0.0027, Reg:831.6536) beta=12.12
Iter 12000 | Total loss: 475.2458 (MSE:0.0028, Reg:475.2431) beta=11.00
Iter 13000 | Total loss: 210.1256 (MSE:0.0027, Reg:210.1229) beta=9.88
Iter 14000 | Total loss: 65.8631 (MSE:0.0025, Reg:65.8607) beta=8.75
Iter 15000 | Total loss: 13.0027 (MSE:0.0027, Reg:13.0000) beta=7.62
Iter 16000 | Total loss: 1.0026 (MSE:0.0026, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 131848.5938 (MSE:0.0020, Reg:131848.5938) beta=20.00
Iter  5000 | Total loss: 4600.6680 (MSE:0.0020, Reg:4600.6660) beta=18.88
Iter  6000 | Total loss: 2713.3401 (MSE:0.0022, Reg:2713.3379) beta=17.75
Iter  7000 | Total loss: 1959.6163 (MSE:0.0021, Reg:1959.6143) beta=16.62
Iter  8000 | Total loss: 1472.0743 (MSE:0.0020, Reg:1472.0723) beta=15.50
Iter  9000 | Total loss: 1116.8090 (MSE:0.0020, Reg:1116.8069) beta=14.38
Iter 10000 | Total loss: 824.1808 (MSE:0.0021, Reg:824.1786) beta=13.25
Iter 11000 | Total loss: 547.5911 (MSE:0.0022, Reg:547.5890) beta=12.12
Iter 12000 | Total loss: 322.4649 (MSE:0.0021, Reg:322.4628) beta=11.00
Iter 13000 | Total loss: 150.6840 (MSE:0.0021, Reg:150.6819) beta=9.88
Iter 14000 | Total loss: 46.0019 (MSE:0.0021, Reg:45.9998) beta=8.75
Iter 15000 | Total loss: 8.0020 (MSE:0.0020, Reg:8.0000) beta=7.62
Iter 16000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 308096.0312 (MSE:0.0069, Reg:308096.0312) beta=20.00
Iter  5000 | Total loss: 22084.7383 (MSE:0.0075, Reg:22084.7305) beta=18.88
Iter  6000 | Total loss: 14916.9971 (MSE:0.0069, Reg:14916.9902) beta=17.75
Iter  7000 | Total loss: 11208.1562 (MSE:0.0074, Reg:11208.1484) beta=16.62
Iter  8000 | Total loss: 8419.3145 (MSE:0.0071, Reg:8419.3076) beta=15.50
Iter  9000 | Total loss: 6207.9780 (MSE:0.0070, Reg:6207.9712) beta=14.38
Iter 10000 | Total loss: 4285.0771 (MSE:0.0073, Reg:4285.0698) beta=13.25
Iter 11000 | Total loss: 2582.2383 (MSE:0.0073, Reg:2582.2310) beta=12.12
Iter 12000 | Total loss: 1288.3907 (MSE:0.0072, Reg:1288.3835) beta=11.00
Iter 13000 | Total loss: 458.3286 (MSE:0.0074, Reg:458.3213) beta=9.88
Iter 14000 | Total loss: 80.5011 (MSE:0.0068, Reg:80.4943) beta=8.75
Iter 15000 | Total loss: 1.8665 (MSE:0.0071, Reg:1.8594) beta=7.62
Iter 16000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 30222.5664 (MSE:0.0023, Reg:30222.5645) beta=20.00
Iter  5000 | Total loss: 2119.3127 (MSE:0.0026, Reg:2119.3101) beta=18.88
Iter  6000 | Total loss: 1702.8550 (MSE:0.0024, Reg:1702.8525) beta=17.75
Iter  7000 | Total loss: 1420.0662 (MSE:0.0025, Reg:1420.0637) beta=16.62
Iter  8000 | Total loss: 1131.2782 (MSE:0.0025, Reg:1131.2756) beta=15.50
Iter  9000 | Total loss: 920.0995 (MSE:0.0024, Reg:920.0972) beta=14.38
Iter 10000 | Total loss: 684.4276 (MSE:0.0026, Reg:684.4249) beta=13.25
Iter 11000 | Total loss: 431.6848 (MSE:0.0026, Reg:431.6823) beta=12.12
Iter 12000 | Total loss: 222.9282 (MSE:0.0024, Reg:222.9258) beta=11.00
Iter 13000 | Total loss: 98.6389 (MSE:0.0025, Reg:98.6364) beta=9.88
Iter 14000 | Total loss: 29.0025 (MSE:0.0025, Reg:29.0000) beta=8.75
Iter 15000 | Total loss: 2.0027 (MSE:0.0027, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 301933.8125 (MSE:0.0031, Reg:301933.8125) beta=20.00
Iter  5000 | Total loss: 10411.2939 (MSE:0.0031, Reg:10411.2910) beta=18.88
Iter  6000 | Total loss: 6283.8628 (MSE:0.0034, Reg:6283.8594) beta=17.75
Iter  7000 | Total loss: 4447.8940 (MSE:0.0033, Reg:4447.8906) beta=16.62
Iter  8000 | Total loss: 3330.8899 (MSE:0.0032, Reg:3330.8867) beta=15.50
Iter  9000 | Total loss: 2418.9023 (MSE:0.0033, Reg:2418.8989) beta=14.38
Iter 10000 | Total loss: 1692.9232 (MSE:0.0033, Reg:1692.9199) beta=13.25
Iter 11000 | Total loss: 1087.3931 (MSE:0.0033, Reg:1087.3898) beta=12.12
Iter 12000 | Total loss: 625.5706 (MSE:0.0032, Reg:625.5674) beta=11.00
Iter 13000 | Total loss: 284.2184 (MSE:0.0034, Reg:284.2150) beta=9.88
Iter 14000 | Total loss: 79.1870 (MSE:0.0031, Reg:79.1839) beta=8.75
Iter 15000 | Total loss: 12.0032 (MSE:0.0032, Reg:12.0000) beta=7.62
Iter 16000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2277 (MSE:0.2277, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2104 (MSE:0.2104, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2014 (MSE:0.2014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1907 (MSE:0.1907, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 246160.0000 (MSE:0.1932, Reg:246159.8125) beta=20.00
Iter  5000 | Total loss: 46287.2344 (MSE:0.1893, Reg:46287.0469) beta=18.88
Iter  6000 | Total loss: 32384.5488 (MSE:0.1862, Reg:32384.3633) beta=17.75
Iter  7000 | Total loss: 22803.7227 (MSE:0.1978, Reg:22803.5254) beta=16.62
Iter  8000 | Total loss: 15543.0371 (MSE:0.1885, Reg:15542.8486) beta=15.50
Iter  9000 | Total loss: 9905.4326 (MSE:0.1925, Reg:9905.2402) beta=14.38
Iter 10000 | Total loss: 5274.6699 (MSE:0.1870, Reg:5274.4829) beta=13.25
Iter 11000 | Total loss: 2314.9900 (MSE:0.1923, Reg:2314.7979) beta=12.12
Iter 12000 | Total loss: 652.7249 (MSE:0.1943, Reg:652.5305) beta=11.00
Iter 13000 | Total loss: 99.7926 (MSE:0.1815, Reg:99.6111) beta=9.88
Iter 14000 | Total loss: 5.2012 (MSE:0.2012, Reg:5.0000) beta=8.75
Iter 15000 | Total loss: 0.1908 (MSE:0.1908, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1912 (MSE:0.1912, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1949 (MSE:0.1949, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1981 (MSE:0.1981, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1968 (MSE:0.1968, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1904 (MSE:0.1904, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1623 (MSE:0.1623, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1157 (MSE:0.1157, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1121 (MSE:0.1121, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1205 (MSE:0.1205, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 37889.6484 (MSE:0.1278, Reg:37889.5195) beta=20.00
Iter  5000 | Total loss: 7429.3579 (MSE:0.1286, Reg:7429.2295) beta=18.88
Iter  6000 | Total loss: 5606.6938 (MSE:0.1244, Reg:5606.5693) beta=17.75
Iter  7000 | Total loss: 4176.3633 (MSE:0.1273, Reg:4176.2358) beta=16.62
Iter  8000 | Total loss: 3047.2764 (MSE:0.1236, Reg:3047.1528) beta=15.50
Iter  9000 | Total loss: 2050.8232 (MSE:0.1283, Reg:2050.6948) beta=14.38
Iter 10000 | Total loss: 1222.4894 (MSE:0.1159, Reg:1222.3734) beta=13.25
Iter 11000 | Total loss: 573.8087 (MSE:0.1247, Reg:573.6840) beta=12.12
Iter 12000 | Total loss: 179.0990 (MSE:0.1171, Reg:178.9819) beta=11.00
Iter 13000 | Total loss: 45.1334 (MSE:0.1334, Reg:45.0000) beta=9.88
Iter 14000 | Total loss: 5.9815 (MSE:0.1127, Reg:5.8687) beta=8.75
Iter 15000 | Total loss: 1.1172 (MSE:0.1172, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.1185 (MSE:0.1185, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1196 (MSE:0.1196, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1152 (MSE:0.1152, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1104 (MSE:0.1104, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1224 (MSE:0.1224, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 69.154%
Total time: 869.26 sec

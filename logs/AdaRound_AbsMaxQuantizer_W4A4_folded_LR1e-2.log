
Case: [ resnet18_AdaRound_AbsMaxQuantizer_CH_W4A4_BNFold_AdaRoundLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: AbsMaxQuantizer
    - dstDtype: INT4
    - per_channel: False

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    ReLU merged
    Conv2d <- BatchNorm2d    BN Folded!
    Parent class is AbsMaxQuantizer
    Initiated the V
    Parent class is AbsMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0745 (MSE:0.0745, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 601.1487 (MSE:0.0039, Reg:601.1448) beta=20.00
Iter  5000 | Total loss: 29.0033 (MSE:0.0033, Reg:29.0000) beta=18.88
Iter  6000 | Total loss: 21.0039 (MSE:0.0039, Reg:21.0000) beta=17.75
Iter  7000 | Total loss: 12.0038 (MSE:0.0038, Reg:12.0000) beta=16.62
Iter  8000 | Total loss: 12.0035 (MSE:0.0035, Reg:12.0000) beta=15.50
Iter  9000 | Total loss: 7.0036 (MSE:0.0036, Reg:7.0000) beta=14.38
Iter 10000 | Total loss: 6.0035 (MSE:0.0035, Reg:6.0000) beta=13.25
Iter 11000 | Total loss: 6.0036 (MSE:0.0036, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 5.0039 (MSE:0.0039, Reg:5.0000) beta=11.00
Iter 13000 | Total loss: 4.0034 (MSE:0.0034, Reg:4.0000) beta=9.88
Iter 14000 | Total loss: 2.0036 (MSE:0.0036, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 1.9742 (MSE:0.0037, Reg:1.9705) beta=7.62
Iter 16000 | Total loss: 1.0036 (MSE:0.0036, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0151 (MSE:0.0151, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1502.2550 (MSE:0.0026, Reg:1502.2524) beta=20.00
Iter  5000 | Total loss: 123.0025 (MSE:0.0028, Reg:122.9997) beta=18.88
Iter  6000 | Total loss: 75.9920 (MSE:0.0031, Reg:75.9889) beta=17.75
Iter  7000 | Total loss: 46.0026 (MSE:0.0026, Reg:46.0000) beta=16.62
Iter  8000 | Total loss: 31.0026 (MSE:0.0026, Reg:31.0000) beta=15.50
Iter  9000 | Total loss: 18.0026 (MSE:0.0026, Reg:18.0000) beta=14.38
Iter 10000 | Total loss: 12.0026 (MSE:0.0026, Reg:12.0000) beta=13.25
Iter 11000 | Total loss: 3.0033 (MSE:0.0033, Reg:3.0000) beta=12.12
Iter 12000 | Total loss: 3.0030 (MSE:0.0030, Reg:3.0000) beta=11.00
Iter 13000 | Total loss: 2.0028 (MSE:0.0028, Reg:2.0000) beta=9.88
Iter 14000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0351 (MSE:0.0351, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0157 (MSE:0.0157, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0167 (MSE:0.0167, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2386.3901 (MSE:0.0154, Reg:2386.3748) beta=20.00
Iter  5000 | Total loss: 391.1606 (MSE:0.0168, Reg:391.1438) beta=18.88
Iter  6000 | Total loss: 267.0142 (MSE:0.0144, Reg:266.9998) beta=17.75
Iter  7000 | Total loss: 203.3697 (MSE:0.0165, Reg:203.3532) beta=16.62
Iter  8000 | Total loss: 152.9958 (MSE:0.0147, Reg:152.9812) beta=15.50
Iter  9000 | Total loss: 95.0170 (MSE:0.0170, Reg:95.0000) beta=14.38
Iter 10000 | Total loss: 53.0157 (MSE:0.0157, Reg:53.0000) beta=13.25
Iter 11000 | Total loss: 28.0151 (MSE:0.0151, Reg:28.0000) beta=12.12
Iter 12000 | Total loss: 9.8576 (MSE:0.0154, Reg:9.8422) beta=11.00
Iter 13000 | Total loss: 2.0162 (MSE:0.0162, Reg:2.0000) beta=9.88
Iter 14000 | Total loss: 0.0159 (MSE:0.0159, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0156 (MSE:0.0156, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0159 (MSE:0.0159, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0146 (MSE:0.0146, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0177 (MSE:0.0177, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0152 (MSE:0.0152, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0168 (MSE:0.0168, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0134 (MSE:0.0134, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1972.9954 (MSE:0.0049, Reg:1972.9905) beta=20.00
Iter  5000 | Total loss: 265.9696 (MSE:0.0056, Reg:265.9640) beta=18.88
Iter  6000 | Total loss: 174.9972 (MSE:0.0055, Reg:174.9917) beta=17.75
Iter  7000 | Total loss: 111.6634 (MSE:0.0053, Reg:111.6581) beta=16.62
Iter  8000 | Total loss: 67.7507 (MSE:0.0055, Reg:67.7452) beta=15.50
Iter  9000 | Total loss: 56.0052 (MSE:0.0052, Reg:56.0000) beta=14.38
Iter 10000 | Total loss: 28.9934 (MSE:0.0055, Reg:28.9879) beta=13.25
Iter 11000 | Total loss: 10.7239 (MSE:0.0052, Reg:10.7187) beta=12.12
Iter 12000 | Total loss: 7.0053 (MSE:0.0053, Reg:7.0000) beta=11.00
Iter 13000 | Total loss: 7.0054 (MSE:0.0054, Reg:7.0000) beta=9.88
Iter 14000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0837 (MSE:0.0837, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0534 (MSE:0.0534, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0457 (MSE:0.0457, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0437 (MSE:0.0437, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3771.4819 (MSE:0.0481, Reg:3771.4338) beta=20.00
Iter  5000 | Total loss: 785.0469 (MSE:0.0494, Reg:784.9974) beta=18.88
Iter  6000 | Total loss: 622.3311 (MSE:0.0500, Reg:622.2810) beta=17.75
Iter  7000 | Total loss: 514.8319 (MSE:0.0463, Reg:514.7856) beta=16.62
Iter  8000 | Total loss: 408.0479 (MSE:0.0487, Reg:407.9992) beta=15.50
Iter  9000 | Total loss: 295.6195 (MSE:0.0468, Reg:295.5727) beta=14.38
Iter 10000 | Total loss: 189.1250 (MSE:0.0497, Reg:189.0753) beta=13.25
Iter 11000 | Total loss: 117.9230 (MSE:0.0487, Reg:117.8743) beta=12.12
Iter 12000 | Total loss: 49.0473 (MSE:0.0473, Reg:49.0000) beta=11.00
Iter 13000 | Total loss: 17.0484 (MSE:0.0484, Reg:17.0000) beta=9.88
Iter 14000 | Total loss: 3.0539 (MSE:0.0539, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0495 (MSE:0.0495, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0486 (MSE:0.0486, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0518 (MSE:0.0518, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0481 (MSE:0.0481, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0506 (MSE:0.0506, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0124 (MSE:0.0124, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4929.1470 (MSE:0.0079, Reg:4929.1392) beta=20.00
Iter  5000 | Total loss: 733.9946 (MSE:0.0080, Reg:733.9866) beta=18.88
Iter  6000 | Total loss: 494.1891 (MSE:0.0082, Reg:494.1809) beta=17.75
Iter  7000 | Total loss: 374.8171 (MSE:0.0078, Reg:374.8093) beta=16.62
Iter  8000 | Total loss: 264.5174 (MSE:0.0077, Reg:264.5097) beta=15.50
Iter  9000 | Total loss: 177.9836 (MSE:0.0082, Reg:177.9754) beta=14.38
Iter 10000 | Total loss: 107.9664 (MSE:0.0082, Reg:107.9582) beta=13.25
Iter 11000 | Total loss: 64.0076 (MSE:0.0076, Reg:64.0000) beta=12.12
Iter 12000 | Total loss: 31.4938 (MSE:0.0082, Reg:31.4856) beta=11.00
Iter 13000 | Total loss: 17.0083 (MSE:0.0083, Reg:17.0000) beta=9.88
Iter 14000 | Total loss: 2.0076 (MSE:0.0076, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0080 (MSE:0.0080, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0455 (MSE:0.0455, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0266 (MSE:0.0266, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0272 (MSE:0.0272, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0272 (MSE:0.0272, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15941.1504 (MSE:0.0282, Reg:15941.1221) beta=20.00
Iter  5000 | Total loss: 2064.5603 (MSE:0.0270, Reg:2064.5332) beta=18.88
Iter  6000 | Total loss: 1604.7010 (MSE:0.0276, Reg:1604.6735) beta=17.75
Iter  7000 | Total loss: 1315.1919 (MSE:0.0280, Reg:1315.1638) beta=16.62
Iter  8000 | Total loss: 1071.0280 (MSE:0.0280, Reg:1071.0000) beta=15.50
Iter  9000 | Total loss: 805.4971 (MSE:0.0274, Reg:805.4697) beta=14.38
Iter 10000 | Total loss: 502.8803 (MSE:0.0295, Reg:502.8508) beta=13.25
Iter 11000 | Total loss: 291.1412 (MSE:0.0270, Reg:291.1142) beta=12.12
Iter 12000 | Total loss: 89.5394 (MSE:0.0275, Reg:89.5119) beta=11.00
Iter 13000 | Total loss: 26.7315 (MSE:0.0295, Reg:26.7020) beta=9.88
Iter 14000 | Total loss: 6.0265 (MSE:0.0266, Reg:5.9999) beta=8.75
Iter 15000 | Total loss: 0.0268 (MSE:0.0268, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0278 (MSE:0.0278, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0270 (MSE:0.0270, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0282 (MSE:0.0282, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0281 (MSE:0.0281, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0265 (MSE:0.0265, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0273 (MSE:0.0273, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0116 (MSE:0.0116, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0116 (MSE:0.0116, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0121 (MSE:0.0121, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1425.8552 (MSE:0.0122, Reg:1425.8430) beta=20.00
Iter  5000 | Total loss: 237.0111 (MSE:0.0113, Reg:236.9998) beta=18.88
Iter  6000 | Total loss: 201.0132 (MSE:0.0132, Reg:201.0000) beta=17.75
Iter  7000 | Total loss: 166.0121 (MSE:0.0121, Reg:166.0000) beta=16.62
Iter  8000 | Total loss: 137.0125 (MSE:0.0124, Reg:137.0000) beta=15.50
Iter  9000 | Total loss: 108.9280 (MSE:0.0125, Reg:108.9155) beta=14.38
Iter 10000 | Total loss: 76.0140 (MSE:0.0140, Reg:76.0000) beta=13.25
Iter 11000 | Total loss: 45.0123 (MSE:0.0123, Reg:45.0000) beta=12.12
Iter 12000 | Total loss: 32.9536 (MSE:0.0124, Reg:32.9411) beta=11.00
Iter 13000 | Total loss: 12.0121 (MSE:0.0121, Reg:12.0000) beta=9.88
Iter 14000 | Total loss: 6.0129 (MSE:0.0129, Reg:6.0000) beta=8.75
Iter 15000 | Total loss: 1.0127 (MSE:0.0127, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0123 (MSE:0.0123, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0127 (MSE:0.0127, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0123 (MSE:0.0123, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0125 (MSE:0.0125, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0122 (MSE:0.0122, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 11640.7070 (MSE:0.0049, Reg:11640.7021) beta=20.00
Iter  5000 | Total loss: 1593.5101 (MSE:0.0045, Reg:1593.5056) beta=18.88
Iter  6000 | Total loss: 1109.5168 (MSE:0.0047, Reg:1109.5122) beta=17.75
Iter  7000 | Total loss: 849.2313 (MSE:0.0046, Reg:849.2267) beta=16.62
Iter  8000 | Total loss: 628.3735 (MSE:0.0046, Reg:628.3690) beta=15.50
Iter  9000 | Total loss: 447.9992 (MSE:0.0046, Reg:447.9946) beta=14.38
Iter 10000 | Total loss: 280.6400 (MSE:0.0049, Reg:280.6351) beta=13.25
Iter 11000 | Total loss: 143.0046 (MSE:0.0046, Reg:143.0000) beta=12.12
Iter 12000 | Total loss: 62.0049 (MSE:0.0049, Reg:62.0000) beta=11.00
Iter 13000 | Total loss: 17.0046 (MSE:0.0046, Reg:17.0000) beta=9.88
Iter 14000 | Total loss: 2.0047 (MSE:0.0047, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0242 (MSE:0.0242, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0257 (MSE:0.0257, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0247 (MSE:0.0247, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 18114.1934 (MSE:0.0251, Reg:18114.1680) beta=20.00
Iter  5000 | Total loss: 2662.9873 (MSE:0.0245, Reg:2662.9629) beta=18.88
Iter  6000 | Total loss: 2057.1865 (MSE:0.0266, Reg:2057.1599) beta=17.75
Iter  7000 | Total loss: 1633.9385 (MSE:0.0260, Reg:1633.9125) beta=16.62
Iter  8000 | Total loss: 1286.5201 (MSE:0.0246, Reg:1286.4956) beta=15.50
Iter  9000 | Total loss: 946.8643 (MSE:0.0243, Reg:946.8400) beta=14.38
Iter 10000 | Total loss: 604.2014 (MSE:0.0257, Reg:604.1758) beta=13.25
Iter 11000 | Total loss: 312.3065 (MSE:0.0257, Reg:312.2809) beta=12.12
Iter 12000 | Total loss: 127.1270 (MSE:0.0266, Reg:127.1004) beta=11.00
Iter 13000 | Total loss: 37.8615 (MSE:0.0259, Reg:37.8356) beta=9.88
Iter 14000 | Total loss: 3.0242 (MSE:0.0242, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0241 (MSE:0.0241, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0233 (MSE:0.0233, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0258 (MSE:0.0258, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0251 (MSE:0.0251, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0264 (MSE:0.0264, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0277 (MSE:0.0277, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0108 (MSE:0.0108, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 22076.7754 (MSE:0.0068, Reg:22076.7695) beta=20.00
Iter  5000 | Total loss: 2770.6943 (MSE:0.0067, Reg:2770.6875) beta=18.88
Iter  6000 | Total loss: 1972.6318 (MSE:0.0069, Reg:1972.6249) beta=17.75
Iter  7000 | Total loss: 1428.6918 (MSE:0.0067, Reg:1428.6851) beta=16.62
Iter  8000 | Total loss: 1064.0750 (MSE:0.0068, Reg:1064.0681) beta=15.50
Iter  9000 | Total loss: 727.5683 (MSE:0.0068, Reg:727.5615) beta=14.38
Iter 10000 | Total loss: 447.0064 (MSE:0.0066, Reg:446.9998) beta=13.25
Iter 11000 | Total loss: 240.3837 (MSE:0.0069, Reg:240.3769) beta=12.12
Iter 12000 | Total loss: 93.9924 (MSE:0.0069, Reg:93.9856) beta=11.00
Iter 13000 | Total loss: 32.0041 (MSE:0.0069, Reg:31.9973) beta=9.88
Iter 14000 | Total loss: 3.0068 (MSE:0.0068, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0439 (MSE:0.0439, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0216 (MSE:0.0216, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0212 (MSE:0.0212, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 55480.1250 (MSE:0.0222, Reg:55480.1016) beta=20.00
Iter  5000 | Total loss: 7476.6392 (MSE:0.0222, Reg:7476.6172) beta=18.88
Iter  6000 | Total loss: 5340.0161 (MSE:0.0224, Reg:5339.9937) beta=17.75
Iter  7000 | Total loss: 3990.5520 (MSE:0.0227, Reg:3990.5293) beta=16.62
Iter  8000 | Total loss: 2840.0078 (MSE:0.0228, Reg:2839.9849) beta=15.50
Iter  9000 | Total loss: 1862.2990 (MSE:0.0218, Reg:1862.2771) beta=14.38
Iter 10000 | Total loss: 1119.0709 (MSE:0.0210, Reg:1119.0499) beta=13.25
Iter 11000 | Total loss: 506.7068 (MSE:0.0212, Reg:506.6856) beta=12.12
Iter 12000 | Total loss: 172.4519 (MSE:0.0214, Reg:172.4305) beta=11.00
Iter 13000 | Total loss: 37.9438 (MSE:0.0220, Reg:37.9218) beta=9.88
Iter 14000 | Total loss: 1.9958 (MSE:0.0222, Reg:1.9735) beta=8.75
Iter 15000 | Total loss: 0.0211 (MSE:0.0211, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0219 (MSE:0.0219, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0216 (MSE:0.0216, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0227 (MSE:0.0227, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0067 (MSE:0.0067, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5682.8652 (MSE:0.0025, Reg:5682.8628) beta=20.00
Iter  5000 | Total loss: 717.6597 (MSE:0.0027, Reg:717.6570) beta=18.88
Iter  6000 | Total loss: 586.7797 (MSE:0.0026, Reg:586.7771) beta=17.75
Iter  7000 | Total loss: 470.2339 (MSE:0.0026, Reg:470.2313) beta=16.62
Iter  8000 | Total loss: 384.8896 (MSE:0.0027, Reg:384.8869) beta=15.50
Iter  9000 | Total loss: 284.4725 (MSE:0.0026, Reg:284.4698) beta=14.38
Iter 10000 | Total loss: 202.6151 (MSE:0.0026, Reg:202.6125) beta=13.25
Iter 11000 | Total loss: 123.7990 (MSE:0.0025, Reg:123.7966) beta=12.12
Iter 12000 | Total loss: 72.8689 (MSE:0.0026, Reg:72.8663) beta=11.00
Iter 13000 | Total loss: 37.0023 (MSE:0.0028, Reg:36.9995) beta=9.88
Iter 14000 | Total loss: 15.0026 (MSE:0.0026, Reg:15.0000) beta=8.75
Iter 15000 | Total loss: 3.0027 (MSE:0.0027, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 45439.9414 (MSE:0.0024, Reg:45439.9375) beta=20.00
Iter  5000 | Total loss: 2626.5715 (MSE:0.0023, Reg:2626.5693) beta=18.88
Iter  6000 | Total loss: 1561.0416 (MSE:0.0024, Reg:1561.0392) beta=17.75
Iter  7000 | Total loss: 1036.0770 (MSE:0.0024, Reg:1036.0746) beta=16.62
Iter  8000 | Total loss: 728.3449 (MSE:0.0022, Reg:728.3428) beta=15.50
Iter  9000 | Total loss: 507.3187 (MSE:0.0023, Reg:507.3164) beta=14.38
Iter 10000 | Total loss: 306.3023 (MSE:0.0024, Reg:306.2999) beta=13.25
Iter 11000 | Total loss: 205.0013 (MSE:0.0021, Reg:204.9993) beta=12.12
Iter 12000 | Total loss: 96.0024 (MSE:0.0025, Reg:95.9999) beta=11.00
Iter 13000 | Total loss: 36.3602 (MSE:0.0023, Reg:36.3578) beta=9.88
Iter 14000 | Total loss: 10.7373 (MSE:0.0022, Reg:10.7351) beta=8.75
Iter 15000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0373 (MSE:0.0373, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0196 (MSE:0.0196, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 59435.5000 (MSE:0.0193, Reg:59435.4805) beta=20.00
Iter  5000 | Total loss: 6683.2056 (MSE:0.0188, Reg:6683.1870) beta=18.88
Iter  6000 | Total loss: 4512.5771 (MSE:0.0188, Reg:4512.5586) beta=17.75
Iter  7000 | Total loss: 3200.9333 (MSE:0.0188, Reg:3200.9146) beta=16.62
Iter  8000 | Total loss: 2285.7568 (MSE:0.0199, Reg:2285.7368) beta=15.50
Iter  9000 | Total loss: 1549.4709 (MSE:0.0189, Reg:1549.4520) beta=14.38
Iter 10000 | Total loss: 948.2543 (MSE:0.0195, Reg:948.2349) beta=13.25
Iter 11000 | Total loss: 513.6290 (MSE:0.0181, Reg:513.6110) beta=12.12
Iter 12000 | Total loss: 214.6311 (MSE:0.0199, Reg:214.6112) beta=11.00
Iter 13000 | Total loss: 74.0193 (MSE:0.0196, Reg:73.9998) beta=9.88
Iter 14000 | Total loss: 15.0193 (MSE:0.0193, Reg:15.0000) beta=8.75
Iter 15000 | Total loss: 2.0197 (MSE:0.0197, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 1.0177 (MSE:0.0177, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0197 (MSE:0.0197, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0183 (MSE:0.0183, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0199 (MSE:0.0199, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0198 (MSE:0.0198, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 138614.8438 (MSE:0.0023, Reg:138614.8438) beta=20.00
Iter  5000 | Total loss: 3730.9392 (MSE:0.0026, Reg:3730.9365) beta=18.88
Iter  6000 | Total loss: 1649.5222 (MSE:0.0025, Reg:1649.5198) beta=17.75
Iter  7000 | Total loss: 942.4575 (MSE:0.0024, Reg:942.4551) beta=16.62
Iter  8000 | Total loss: 635.3206 (MSE:0.0024, Reg:635.3181) beta=15.50
Iter  9000 | Total loss: 425.6245 (MSE:0.0021, Reg:425.6224) beta=14.38
Iter 10000 | Total loss: 278.8873 (MSE:0.0023, Reg:278.8850) beta=13.25
Iter 11000 | Total loss: 169.0020 (MSE:0.0025, Reg:168.9996) beta=12.12
Iter 12000 | Total loss: 103.0024 (MSE:0.0024, Reg:103.0000) beta=11.00
Iter 13000 | Total loss: 54.7051 (MSE:0.0024, Reg:54.7027) beta=9.88
Iter 14000 | Total loss: 25.0012 (MSE:0.0024, Reg:24.9988) beta=8.75
Iter 15000 | Total loss: 4.0024 (MSE:0.0024, Reg:4.0000) beta=7.62
Iter 16000 | Total loss: 0.9938 (MSE:0.0024, Reg:0.9914) beta=6.50
Iter 17000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0610 (MSE:0.0610, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0416 (MSE:0.0416, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0455 (MSE:0.0455, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0437 (MSE:0.0437, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 195852.6562 (MSE:0.0429, Reg:195852.6094) beta=20.00
Iter  5000 | Total loss: 23699.8457 (MSE:0.0397, Reg:23699.8066) beta=18.88
Iter  6000 | Total loss: 16404.9629 (MSE:0.0417, Reg:16404.9219) beta=17.75
Iter  7000 | Total loss: 11329.2578 (MSE:0.0411, Reg:11329.2168) beta=16.62
Iter  8000 | Total loss: 7706.7085 (MSE:0.0457, Reg:7706.6626) beta=15.50
Iter  9000 | Total loss: 4873.9839 (MSE:0.0454, Reg:4873.9385) beta=14.38
Iter 10000 | Total loss: 2730.3181 (MSE:0.0466, Reg:2730.2715) beta=13.25
Iter 11000 | Total loss: 1258.8785 (MSE:0.0407, Reg:1258.8378) beta=12.12
Iter 12000 | Total loss: 413.4413 (MSE:0.0433, Reg:413.3980) beta=11.00
Iter 13000 | Total loss: 68.9663 (MSE:0.0425, Reg:68.9238) beta=9.88
Iter 14000 | Total loss: 4.0431 (MSE:0.0431, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 0.0416 (MSE:0.0416, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0396 (MSE:0.0396, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0420 (MSE:0.0420, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0413 (MSE:0.0413, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0455 (MSE:0.0455, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0402 (MSE:0.0402, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0261 (MSE:0.0261, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0169 (MSE:0.0169, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0175 (MSE:0.0175, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 22810.2754 (MSE:0.0170, Reg:22810.2578) beta=20.00
Iter  5000 | Total loss: 3729.0386 (MSE:0.0171, Reg:3729.0215) beta=18.88
Iter  6000 | Total loss: 2874.0789 (MSE:0.0178, Reg:2874.0610) beta=17.75
Iter  7000 | Total loss: 2285.1592 (MSE:0.0171, Reg:2285.1421) beta=16.62
Iter  8000 | Total loss: 1764.2272 (MSE:0.0186, Reg:1764.2085) beta=15.50
Iter  9000 | Total loss: 1259.1344 (MSE:0.0172, Reg:1259.1172) beta=14.38
Iter 10000 | Total loss: 777.7589 (MSE:0.0163, Reg:777.7426) beta=13.25
Iter 11000 | Total loss: 409.1984 (MSE:0.0174, Reg:409.1810) beta=12.12
Iter 12000 | Total loss: 152.2740 (MSE:0.0173, Reg:152.2567) beta=11.00
Iter 13000 | Total loss: 34.7813 (MSE:0.0159, Reg:34.7654) beta=9.88
Iter 14000 | Total loss: 5.0174 (MSE:0.0174, Reg:5.0000) beta=8.75
Iter 15000 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0167 (MSE:0.0167, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0187 (MSE:0.0187, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0179 (MSE:0.0179, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 388202.4062 (MSE:0.0028, Reg:388202.4062) beta=20.00
Iter  5000 | Total loss: 4416.8735 (MSE:0.0026, Reg:4416.8711) beta=18.88
Iter  6000 | Total loss: 1561.0962 (MSE:0.0030, Reg:1561.0931) beta=17.75
Iter  7000 | Total loss: 830.7803 (MSE:0.0026, Reg:830.7777) beta=16.62
Iter  8000 | Total loss: 526.9136 (MSE:0.0027, Reg:526.9109) beta=15.50
Iter  9000 | Total loss: 346.9551 (MSE:0.0026, Reg:346.9525) beta=14.38
Iter 10000 | Total loss: 248.7261 (MSE:0.0028, Reg:248.7233) beta=13.25
Iter 11000 | Total loss: 157.3192 (MSE:0.0025, Reg:157.3167) beta=12.12
Iter 12000 | Total loss: 96.6597 (MSE:0.0027, Reg:96.6570) beta=11.00
Iter 13000 | Total loss: 48.6394 (MSE:0.0027, Reg:48.6367) beta=9.88
Iter 14000 | Total loss: 16.0028 (MSE:0.0028, Reg:16.0000) beta=8.75
Iter 15000 | Total loss: 0.9531 (MSE:0.0027, Reg:0.9504) beta=7.62
Iter 16000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 2.1919 (MSE:2.1919, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 1.8805 (MSE:1.8805, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 1.5619 (MSE:1.5619, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 1.2572 (MSE:1.2572, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 183010.3281 (MSE:1.1922, Reg:183009.1406) beta=20.00
Iter  5000 | Total loss: 34586.2773 (MSE:1.1482, Reg:34585.1289) beta=18.88
Iter  6000 | Total loss: 22021.4395 (MSE:1.3113, Reg:22020.1289) beta=17.75
Iter  7000 | Total loss: 14123.9443 (MSE:1.1960, Reg:14122.7480) beta=16.62
Iter  8000 | Total loss: 9042.0781 (MSE:1.1330, Reg:9040.9453) beta=15.50
Iter  9000 | Total loss: 5420.3979 (MSE:1.2055, Reg:5419.1924) beta=14.38
Iter 10000 | Total loss: 2845.8887 (MSE:1.1537, Reg:2844.7349) beta=13.25
Iter 11000 | Total loss: 1227.5636 (MSE:1.1114, Reg:1226.4521) beta=12.12
Iter 12000 | Total loss: 341.0877 (MSE:1.1193, Reg:339.9684) beta=11.00
Iter 13000 | Total loss: 57.7428 (MSE:1.1195, Reg:56.6233) beta=9.88
Iter 14000 | Total loss: 5.0750 (MSE:1.0750, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 1.1344 (MSE:1.1344, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 1.2056 (MSE:1.2056, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 1.0955 (MSE:1.0955, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 1.0906 (MSE:1.0906, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 1.0365 (MSE:1.0365, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 1.0916 (MSE:1.0916, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 1.9122 (MSE:1.9122, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 1.2029 (MSE:1.2029, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 1.0315 (MSE:1.0315, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 1.0617 (MSE:1.0617, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 29571.5059 (MSE:1.0153, Reg:29570.4902) beta=20.00
Iter  5000 | Total loss: 6009.0059 (MSE:1.0331, Reg:6007.9727) beta=18.88
Iter  6000 | Total loss: 4133.3198 (MSE:0.9026, Reg:4132.4170) beta=17.75
Iter  7000 | Total loss: 2808.5469 (MSE:1.0347, Reg:2807.5122) beta=16.62
Iter  8000 | Total loss: 1820.5920 (MSE:1.0592, Reg:1819.5328) beta=15.50
Iter  9000 | Total loss: 1118.9147 (MSE:1.0682, Reg:1117.8464) beta=14.38
Iter 10000 | Total loss: 626.9177 (MSE:0.9705, Reg:625.9473) beta=13.25
Iter 11000 | Total loss: 305.9917 (MSE:0.9153, Reg:305.0764) beta=12.12
Iter 12000 | Total loss: 125.1004 (MSE:1.1004, Reg:124.0000) beta=11.00
Iter 13000 | Total loss: 40.9850 (MSE:1.0059, Reg:39.9791) beta=9.88
Iter 14000 | Total loss: 12.0137 (MSE:0.9902, Reg:11.0235) beta=8.75
Iter 15000 | Total loss: 2.0597 (MSE:1.0597, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.9578 (MSE:0.9578, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.9216 (MSE:0.9216, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.9666 (MSE:0.9666, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 1.0425 (MSE:1.0425, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 1.0576 (MSE:1.0576, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 22.928%
Total time: 1224.91 sec

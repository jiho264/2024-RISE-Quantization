
Case: [ resnet18_AdaRound_NormQuantizer_CH_W4A4_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - AdaRound: True

- activation params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: False
    - p: 2.4

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0086 (MSE:0.0086, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0078 (MSE:0.0078, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0074 (MSE:0.0074, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0094 (MSE:0.0094, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0082 (MSE:0.0082, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0099 (MSE:0.0099, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0266 (MSE:0.0266, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0257 (MSE:0.0257, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0240 (MSE:0.0240, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0254 (MSE:0.0254, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0267 (MSE:0.0267, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0285 (MSE:0.0285, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0261 (MSE:0.0261, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0245 (MSE:0.0245, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0251 (MSE:0.0251, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0254 (MSE:0.0254, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0337 (MSE:0.0337, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0288 (MSE:0.0288, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0256 (MSE:0.0256, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0264 (MSE:0.0264, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0259 (MSE:0.0259, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0261 (MSE:0.0261, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0261 (MSE:0.0261, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0249 (MSE:0.0249, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0281 (MSE:0.0281, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0254 (MSE:0.0254, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0467 (MSE:0.0467, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0525 (MSE:0.0525, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0535 (MSE:0.0535, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0536 (MSE:0.0536, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0520 (MSE:0.0520, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0543 (MSE:0.0543, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0495 (MSE:0.0495, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0541 (MSE:0.0541, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0501 (MSE:0.0501, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0560 (MSE:0.0560, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0514 (MSE:0.0514, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0509 (MSE:0.0509, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0528 (MSE:0.0528, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0519 (MSE:0.0519, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0534 (MSE:0.0534, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0527 (MSE:0.0527, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0525 (MSE:0.0525, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0502 (MSE:0.0502, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0576 (MSE:0.0576, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0515 (MSE:0.0515, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0535 (MSE:0.0535, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0329 (MSE:0.0329, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0300 (MSE:0.0300, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0318 (MSE:0.0318, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0326 (MSE:0.0326, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0314 (MSE:0.0314, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0357 (MSE:0.0357, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0359 (MSE:0.0359, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0324 (MSE:0.0324, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0350 (MSE:0.0350, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0312 (MSE:0.0312, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0339 (MSE:0.0339, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0333 (MSE:0.0333, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0315 (MSE:0.0315, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0326 (MSE:0.0326, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0326 (MSE:0.0326, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0338 (MSE:0.0338, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0327 (MSE:0.0327, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0318 (MSE:0.0318, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0340 (MSE:0.0340, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0293 (MSE:0.0293, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0345 (MSE:0.0345, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0921 (MSE:0.0921, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1144 (MSE:0.1144, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0897 (MSE:0.0897, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0900 (MSE:0.0900, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0975 (MSE:0.0975, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1046 (MSE:0.1046, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1031 (MSE:0.1031, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0893 (MSE:0.0893, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0930 (MSE:0.0930, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0958 (MSE:0.0958, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0998 (MSE:0.0998, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1009 (MSE:0.1009, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0961 (MSE:0.0961, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0973 (MSE:0.0973, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1107 (MSE:0.1107, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0930 (MSE:0.0930, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0928 (MSE:0.0928, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0998 (MSE:0.0998, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1031 (MSE:0.1031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1002 (MSE:0.1002, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1029 (MSE:0.1029, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0425 (MSE:0.0425, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0427 (MSE:0.0427, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0441 (MSE:0.0441, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0446 (MSE:0.0446, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0439 (MSE:0.0439, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0453 (MSE:0.0453, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0466 (MSE:0.0466, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0436 (MSE:0.0436, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0441 (MSE:0.0441, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0453 (MSE:0.0453, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0456 (MSE:0.0456, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0440 (MSE:0.0440, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0483 (MSE:0.0483, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0440 (MSE:0.0440, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0434 (MSE:0.0434, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0466 (MSE:0.0466, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0476 (MSE:0.0476, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0419 (MSE:0.0419, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0718 (MSE:0.0718, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0650 (MSE:0.0650, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0688 (MSE:0.0688, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0702 (MSE:0.0702, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0745 (MSE:0.0745, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0712 (MSE:0.0712, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0754 (MSE:0.0754, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0778 (MSE:0.0778, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0713 (MSE:0.0713, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0695 (MSE:0.0695, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0810 (MSE:0.0810, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0669 (MSE:0.0669, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0673 (MSE:0.0673, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0761 (MSE:0.0761, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0720 (MSE:0.0720, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0691 (MSE:0.0691, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0707 (MSE:0.0707, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0739 (MSE:0.0739, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0749 (MSE:0.0749, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0726 (MSE:0.0726, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0720 (MSE:0.0720, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0221 (MSE:0.0221, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0221 (MSE:0.0221, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0222 (MSE:0.0222, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0239 (MSE:0.0239, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0236 (MSE:0.0236, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0218 (MSE:0.0218, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0258 (MSE:0.0258, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0225 (MSE:0.0225, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0243 (MSE:0.0243, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0237 (MSE:0.0237, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0278 (MSE:0.0278, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0251 (MSE:0.0251, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0233 (MSE:0.0233, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0252 (MSE:0.0252, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0250 (MSE:0.0250, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0239 (MSE:0.0239, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0245 (MSE:0.0245, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0235 (MSE:0.0235, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0238 (MSE:0.0238, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0483 (MSE:0.0483, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0591 (MSE:0.0591, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0579 (MSE:0.0579, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0581 (MSE:0.0581, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0616 (MSE:0.0616, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0543 (MSE:0.0543, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0584 (MSE:0.0584, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0603 (MSE:0.0603, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0588 (MSE:0.0588, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0595 (MSE:0.0595, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0606 (MSE:0.0606, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0543 (MSE:0.0543, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0603 (MSE:0.0603, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0550 (MSE:0.0550, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0581 (MSE:0.0581, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0593 (MSE:0.0593, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0591 (MSE:0.0591, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0588 (MSE:0.0588, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0587 (MSE:0.0587, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0593 (MSE:0.0593, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0638 (MSE:0.0638, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0701 (MSE:0.0701, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0658 (MSE:0.0658, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0744 (MSE:0.0744, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0687 (MSE:0.0687, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0727 (MSE:0.0727, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0707 (MSE:0.0707, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0761 (MSE:0.0761, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0712 (MSE:0.0712, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0697 (MSE:0.0697, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0705 (MSE:0.0705, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0736 (MSE:0.0736, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0738 (MSE:0.0738, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0755 (MSE:0.0755, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0759 (MSE:0.0759, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0691 (MSE:0.0691, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0666 (MSE:0.0666, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0668 (MSE:0.0668, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0682 (MSE:0.0682, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0713 (MSE:0.0713, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0708 (MSE:0.0708, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0818 (MSE:0.0818, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0665 (MSE:0.0665, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0657 (MSE:0.0657, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0627 (MSE:0.0627, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0616 (MSE:0.0616, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0631 (MSE:0.0631, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0652 (MSE:0.0652, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0659 (MSE:0.0659, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0650 (MSE:0.0650, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0653 (MSE:0.0653, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0636 (MSE:0.0636, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0629 (MSE:0.0629, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0644 (MSE:0.0644, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0642 (MSE:0.0642, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0639 (MSE:0.0639, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0624 (MSE:0.0624, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0622 (MSE:0.0622, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0654 (MSE:0.0654, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0599 (MSE:0.0599, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0675 (MSE:0.0675, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0638 (MSE:0.0638, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0593 (MSE:0.0593, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0826 (MSE:0.0826, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0793 (MSE:0.0793, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0804 (MSE:0.0804, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0847 (MSE:0.0847, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0837 (MSE:0.0837, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0778 (MSE:0.0778, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0787 (MSE:0.0787, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0822 (MSE:0.0822, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0808 (MSE:0.0808, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0789 (MSE:0.0789, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0821 (MSE:0.0821, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0818 (MSE:0.0818, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0831 (MSE:0.0831, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0783 (MSE:0.0783, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0790 (MSE:0.0790, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0827 (MSE:0.0827, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0857 (MSE:0.0857, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0784 (MSE:0.0784, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0823 (MSE:0.0823, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0885 (MSE:0.0885, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0886 (MSE:0.0886, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0627 (MSE:0.0627, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0751 (MSE:0.0751, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0850 (MSE:0.0850, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0833 (MSE:0.0833, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0822 (MSE:0.0822, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0895 (MSE:0.0895, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0851 (MSE:0.0851, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0890 (MSE:0.0890, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0794 (MSE:0.0794, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0805 (MSE:0.0805, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0934 (MSE:0.0934, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0783 (MSE:0.0783, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0901 (MSE:0.0901, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0834 (MSE:0.0834, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0803 (MSE:0.0803, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0841 (MSE:0.0841, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0822 (MSE:0.0822, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0879 (MSE:0.0879, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0841 (MSE:0.0841, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0852 (MSE:0.0852, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0874 (MSE:0.0874, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1319 (MSE:0.1319, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1179 (MSE:0.1179, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1202 (MSE:0.1202, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1281 (MSE:0.1281, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1240 (MSE:0.1240, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1145 (MSE:0.1145, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1097 (MSE:0.1097, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1129 (MSE:0.1129, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.1196 (MSE:0.1196, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.1125 (MSE:0.1125, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1160 (MSE:0.1160, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1103 (MSE:0.1103, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1213 (MSE:0.1213, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1200 (MSE:0.1200, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1141 (MSE:0.1141, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1268 (MSE:0.1268, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1072 (MSE:0.1072, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1151 (MSE:0.1151, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1098 (MSE:0.1098, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1214 (MSE:0.1214, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1225 (MSE:0.1225, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0625 (MSE:0.0625, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0629 (MSE:0.0629, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0636 (MSE:0.0636, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0604 (MSE:0.0604, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0624 (MSE:0.0624, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0661 (MSE:0.0661, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0688 (MSE:0.0688, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0584 (MSE:0.0584, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0630 (MSE:0.0630, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0586 (MSE:0.0586, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0591 (MSE:0.0591, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0661 (MSE:0.0661, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0617 (MSE:0.0617, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0599 (MSE:0.0599, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0627 (MSE:0.0627, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0634 (MSE:0.0634, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0654 (MSE:0.0654, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0655 (MSE:0.0655, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0619 (MSE:0.0619, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0620 (MSE:0.0620, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0699 (MSE:0.0699, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2371 (MSE:0.2371, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2164 (MSE:0.2164, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2510 (MSE:0.2510, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2242 (MSE:0.2242, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.2131 (MSE:0.2131, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.2048 (MSE:0.2048, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.2282 (MSE:0.2282, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.2035 (MSE:0.2035, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.2542 (MSE:0.2542, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.2530 (MSE:0.2530, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.2355 (MSE:0.2355, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.2000 (MSE:0.2000, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.2320 (MSE:0.2320, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.2267 (MSE:0.2267, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.2346 (MSE:0.2346, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.2292 (MSE:0.2292, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2007 (MSE:0.2007, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2227 (MSE:0.2227, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2201 (MSE:0.2201, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2532 (MSE:0.2532, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2251 (MSE:0.2251, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0589 (MSE:0.0589, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0571 (MSE:0.0571, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0612 (MSE:0.0612, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0600 (MSE:0.0600, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0589 (MSE:0.0589, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0582 (MSE:0.0582, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0593 (MSE:0.0593, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0557 (MSE:0.0557, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0666 (MSE:0.0666, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0571 (MSE:0.0571, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0555 (MSE:0.0555, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0589 (MSE:0.0589, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0584 (MSE:0.0584, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0545 (MSE:0.0545, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0577 (MSE:0.0577, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0533 (MSE:0.0533, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0555 (MSE:0.0555, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0652 (MSE:0.0652, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0649 (MSE:0.0649, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0595 (MSE:0.0595, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0551 (MSE:0.0551, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2263 (MSE:0.2263, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1551 (MSE:0.1551, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1734 (MSE:0.1734, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1384 (MSE:0.1384, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1576 (MSE:0.1576, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1609 (MSE:0.1609, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1664 (MSE:0.1664, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1571 (MSE:0.1571, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.1446 (MSE:0.1446, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.1513 (MSE:0.1513, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1501 (MSE:0.1501, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1695 (MSE:0.1695, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1622 (MSE:0.1622, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1638 (MSE:0.1638, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1891 (MSE:0.1891, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1682 (MSE:0.1682, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1568 (MSE:0.1568, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1479 (MSE:0.1479, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1495 (MSE:0.1495, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1587 (MSE:0.1587, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1535 (MSE:0.1535, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 4.0002 (MSE:4.0002, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 3.9656 (MSE:3.9656, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 3.6468 (MSE:3.6468, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 3.6146 (MSE:3.6146, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 4.0498 (MSE:4.0498, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 4.0215 (MSE:4.0215, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 3.7613 (MSE:3.7613, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 4.0153 (MSE:4.0153, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 3.9730 (MSE:3.9730, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 3.8999 (MSE:3.8999, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 3.6984 (MSE:3.6984, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 3.7290 (MSE:3.7290, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 3.7728 (MSE:3.7728, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 3.7703 (MSE:3.7703, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 3.4551 (MSE:3.4551, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 3.7075 (MSE:3.7075, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 3.9744 (MSE:3.9744, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 3.5898 (MSE:3.5898, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 3.7293 (MSE:3.7293, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 3.6224 (MSE:3.6224, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 3.5308 (MSE:3.5308, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 8.0343 (MSE:8.0343, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 6.9570 (MSE:6.9570, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 7.2159 (MSE:7.2159, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 7.3550 (MSE:7.3550, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7.5731 (MSE:7.5731, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 7.9018 (MSE:7.9018, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 6.6204 (MSE:6.6204, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 7.5911 (MSE:7.5911, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 7.3234 (MSE:7.3234, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 6.8527 (MSE:6.8527, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 7.0359 (MSE:7.0359, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 6.9165 (MSE:6.9165, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 6.8453 (MSE:6.8453, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 7.7532 (MSE:7.7532, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 7.6061 (MSE:7.6061, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 7.5141 (MSE:7.5141, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 6.6084 (MSE:6.6084, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 7.1703 (MSE:7.1703, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 7.1067 (MSE:7.1067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 6.9664 (MSE:6.9664, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 8.0428 (MSE:8.0428, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 0.100%
Total time: 1433.33 sec


Case: [ resnet18_AdaRound_NormQuantizer_CH_W4A32_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - AdaRound: True

- activation params:

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 1894.5814 (MSE:0.0003, Reg:1894.5811) beta=20.00
Iter  5000 | Total loss: 18.0050 (MSE:0.0050, Reg:18.0000) beta=18.88
Iter  6000 | Total loss: 10.0036 (MSE:0.0036, Reg:10.0000) beta=17.75
Iter  7000 | Total loss: 7.0035 (MSE:0.0035, Reg:7.0000) beta=16.62
Iter  8000 | Total loss: 6.9583 (MSE:0.0037, Reg:6.9546) beta=15.50
Iter  9000 | Total loss: 2.8613 (MSE:0.0037, Reg:2.8575) beta=14.38
Iter 10000 | Total loss: 2.0039 (MSE:0.0039, Reg:2.0000) beta=13.25
Iter 11000 | Total loss: 0.6678 (MSE:0.0031, Reg:0.6647) beta=12.12
Iter 12000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0077 (MSE:0.0077, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 6276.8584 (MSE:0.0009, Reg:6276.8574) beta=20.00
Iter  5000 | Total loss: 339.3390 (MSE:0.0025, Reg:339.3365) beta=18.88
Iter  6000 | Total loss: 152.4103 (MSE:0.0027, Reg:152.4075) beta=17.75
Iter  7000 | Total loss: 101.6786 (MSE:0.0028, Reg:101.6758) beta=16.62
Iter  8000 | Total loss: 69.7267 (MSE:0.0028, Reg:69.7239) beta=15.50
Iter  9000 | Total loss: 39.4781 (MSE:0.0027, Reg:39.4754) beta=14.38
Iter 10000 | Total loss: 26.5267 (MSE:0.0026, Reg:26.5241) beta=13.25
Iter 11000 | Total loss: 16.9271 (MSE:0.0027, Reg:16.9244) beta=12.12
Iter 12000 | Total loss: 10.8906 (MSE:0.0027, Reg:10.8880) beta=11.00
Iter 13000 | Total loss: 4.8143 (MSE:0.0027, Reg:4.8116) beta=9.88
Iter 14000 | Total loss: 3.0025 (MSE:0.0025, Reg:3.0000) beta=8.75
Iter 15000 | Total loss: 0.9606 (MSE:0.0028, Reg:0.9579) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8160.6206 (MSE:0.0025, Reg:8160.6182) beta=20.00
Iter  5000 | Total loss: 1056.3301 (MSE:0.0027, Reg:1056.3274) beta=18.88
Iter  6000 | Total loss: 650.4542 (MSE:0.0027, Reg:650.4515) beta=17.75
Iter  7000 | Total loss: 477.8495 (MSE:0.0028, Reg:477.8467) beta=16.62
Iter  8000 | Total loss: 353.9933 (MSE:0.0029, Reg:353.9904) beta=15.50
Iter  9000 | Total loss: 255.5056 (MSE:0.0028, Reg:255.5029) beta=14.38
Iter 10000 | Total loss: 191.3857 (MSE:0.0028, Reg:191.3829) beta=13.25
Iter 11000 | Total loss: 128.5254 (MSE:0.0025, Reg:128.5228) beta=12.12
Iter 12000 | Total loss: 91.5081 (MSE:0.0028, Reg:91.5053) beta=11.00
Iter 13000 | Total loss: 50.9015 (MSE:0.0025, Reg:50.8990) beta=9.88
Iter 14000 | Total loss: 26.6189 (MSE:0.0027, Reg:26.6162) beta=8.75
Iter 15000 | Total loss: 7.8898 (MSE:0.0028, Reg:7.8870) beta=7.62
Iter 16000 | Total loss: 3.3690 (MSE:0.0028, Reg:3.3662) beta=6.50
Iter 17000 | Total loss: 1.0699 (MSE:0.0032, Reg:1.0667) beta=5.38
Iter 18000 | Total loss: 0.1766 (MSE:0.0026, Reg:0.1740) beta=4.25
Iter 19000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5500.3179 (MSE:0.0025, Reg:5500.3154) beta=20.00
Iter  5000 | Total loss: 674.4276 (MSE:0.0030, Reg:674.4246) beta=18.88
Iter  6000 | Total loss: 451.5956 (MSE:0.0034, Reg:451.5922) beta=17.75
Iter  7000 | Total loss: 354.9620 (MSE:0.0028, Reg:354.9593) beta=16.62
Iter  8000 | Total loss: 251.1021 (MSE:0.0029, Reg:251.0992) beta=15.50
Iter  9000 | Total loss: 187.4940 (MSE:0.0025, Reg:187.4915) beta=14.38
Iter 10000 | Total loss: 123.2702 (MSE:0.0031, Reg:123.2671) beta=13.25
Iter 11000 | Total loss: 82.8816 (MSE:0.0030, Reg:82.8785) beta=12.12
Iter 12000 | Total loss: 57.7975 (MSE:0.0028, Reg:57.7946) beta=11.00
Iter 13000 | Total loss: 31.4219 (MSE:0.0028, Reg:31.4190) beta=9.88
Iter 14000 | Total loss: 9.5704 (MSE:0.0028, Reg:9.5676) beta=8.75
Iter 15000 | Total loss: 5.4129 (MSE:0.0031, Reg:5.4099) beta=7.62
Iter 16000 | Total loss: 2.0028 (MSE:0.0028, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0126 (MSE:0.0126, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0090 (MSE:0.0090, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0079 (MSE:0.0079, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 8110.2935 (MSE:0.0072, Reg:8110.2861) beta=20.00
Iter  5000 | Total loss: 1361.6505 (MSE:0.0075, Reg:1361.6429) beta=18.88
Iter  6000 | Total loss: 945.8030 (MSE:0.0072, Reg:945.7958) beta=17.75
Iter  7000 | Total loss: 720.2735 (MSE:0.0080, Reg:720.2655) beta=16.62
Iter  8000 | Total loss: 557.5144 (MSE:0.0073, Reg:557.5071) beta=15.50
Iter  9000 | Total loss: 421.1283 (MSE:0.0071, Reg:421.1212) beta=14.38
Iter 10000 | Total loss: 332.9836 (MSE:0.0076, Reg:332.9760) beta=13.25
Iter 11000 | Total loss: 261.4128 (MSE:0.0072, Reg:261.4055) beta=12.12
Iter 12000 | Total loss: 191.2176 (MSE:0.0074, Reg:191.2102) beta=11.00
Iter 13000 | Total loss: 132.8266 (MSE:0.0078, Reg:132.8188) beta=9.88
Iter 14000 | Total loss: 68.0739 (MSE:0.0079, Reg:68.0660) beta=8.75
Iter 15000 | Total loss: 25.4171 (MSE:0.0078, Reg:25.4092) beta=7.62
Iter 16000 | Total loss: 7.0885 (MSE:0.0081, Reg:7.0804) beta=6.50
Iter 17000 | Total loss: 0.9996 (MSE:0.0074, Reg:0.9922) beta=5.38
Iter 18000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 11981.9365 (MSE:0.0037, Reg:11981.9326) beta=20.00
Iter  5000 | Total loss: 1000.3046 (MSE:0.0039, Reg:1000.3007) beta=18.88
Iter  6000 | Total loss: 554.9816 (MSE:0.0040, Reg:554.9775) beta=17.75
Iter  7000 | Total loss: 377.4335 (MSE:0.0038, Reg:377.4297) beta=16.62
Iter  8000 | Total loss: 266.7681 (MSE:0.0041, Reg:266.7641) beta=15.50
Iter  9000 | Total loss: 195.5390 (MSE:0.0038, Reg:195.5352) beta=14.38
Iter 10000 | Total loss: 142.3313 (MSE:0.0041, Reg:142.3272) beta=13.25
Iter 11000 | Total loss: 100.3131 (MSE:0.0042, Reg:100.3089) beta=12.12
Iter 12000 | Total loss: 72.5597 (MSE:0.0044, Reg:72.5554) beta=11.00
Iter 13000 | Total loss: 45.6759 (MSE:0.0040, Reg:45.6719) beta=9.88
Iter 14000 | Total loss: 24.1625 (MSE:0.0041, Reg:24.1585) beta=8.75
Iter 15000 | Total loss: 7.4337 (MSE:0.0041, Reg:7.4296) beta=7.62
Iter 16000 | Total loss: 3.0037 (MSE:0.0040, Reg:2.9997) beta=6.50
Iter 17000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 25191.7656 (MSE:0.0054, Reg:25191.7598) beta=20.00
Iter  5000 | Total loss: 2013.7454 (MSE:0.0057, Reg:2013.7396) beta=18.88
Iter  6000 | Total loss: 1174.7120 (MSE:0.0054, Reg:1174.7067) beta=17.75
Iter  7000 | Total loss: 830.9185 (MSE:0.0052, Reg:830.9133) beta=16.62
Iter  8000 | Total loss: 636.0893 (MSE:0.0055, Reg:636.0838) beta=15.50
Iter  9000 | Total loss: 487.4411 (MSE:0.0053, Reg:487.4358) beta=14.38
Iter 10000 | Total loss: 395.2275 (MSE:0.0057, Reg:395.2218) beta=13.25
Iter 11000 | Total loss: 307.8317 (MSE:0.0052, Reg:307.8265) beta=12.12
Iter 12000 | Total loss: 225.7309 (MSE:0.0053, Reg:225.7256) beta=11.00
Iter 13000 | Total loss: 134.6907 (MSE:0.0052, Reg:134.6854) beta=9.88
Iter 14000 | Total loss: 66.7401 (MSE:0.0054, Reg:66.7346) beta=8.75
Iter 15000 | Total loss: 18.8250 (MSE:0.0055, Reg:18.8195) beta=7.62
Iter 16000 | Total loss: 1.3339 (MSE:0.0053, Reg:1.3286) beta=6.50
Iter 17000 | Total loss: 0.1511 (MSE:0.0052, Reg:0.1459) beta=5.38
Iter 18000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2393.2542 (MSE:0.0023, Reg:2393.2520) beta=20.00
Iter  5000 | Total loss: 215.0069 (MSE:0.0024, Reg:215.0045) beta=18.88
Iter  6000 | Total loss: 140.9439 (MSE:0.0023, Reg:140.9415) beta=17.75
Iter  7000 | Total loss: 116.0464 (MSE:0.0024, Reg:116.0440) beta=16.62
Iter  8000 | Total loss: 94.9389 (MSE:0.0025, Reg:94.9364) beta=15.50
Iter  9000 | Total loss: 76.5902 (MSE:0.0024, Reg:76.5878) beta=14.38
Iter 10000 | Total loss: 60.9196 (MSE:0.0025, Reg:60.9171) beta=13.25
Iter 11000 | Total loss: 51.3611 (MSE:0.0024, Reg:51.3587) beta=12.12
Iter 12000 | Total loss: 37.4759 (MSE:0.0026, Reg:37.4734) beta=11.00
Iter 13000 | Total loss: 28.8971 (MSE:0.0023, Reg:28.8948) beta=9.88
Iter 14000 | Total loss: 18.0070 (MSE:0.0023, Reg:18.0047) beta=8.75
Iter 15000 | Total loss: 9.5079 (MSE:0.0025, Reg:9.5053) beta=7.62
Iter 16000 | Total loss: 2.4406 (MSE:0.0023, Reg:2.4383) beta=6.50
Iter 17000 | Total loss: 0.4352 (MSE:0.0024, Reg:0.4328) beta=5.38
Iter 18000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 25677.8926 (MSE:0.0040, Reg:25677.8887) beta=20.00
Iter  5000 | Total loss: 2291.7991 (MSE:0.0044, Reg:2291.7947) beta=18.88
Iter  6000 | Total loss: 1212.3065 (MSE:0.0041, Reg:1212.3025) beta=17.75
Iter  7000 | Total loss: 762.9880 (MSE:0.0043, Reg:762.9838) beta=16.62
Iter  8000 | Total loss: 540.6906 (MSE:0.0041, Reg:540.6865) beta=15.50
Iter  9000 | Total loss: 418.7413 (MSE:0.0042, Reg:418.7371) beta=14.38
Iter 10000 | Total loss: 310.8858 (MSE:0.0041, Reg:310.8817) beta=13.25
Iter 11000 | Total loss: 233.5245 (MSE:0.0047, Reg:233.5198) beta=12.12
Iter 12000 | Total loss: 169.4097 (MSE:0.0041, Reg:169.4057) beta=11.00
Iter 13000 | Total loss: 106.6024 (MSE:0.0043, Reg:106.5981) beta=9.88
Iter 14000 | Total loss: 57.4665 (MSE:0.0043, Reg:57.4621) beta=8.75
Iter 15000 | Total loss: 26.0911 (MSE:0.0043, Reg:26.0868) beta=7.62
Iter 16000 | Total loss: 5.9045 (MSE:0.0042, Reg:5.9003) beta=6.50
Iter 17000 | Total loss: 1.2218 (MSE:0.0042, Reg:1.2176) beta=5.38
Iter 18000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 38535.3477 (MSE:0.0048, Reg:38535.3438) beta=20.00
Iter  5000 | Total loss: 2279.4873 (MSE:0.0050, Reg:2279.4822) beta=18.88
Iter  6000 | Total loss: 1218.7955 (MSE:0.0045, Reg:1218.7910) beta=17.75
Iter  7000 | Total loss: 789.1756 (MSE:0.0047, Reg:789.1709) beta=16.62
Iter  8000 | Total loss: 589.5103 (MSE:0.0048, Reg:589.5055) beta=15.50
Iter  9000 | Total loss: 464.8958 (MSE:0.0051, Reg:464.8907) beta=14.38
Iter 10000 | Total loss: 363.5489 (MSE:0.0049, Reg:363.5440) beta=13.25
Iter 11000 | Total loss: 280.8470 (MSE:0.0046, Reg:280.8424) beta=12.12
Iter 12000 | Total loss: 206.7181 (MSE:0.0049, Reg:206.7132) beta=11.00
Iter 13000 | Total loss: 148.9174 (MSE:0.0048, Reg:148.9126) beta=9.88
Iter 14000 | Total loss: 89.8487 (MSE:0.0045, Reg:89.8441) beta=8.75
Iter 15000 | Total loss: 51.2002 (MSE:0.0048, Reg:51.1954) beta=7.62
Iter 16000 | Total loss: 18.4000 (MSE:0.0047, Reg:18.3952) beta=6.50
Iter 17000 | Total loss: 3.4364 (MSE:0.0046, Reg:3.4318) beta=5.38
Iter 18000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 56330.1758 (MSE:0.0040, Reg:56330.1719) beta=20.00
Iter  5000 | Total loss: 1752.3507 (MSE:0.0046, Reg:1752.3462) beta=18.88
Iter  6000 | Total loss: 804.8326 (MSE:0.0041, Reg:804.8285) beta=17.75
Iter  7000 | Total loss: 471.0930 (MSE:0.0045, Reg:471.0885) beta=16.62
Iter  8000 | Total loss: 343.1495 (MSE:0.0045, Reg:343.1451) beta=15.50
Iter  9000 | Total loss: 258.9562 (MSE:0.0047, Reg:258.9516) beta=14.38
Iter 10000 | Total loss: 184.3153 (MSE:0.0044, Reg:184.3109) beta=13.25
Iter 11000 | Total loss: 139.4133 (MSE:0.0045, Reg:139.4089) beta=12.12
Iter 12000 | Total loss: 102.8091 (MSE:0.0044, Reg:102.8047) beta=11.00
Iter 13000 | Total loss: 71.3197 (MSE:0.0046, Reg:71.3151) beta=9.88
Iter 14000 | Total loss: 46.0794 (MSE:0.0044, Reg:46.0750) beta=8.75
Iter 15000 | Total loss: 30.5370 (MSE:0.0043, Reg:30.5326) beta=7.62
Iter 16000 | Total loss: 14.2518 (MSE:0.0046, Reg:14.2473) beta=6.50
Iter 17000 | Total loss: 4.4883 (MSE:0.0044, Reg:4.4839) beta=5.38
Iter 18000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 112358.2734 (MSE:0.0043, Reg:112358.2656) beta=20.00
Iter  5000 | Total loss: 2305.2288 (MSE:0.0051, Reg:2305.2236) beta=18.88
Iter  6000 | Total loss: 1107.5830 (MSE:0.0046, Reg:1107.5784) beta=17.75
Iter  7000 | Total loss: 670.0775 (MSE:0.0052, Reg:670.0723) beta=16.62
Iter  8000 | Total loss: 500.9355 (MSE:0.0050, Reg:500.9305) beta=15.50
Iter  9000 | Total loss: 376.3053 (MSE:0.0052, Reg:376.3001) beta=14.38
Iter 10000 | Total loss: 278.2040 (MSE:0.0051, Reg:278.1989) beta=13.25
Iter 11000 | Total loss: 196.8203 (MSE:0.0048, Reg:196.8156) beta=12.12
Iter 12000 | Total loss: 149.2660 (MSE:0.0051, Reg:149.2608) beta=11.00
Iter 13000 | Total loss: 111.4097 (MSE:0.0047, Reg:111.4050) beta=9.88
Iter 14000 | Total loss: 78.6655 (MSE:0.0049, Reg:78.6606) beta=8.75
Iter 15000 | Total loss: 48.5116 (MSE:0.0048, Reg:48.5069) beta=7.62
Iter 16000 | Total loss: 23.1368 (MSE:0.0050, Reg:23.1318) beta=6.50
Iter 17000 | Total loss: 3.6966 (MSE:0.0050, Reg:3.6915) beta=5.38
Iter 18000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0047 (MSE:0.0047, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 12449.3613 (MSE:0.0004, Reg:12449.3613) beta=20.00
Iter  5000 | Total loss: 257.2064 (MSE:0.0005, Reg:257.2059) beta=18.88
Iter  6000 | Total loss: 134.4938 (MSE:0.0005, Reg:134.4933) beta=17.75
Iter  7000 | Total loss: 87.8386 (MSE:0.0005, Reg:87.8380) beta=16.62
Iter  8000 | Total loss: 67.1447 (MSE:0.0005, Reg:67.1442) beta=15.50
Iter  9000 | Total loss: 52.2974 (MSE:0.0005, Reg:52.2969) beta=14.38
Iter 10000 | Total loss: 44.0365 (MSE:0.0005, Reg:44.0360) beta=13.25
Iter 11000 | Total loss: 36.9734 (MSE:0.0005, Reg:36.9729) beta=12.12
Iter 12000 | Total loss: 28.0005 (MSE:0.0005, Reg:28.0000) beta=11.00
Iter 13000 | Total loss: 24.0006 (MSE:0.0006, Reg:24.0000) beta=9.88
Iter 14000 | Total loss: 17.0547 (MSE:0.0005, Reg:17.0542) beta=8.75
Iter 15000 | Total loss: 8.7823 (MSE:0.0006, Reg:8.7817) beta=7.62
Iter 16000 | Total loss: 3.4163 (MSE:0.0005, Reg:3.4158) beta=6.50
Iter 17000 | Total loss: 1.0005 (MSE:0.0005, Reg:1.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 108419.7500 (MSE:0.0027, Reg:108419.7500) beta=20.00
Iter  5000 | Total loss: 1081.1289 (MSE:0.0036, Reg:1081.1254) beta=18.88
Iter  6000 | Total loss: 498.7933 (MSE:0.0035, Reg:498.7898) beta=17.75
Iter  7000 | Total loss: 333.9720 (MSE:0.0034, Reg:333.9686) beta=16.62
Iter  8000 | Total loss: 234.8898 (MSE:0.0034, Reg:234.8864) beta=15.50
Iter  9000 | Total loss: 179.6361 (MSE:0.0035, Reg:179.6326) beta=14.38
Iter 10000 | Total loss: 131.8694 (MSE:0.0033, Reg:131.8661) beta=13.25
Iter 11000 | Total loss: 94.6435 (MSE:0.0031, Reg:94.6404) beta=12.12
Iter 12000 | Total loss: 73.3162 (MSE:0.0034, Reg:73.3128) beta=11.00
Iter 13000 | Total loss: 51.6772 (MSE:0.0035, Reg:51.6737) beta=9.88
Iter 14000 | Total loss: 36.1190 (MSE:0.0033, Reg:36.1157) beta=8.75
Iter 15000 | Total loss: 18.8215 (MSE:0.0035, Reg:18.8180) beta=7.62
Iter 16000 | Total loss: 8.4893 (MSE:0.0035, Reg:8.4858) beta=6.50
Iter 17000 | Total loss: 1.0536 (MSE:0.0034, Reg:1.0502) beta=5.38
Iter 18000 | Total loss: 0.7519 (MSE:0.0034, Reg:0.7485) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 144127.9062 (MSE:0.0038, Reg:144127.9062) beta=20.00
Iter  5000 | Total loss: 2151.5791 (MSE:0.0045, Reg:2151.5747) beta=18.88
Iter  6000 | Total loss: 842.0946 (MSE:0.0041, Reg:842.0905) beta=17.75
Iter  7000 | Total loss: 553.9306 (MSE:0.0041, Reg:553.9266) beta=16.62
Iter  8000 | Total loss: 402.1852 (MSE:0.0042, Reg:402.1810) beta=15.50
Iter  9000 | Total loss: 288.0211 (MSE:0.0044, Reg:288.0167) beta=14.38
Iter 10000 | Total loss: 214.8549 (MSE:0.0041, Reg:214.8508) beta=13.25
Iter 11000 | Total loss: 162.7915 (MSE:0.0042, Reg:162.7873) beta=12.12
Iter 12000 | Total loss: 116.8068 (MSE:0.0045, Reg:116.8022) beta=11.00
Iter 13000 | Total loss: 84.8649 (MSE:0.0043, Reg:84.8606) beta=9.88
Iter 14000 | Total loss: 53.9010 (MSE:0.0039, Reg:53.8971) beta=8.75
Iter 15000 | Total loss: 29.9362 (MSE:0.0043, Reg:29.9319) beta=7.62
Iter 16000 | Total loss: 13.1858 (MSE:0.0042, Reg:13.1815) beta=6.50
Iter 17000 | Total loss: 3.2797 (MSE:0.0044, Reg:3.2753) beta=5.38
Iter 18000 | Total loss: 0.6792 (MSE:0.0040, Reg:0.6752) beta=4.25
Iter 19000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 232533.5312 (MSE:0.0033, Reg:232533.5312) beta=20.00
Iter  5000 | Total loss: 283.5542 (MSE:0.0037, Reg:283.5505) beta=18.88
Iter  6000 | Total loss: 149.6610 (MSE:0.0038, Reg:149.6573) beta=17.75
Iter  7000 | Total loss: 89.8995 (MSE:0.0038, Reg:89.8957) beta=16.62
Iter  8000 | Total loss: 63.5851 (MSE:0.0037, Reg:63.5814) beta=15.50
Iter  9000 | Total loss: 48.0032 (MSE:0.0036, Reg:47.9995) beta=14.38
Iter 10000 | Total loss: 35.0656 (MSE:0.0038, Reg:35.0618) beta=13.25
Iter 11000 | Total loss: 21.9979 (MSE:0.0038, Reg:21.9941) beta=12.12
Iter 12000 | Total loss: 10.9976 (MSE:0.0037, Reg:10.9939) beta=11.00
Iter 13000 | Total loss: 6.0037 (MSE:0.0037, Reg:6.0000) beta=9.88
Iter 14000 | Total loss: 4.0039 (MSE:0.0039, Reg:4.0000) beta=8.75
Iter 15000 | Total loss: 3.9150 (MSE:0.0038, Reg:3.9112) beta=7.62
Iter 16000 | Total loss: 0.6172 (MSE:0.0037, Reg:0.6136) beta=6.50
Iter 17000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0117 (MSE:0.0117, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0109 (MSE:0.0109, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0114 (MSE:0.0114, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 500477.7500 (MSE:0.0108, Reg:500477.7500) beta=20.00
Iter  5000 | Total loss: 3307.2808 (MSE:0.0122, Reg:3307.2686) beta=18.88
Iter  6000 | Total loss: 1589.5815 (MSE:0.0114, Reg:1589.5702) beta=17.75
Iter  7000 | Total loss: 1014.2377 (MSE:0.0122, Reg:1014.2255) beta=16.62
Iter  8000 | Total loss: 707.0131 (MSE:0.0117, Reg:707.0013) beta=15.50
Iter  9000 | Total loss: 526.1459 (MSE:0.0117, Reg:526.1343) beta=14.38
Iter 10000 | Total loss: 394.8924 (MSE:0.0118, Reg:394.8806) beta=13.25
Iter 11000 | Total loss: 297.8394 (MSE:0.0116, Reg:297.8278) beta=12.12
Iter 12000 | Total loss: 204.0019 (MSE:0.0123, Reg:203.9896) beta=11.00
Iter 13000 | Total loss: 145.7617 (MSE:0.0122, Reg:145.7495) beta=9.88
Iter 14000 | Total loss: 87.2675 (MSE:0.0113, Reg:87.2561) beta=8.75
Iter 15000 | Total loss: 47.3058 (MSE:0.0115, Reg:47.2943) beta=7.62
Iter 16000 | Total loss: 19.8397 (MSE:0.0114, Reg:19.8283) beta=6.50
Iter 17000 | Total loss: 3.4196 (MSE:0.0119, Reg:3.4077) beta=5.38
Iter 18000 | Total loss: 0.0124 (MSE:0.0124, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0118 (MSE:0.0118, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0123 (MSE:0.0123, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 39249.3555 (MSE:0.0034, Reg:39249.3516) beta=20.00
Iter  5000 | Total loss: 1711.6559 (MSE:0.0042, Reg:1711.6516) beta=18.88
Iter  6000 | Total loss: 894.7770 (MSE:0.0038, Reg:894.7732) beta=17.75
Iter  7000 | Total loss: 604.1204 (MSE:0.0039, Reg:604.1165) beta=16.62
Iter  8000 | Total loss: 459.4333 (MSE:0.0041, Reg:459.4293) beta=15.50
Iter  9000 | Total loss: 350.8452 (MSE:0.0038, Reg:350.8414) beta=14.38
Iter 10000 | Total loss: 270.3961 (MSE:0.0042, Reg:270.3918) beta=13.25
Iter 11000 | Total loss: 200.1143 (MSE:0.0040, Reg:200.1102) beta=12.12
Iter 12000 | Total loss: 141.6811 (MSE:0.0039, Reg:141.6772) beta=11.00
Iter 13000 | Total loss: 85.1497 (MSE:0.0038, Reg:85.1458) beta=9.88
Iter 14000 | Total loss: 48.8455 (MSE:0.0039, Reg:48.8416) beta=8.75
Iter 15000 | Total loss: 26.1950 (MSE:0.0044, Reg:26.1906) beta=7.62
Iter 16000 | Total loss: 11.9852 (MSE:0.0038, Reg:11.9814) beta=6.50
Iter 17000 | Total loss: 1.6699 (MSE:0.0040, Reg:1.6660) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 568475.5625 (MSE:0.0053, Reg:568475.5625) beta=20.00
Iter  5000 | Total loss: 176.1695 (MSE:0.0056, Reg:176.1638) beta=18.88
Iter  6000 | Total loss: 64.4331 (MSE:0.0061, Reg:64.4270) beta=17.75
Iter  7000 | Total loss: 38.5729 (MSE:0.0063, Reg:38.5666) beta=16.62
Iter  8000 | Total loss: 25.5499 (MSE:0.0064, Reg:25.5436) beta=15.50
Iter  9000 | Total loss: 19.0037 (MSE:0.0059, Reg:18.9977) beta=14.38
Iter 10000 | Total loss: 15.0062 (MSE:0.0062, Reg:15.0000) beta=13.25
Iter 11000 | Total loss: 11.0053 (MSE:0.0064, Reg:10.9990) beta=12.12
Iter 12000 | Total loss: 9.0063 (MSE:0.0063, Reg:9.0000) beta=11.00
Iter 13000 | Total loss: 6.0057 (MSE:0.0057, Reg:6.0000) beta=9.88
Iter 14000 | Total loss: 4.1222 (MSE:0.0058, Reg:4.1164) beta=8.75
Iter 15000 | Total loss: 0.9169 (MSE:0.0062, Reg:0.9108) beta=7.62
Iter 16000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0064 (MSE:0.0064, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3764 (MSE:0.3764, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3770 (MSE:0.3770, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.3718 (MSE:0.3718, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3385 (MSE:0.3385, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 374801.0000 (MSE:0.3362, Reg:374800.6562) beta=20.00
Iter  5000 | Total loss: 24122.8633 (MSE:0.3471, Reg:24122.5156) beta=18.88
Iter  6000 | Total loss: 12841.0713 (MSE:0.3602, Reg:12840.7109) beta=17.75
Iter  7000 | Total loss: 8597.8994 (MSE:0.3611, Reg:8597.5381) beta=16.62
Iter  8000 | Total loss: 6134.7173 (MSE:0.3701, Reg:6134.3472) beta=15.50
Iter  9000 | Total loss: 4780.7783 (MSE:0.3564, Reg:4780.4219) beta=14.38
Iter 10000 | Total loss: 3856.7432 (MSE:0.3577, Reg:3856.3855) beta=13.25
Iter 11000 | Total loss: 3103.4885 (MSE:0.3469, Reg:3103.1416) beta=12.12
Iter 12000 | Total loss: 2430.2319 (MSE:0.3931, Reg:2429.8389) beta=11.00
Iter 13000 | Total loss: 1830.2651 (MSE:0.3450, Reg:1829.9202) beta=9.88
Iter 14000 | Total loss: 1258.0913 (MSE:0.3657, Reg:1257.7256) beta=8.75
Iter 15000 | Total loss: 772.1747 (MSE:0.3636, Reg:771.8111) beta=7.62
Iter 16000 | Total loss: 379.8990 (MSE:0.3639, Reg:379.5351) beta=6.50
Iter 17000 | Total loss: 118.1967 (MSE:0.3835, Reg:117.8132) beta=5.38
Iter 18000 | Total loss: 4.0071 (MSE:0.3654, Reg:3.6416) beta=4.25
Iter 19000 | Total loss: 0.3527 (MSE:0.3527, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3470 (MSE:0.3470, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.3662 (MSE:0.3662, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2746 (MSE:0.2746, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2612 (MSE:0.2612, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2827 (MSE:0.2827, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 89416.8359 (MSE:0.2594, Reg:89416.5781) beta=20.00
Iter  5000 | Total loss: 2708.3030 (MSE:0.3400, Reg:2707.9629) beta=18.88
Iter  6000 | Total loss: 1107.3770 (MSE:0.3891, Reg:1106.9878) beta=17.75
Iter  7000 | Total loss: 734.2499 (MSE:0.3478, Reg:733.9021) beta=16.62
Iter  8000 | Total loss: 507.2377 (MSE:0.3250, Reg:506.9127) beta=15.50
Iter  9000 | Total loss: 398.1783 (MSE:0.3113, Reg:397.8670) beta=14.38
Iter 10000 | Total loss: 323.2596 (MSE:0.2962, Reg:322.9633) beta=13.25
Iter 11000 | Total loss: 277.4594 (MSE:0.3132, Reg:277.1462) beta=12.12
Iter 12000 | Total loss: 214.4019 (MSE:0.3067, Reg:214.0952) beta=11.00
Iter 13000 | Total loss: 159.2962 (MSE:0.3589, Reg:158.9374) beta=9.88
Iter 14000 | Total loss: 125.1618 (MSE:0.3008, Reg:124.8609) beta=8.75
Iter 15000 | Total loss: 81.7894 (MSE:0.3392, Reg:81.4501) beta=7.62
Iter 16000 | Total loss: 42.6110 (MSE:0.3136, Reg:42.2974) beta=6.50
Iter 17000 | Total loss: 9.4228 (MSE:0.3070, Reg:9.1158) beta=5.38
Iter 18000 | Total loss: 1.3530 (MSE:0.3530, Reg:1.0000) beta=4.25
Iter 19000 | Total loss: 0.3533 (MSE:0.3533, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3252 (MSE:0.3252, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 68.210%
Total time: 872.74 sec

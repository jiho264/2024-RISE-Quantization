
Case: [ resnet18_AdaRound_NormQuantizer_CH_W4A8_BNFold ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: True
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: NormQuantizer
    - dstDtype: INT4
    - per_channel: True
    - p: 2.4
    - AdaRound: True

- activation params:
    - scheme: NormQuantizer
    - dstDtype: INT8
    - per_channel: False
    - p: 2.4

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0059 (MSE:0.0059, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0068 (MSE:0.0068, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0108 (MSE:0.0108, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0174 (MSE:0.0174, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0172 (MSE:0.0172, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0166 (MSE:0.0166, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0176 (MSE:0.0176, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0186 (MSE:0.0186, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0169 (MSE:0.0169, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0169 (MSE:0.0169, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0220 (MSE:0.0220, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0189 (MSE:0.0189, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0169 (MSE:0.0169, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0171 (MSE:0.0171, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0175 (MSE:0.0175, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0176 (MSE:0.0176, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0173 (MSE:0.0173, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0185 (MSE:0.0185, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0167 (MSE:0.0167, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0299 (MSE:0.0299, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0390 (MSE:0.0390, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0391 (MSE:0.0391, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0379 (MSE:0.0379, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0389 (MSE:0.0389, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0392 (MSE:0.0392, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0366 (MSE:0.0366, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0386 (MSE:0.0386, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0337 (MSE:0.0337, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0405 (MSE:0.0405, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0366 (MSE:0.0366, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0381 (MSE:0.0381, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0372 (MSE:0.0372, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0369 (MSE:0.0369, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0399 (MSE:0.0399, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0373 (MSE:0.0373, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0367 (MSE:0.0367, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0330 (MSE:0.0330, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0420 (MSE:0.0420, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0360 (MSE:0.0360, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0381 (MSE:0.0381, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0211 (MSE:0.0211, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0197 (MSE:0.0197, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0213 (MSE:0.0213, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0216 (MSE:0.0216, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0211 (MSE:0.0211, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0235 (MSE:0.0235, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0237 (MSE:0.0237, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0217 (MSE:0.0217, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0232 (MSE:0.0232, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0212 (MSE:0.0212, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0226 (MSE:0.0226, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0222 (MSE:0.0222, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0209 (MSE:0.0209, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0217 (MSE:0.0217, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0218 (MSE:0.0218, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0224 (MSE:0.0224, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0219 (MSE:0.0219, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0215 (MSE:0.0215, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0230 (MSE:0.0230, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0235 (MSE:0.0235, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0632 (MSE:0.0632, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0845 (MSE:0.0845, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0667 (MSE:0.0667, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0658 (MSE:0.0658, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0727 (MSE:0.0727, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0745 (MSE:0.0745, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0738 (MSE:0.0738, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0661 (MSE:0.0661, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0704 (MSE:0.0704, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0681 (MSE:0.0681, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0742 (MSE:0.0742, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0741 (MSE:0.0741, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0736 (MSE:0.0736, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0695 (MSE:0.0695, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0823 (MSE:0.0823, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0672 (MSE:0.0672, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0681 (MSE:0.0681, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0755 (MSE:0.0755, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0779 (MSE:0.0779, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0722 (MSE:0.0722, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0741 (MSE:0.0741, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0296 (MSE:0.0296, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0314 (MSE:0.0314, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0310 (MSE:0.0310, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0302 (MSE:0.0302, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0304 (MSE:0.0304, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0313 (MSE:0.0313, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0340 (MSE:0.0340, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0307 (MSE:0.0307, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0311 (MSE:0.0311, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0333 (MSE:0.0333, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0314 (MSE:0.0314, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0319 (MSE:0.0319, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0326 (MSE:0.0326, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0341 (MSE:0.0341, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0313 (MSE:0.0313, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0337 (MSE:0.0337, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0337 (MSE:0.0337, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0333 (MSE:0.0333, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0342 (MSE:0.0342, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0288 (MSE:0.0288, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0340 (MSE:0.0340, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0515 (MSE:0.0515, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0482 (MSE:0.0482, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0499 (MSE:0.0499, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0541 (MSE:0.0541, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0514 (MSE:0.0514, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0543 (MSE:0.0543, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0556 (MSE:0.0556, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0524 (MSE:0.0524, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0584 (MSE:0.0584, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0490 (MSE:0.0490, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0493 (MSE:0.0493, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0562 (MSE:0.0562, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0524 (MSE:0.0524, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0515 (MSE:0.0515, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0521 (MSE:0.0521, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0535 (MSE:0.0535, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0530 (MSE:0.0530, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0531 (MSE:0.0531, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0529 (MSE:0.0529, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0185 (MSE:0.0185, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0195 (MSE:0.0195, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0188 (MSE:0.0188, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0210 (MSE:0.0210, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0180 (MSE:0.0180, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0208 (MSE:0.0208, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0184 (MSE:0.0184, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0231 (MSE:0.0231, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0200 (MSE:0.0200, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0183 (MSE:0.0183, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0192 (MSE:0.0192, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0199 (MSE:0.0199, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0192 (MSE:0.0192, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0191 (MSE:0.0191, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0204 (MSE:0.0204, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0190 (MSE:0.0190, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0196 (MSE:0.0196, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0185 (MSE:0.0185, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0347 (MSE:0.0347, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0423 (MSE:0.0423, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0407 (MSE:0.0407, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0411 (MSE:0.0411, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0431 (MSE:0.0431, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0404 (MSE:0.0404, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0416 (MSE:0.0416, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0427 (MSE:0.0427, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0411 (MSE:0.0411, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0420 (MSE:0.0420, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0424 (MSE:0.0424, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0391 (MSE:0.0391, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0429 (MSE:0.0429, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0394 (MSE:0.0394, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0414 (MSE:0.0414, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0420 (MSE:0.0420, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0423 (MSE:0.0423, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0415 (MSE:0.0415, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0414 (MSE:0.0414, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0431 (MSE:0.0431, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0449 (MSE:0.0449, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0456 (MSE:0.0456, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0442 (MSE:0.0442, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0491 (MSE:0.0491, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0457 (MSE:0.0457, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0462 (MSE:0.0462, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0504 (MSE:0.0504, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0471 (MSE:0.0471, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0474 (MSE:0.0474, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0465 (MSE:0.0465, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0488 (MSE:0.0488, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0486 (MSE:0.0486, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0496 (MSE:0.0496, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0500 (MSE:0.0500, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0453 (MSE:0.0453, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0447 (MSE:0.0447, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0439 (MSE:0.0439, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0461 (MSE:0.0461, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0471 (MSE:0.0471, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0527 (MSE:0.0527, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0477 (MSE:0.0477, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0494 (MSE:0.0494, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0454 (MSE:0.0454, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0449 (MSE:0.0449, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0451 (MSE:0.0451, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0472 (MSE:0.0472, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0467 (MSE:0.0467, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0454 (MSE:0.0454, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0450 (MSE:0.0450, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0464 (MSE:0.0464, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0453 (MSE:0.0453, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0448 (MSE:0.0448, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0469 (MSE:0.0469, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0435 (MSE:0.0435, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0486 (MSE:0.0486, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0459 (MSE:0.0459, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0456 (MSE:0.0456, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0587 (MSE:0.0587, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0604 (MSE:0.0604, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0628 (MSE:0.0628, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0669 (MSE:0.0669, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0652 (MSE:0.0652, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0621 (MSE:0.0621, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0629 (MSE:0.0629, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0648 (MSE:0.0648, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0637 (MSE:0.0637, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0627 (MSE:0.0627, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0641 (MSE:0.0641, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0640 (MSE:0.0640, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0657 (MSE:0.0657, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0628 (MSE:0.0628, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0634 (MSE:0.0634, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0653 (MSE:0.0653, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0683 (MSE:0.0683, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0621 (MSE:0.0621, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0646 (MSE:0.0646, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0688 (MSE:0.0688, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0698 (MSE:0.0698, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0483 (MSE:0.0483, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0528 (MSE:0.0528, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0667 (MSE:0.0667, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0670 (MSE:0.0670, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0669 (MSE:0.0669, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0723 (MSE:0.0723, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0696 (MSE:0.0696, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0716 (MSE:0.0716, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0653 (MSE:0.0653, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0656 (MSE:0.0656, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0744 (MSE:0.0744, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0638 (MSE:0.0638, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0728 (MSE:0.0728, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0682 (MSE:0.0682, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0652 (MSE:0.0652, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0685 (MSE:0.0685, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0670 (MSE:0.0670, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0716 (MSE:0.0716, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0681 (MSE:0.0681, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0697 (MSE:0.0697, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0713 (MSE:0.0713, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1035 (MSE:0.1035, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0990 (MSE:0.0990, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1023 (MSE:0.1023, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1094 (MSE:0.1094, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1050 (MSE:0.1050, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0974 (MSE:0.0974, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0931 (MSE:0.0931, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0980 (MSE:0.0980, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.1025 (MSE:0.1025, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0964 (MSE:0.0964, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.1000 (MSE:0.1000, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0933 (MSE:0.0933, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1036 (MSE:0.1036, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1029 (MSE:0.1029, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0984 (MSE:0.0984, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1076 (MSE:0.1076, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0914 (MSE:0.0914, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0987 (MSE:0.0987, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0940 (MSE:0.0940, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.1036 (MSE:0.1036, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1052 (MSE:0.1052, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0502 (MSE:0.0502, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0515 (MSE:0.0515, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0530 (MSE:0.0530, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0498 (MSE:0.0498, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0506 (MSE:0.0506, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0562 (MSE:0.0562, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0558 (MSE:0.0558, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0513 (MSE:0.0513, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0464 (MSE:0.0464, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0488 (MSE:0.0488, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0527 (MSE:0.0527, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0496 (MSE:0.0496, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0492 (MSE:0.0492, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0525 (MSE:0.0525, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0526 (MSE:0.0526, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0502 (MSE:0.0502, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0508 (MSE:0.0508, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0564 (MSE:0.0564, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1790 (MSE:0.1790, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.1848 (MSE:0.1848, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2105 (MSE:0.2105, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.1883 (MSE:0.1883, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.1818 (MSE:0.1818, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1726 (MSE:0.1726, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1913 (MSE:0.1913, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.1694 (MSE:0.1694, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.2236 (MSE:0.2236, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.2095 (MSE:0.2095, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.2011 (MSE:0.2011, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1682 (MSE:0.1682, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1990 (MSE:0.1990, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1911 (MSE:0.1911, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1996 (MSE:0.1996, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.1955 (MSE:0.1955, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1698 (MSE:0.1698, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.1884 (MSE:0.1884, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1882 (MSE:0.1882, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2128 (MSE:0.2128, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1894 (MSE:0.1894, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0506 (MSE:0.0506, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0493 (MSE:0.0493, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0523 (MSE:0.0523, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0510 (MSE:0.0510, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0506 (MSE:0.0506, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0498 (MSE:0.0498, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0511 (MSE:0.0511, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0483 (MSE:0.0483, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0573 (MSE:0.0573, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0475 (MSE:0.0475, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0507 (MSE:0.0507, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0498 (MSE:0.0498, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0470 (MSE:0.0470, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0497 (MSE:0.0497, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0474 (MSE:0.0474, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0566 (MSE:0.0566, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0564 (MSE:0.0564, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0507 (MSE:0.0507, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0463 (MSE:0.0463, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0828 (MSE:0.0828, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0909 (MSE:0.0909, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.1053 (MSE:0.1053, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0894 (MSE:0.0894, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0987 (MSE:0.0987, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.1016 (MSE:0.1016, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.1058 (MSE:0.1058, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0982 (MSE:0.0982, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0894 (MSE:0.0894, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0932 (MSE:0.0932, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0933 (MSE:0.0933, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.1041 (MSE:0.1041, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.1000 (MSE:0.1000, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.1019 (MSE:0.1019, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.1137 (MSE:0.1137, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0965 (MSE:0.0965, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0981 (MSE:0.0981, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0947 (MSE:0.0947, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0892 (MSE:0.0892, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0966 (MSE:0.0966, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0919 (MSE:0.0919, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 4.0277 (MSE:4.0277, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 3.9297 (MSE:3.9297, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 3.6227 (MSE:3.6227, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 3.5552 (MSE:3.5552, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 3.9931 (MSE:3.9931, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 3.9454 (MSE:3.9454, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 3.7224 (MSE:3.7224, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 3.9730 (MSE:3.9730, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 3.9586 (MSE:3.9586, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 3.8503 (MSE:3.8503, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 3.6740 (MSE:3.6740, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 3.6901 (MSE:3.6901, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 3.7437 (MSE:3.7437, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 3.7523 (MSE:3.7523, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 3.4060 (MSE:3.4060, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 3.6903 (MSE:3.6903, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 3.9395 (MSE:3.9395, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 3.5388 (MSE:3.5388, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 3.6957 (MSE:3.6957, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 3.5753 (MSE:3.5753, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 3.5114 (MSE:3.5114, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    p = 2.4
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 7.7848 (MSE:7.7848, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 6.7078 (MSE:6.7078, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 6.8692 (MSE:6.8692, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 7.0477 (MSE:7.0477, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 7.3402 (MSE:7.3402, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 7.7669 (MSE:7.7669, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 6.5369 (MSE:6.5369, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 7.5407 (MSE:7.5407, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 7.2893 (MSE:7.2893, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 6.8373 (MSE:6.8373, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 7.0226 (MSE:7.0226, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 6.9117 (MSE:6.9117, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 6.8414 (MSE:6.8414, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 7.7485 (MSE:7.7485, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 7.6057 (MSE:7.6057, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 7.5125 (MSE:7.5125, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 6.6084 (MSE:6.6084, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 7.1703 (MSE:7.1703, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 7.1067 (MSE:7.1067, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 6.9653 (MSE:6.9653, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 8.0428 (MSE:8.0428, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 0.198%
Total time: 1433.40 sec

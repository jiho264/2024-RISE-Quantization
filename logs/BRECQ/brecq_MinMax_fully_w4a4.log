
Case: [ resnet18_BRECQ_MinMaxQuantizer_CH_W4A4_RoundingLR0.01 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.01

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - BRECQ: True

- activation params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: False

Replace to QuantLayer
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 0 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
- Quant Block 1 making done !
    2D search with INT4
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantLayer: conv1, torch.Size([64, 3, 7, 7])
    QuantLayer: layer1.0.conv_bn_relu_1, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.0.conv_bn_2, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.1.conv_bn_relu_1, torch.Size([64, 64, 3, 3])
    QuantLayer: layer1.1.conv_bn_2, torch.Size([64, 64, 3, 3])
    QuantLayer: layer2.0.conv_bn_relu_1, torch.Size([128, 64, 3, 3])
    QuantLayer: layer2.0.conv_bn_2, torch.Size([128, 128, 3, 3])
    QuantLayer: layer2.0.conv_bn_down, torch.Size([128, 64, 1, 1])
    QuantLayer: layer2.1.conv_bn_relu_1, torch.Size([128, 128, 3, 3])
    QuantLayer: layer2.1.conv_bn_2, torch.Size([128, 128, 3, 3])
    QuantLayer: layer3.0.conv_bn_relu_1, torch.Size([256, 128, 3, 3])
    QuantLayer: layer3.0.conv_bn_2, torch.Size([256, 256, 3, 3])
    QuantLayer: layer3.0.conv_bn_down, torch.Size([256, 128, 1, 1])
    QuantLayer: layer3.1.conv_bn_relu_1, torch.Size([256, 256, 3, 3])
    QuantLayer: layer3.1.conv_bn_2, torch.Size([256, 256, 3, 3])
    QuantLayer: layer4.0.conv_bn_relu_1, torch.Size([512, 256, 3, 3])
    QuantLayer: layer4.0.conv_bn_2, torch.Size([512, 512, 3, 3])
    QuantLayer: layer4.0.conv_bn_down, torch.Size([512, 256, 1, 1])
    QuantLayer: layer4.1.conv_bn_relu_1, torch.Size([512, 512, 3, 3])
    QuantLayer: layer4.1.conv_bn_2, torch.Size([512, 512, 3, 3])
    QuantLayer: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
conv1........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
conv_bn_relu_1................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_2........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_relu_1................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_2........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
conv_bn_relu_1................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_2........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_down................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_relu_1........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_2................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
conv_bn_relu_1........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_2................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_down........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_relu_1................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_2........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
conv_bn_relu_1................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_2........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_down................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_relu_1........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
conv_bn_2................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
fc........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])
0........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
1........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
0................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
1................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
0........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
1........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
0................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
1................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
    1D search with UINT4
Activation quantizer initialized from QuantLayer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0220 (MSE:0.0220, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 988.5448 (MSE:0.0018, Reg:988.5430) beta=20.00
Iter  5000 | Total loss: 48.9959 (MSE:0.0014, Reg:48.9945) beta=18.88
Iter  6000 | Total loss: 31.6756 (MSE:0.0023, Reg:31.6733) beta=17.75
Iter  7000 | Total loss: 24.0024 (MSE:0.0024, Reg:24.0000) beta=16.62
Iter  8000 | Total loss: 16.0017 (MSE:0.0017, Reg:16.0000) beta=15.50
Iter  9000 | Total loss: 11.0018 (MSE:0.0018, Reg:11.0000) beta=14.38
Iter 10000 | Total loss: 6.0017 (MSE:0.0017, Reg:6.0000) beta=13.25
Iter 11000 | Total loss: 6.0019 (MSE:0.0019, Reg:6.0000) beta=12.12
Iter 12000 | Total loss: 3.0018 (MSE:0.0018, Reg:3.0000) beta=11.00
Iter 13000 | Total loss: 2.0021 (MSE:0.0021, Reg:2.0000) beta=9.88
Iter 14000 | Total loss: 2.0017 (MSE:0.0017, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=2.00

[2/21] BRECQ computing: 0
,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0083 (MSE:0.0083, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9597.1943 (MSE:0.0068, Reg:9597.1875) beta=20.00
Iter  5000 | Total loss: 1200.3209 (MSE:0.0064, Reg:1200.3146) beta=18.88
Iter  6000 | Total loss: 671.3390 (MSE:0.0069, Reg:671.3322) beta=17.75
Iter  7000 | Total loss: 437.5821 (MSE:0.0061, Reg:437.5759) beta=16.62
Iter  8000 | Total loss: 271.0056 (MSE:0.0056, Reg:271.0000) beta=15.50
Iter  9000 | Total loss: 156.6422 (MSE:0.0059, Reg:156.6363) beta=14.38
Iter 10000 | Total loss: 91.9753 (MSE:0.0053, Reg:91.9700) beta=13.25
Iter 11000 | Total loss: 61.8715 (MSE:0.0058, Reg:61.8657) beta=12.12
Iter 12000 | Total loss: 33.6600 (MSE:0.0062, Reg:33.6538) beta=11.00
Iter 13000 | Total loss: 11.0056 (MSE:0.0056, Reg:11.0000) beta=9.88
Iter 14000 | Total loss: 5.0060 (MSE:0.0060, Reg:5.0000) beta=8.75
Iter 15000 | Total loss: 1.0060 (MSE:0.0060, Reg:1.0000) beta=7.62
Iter 16000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=2.00

[3/21] BRECQ computing: 1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0207 (MSE:0.0207, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0159 (MSE:0.0159, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0161 (MSE:0.0161, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 15294.7051 (MSE:0.0158, Reg:15294.6895) beta=20.00
Iter  5000 | Total loss: 2007.2889 (MSE:0.0164, Reg:2007.2725) beta=18.88
Iter  6000 | Total loss: 1242.4338 (MSE:0.0167, Reg:1242.4171) beta=17.75
Iter  7000 | Total loss: 815.1101 (MSE:0.0166, Reg:815.0935) beta=16.62
Iter  8000 | Total loss: 542.5631 (MSE:0.0148, Reg:542.5483) beta=15.50
Iter  9000 | Total loss: 359.8055 (MSE:0.0166, Reg:359.7888) beta=14.38
Iter 10000 | Total loss: 204.2491 (MSE:0.0164, Reg:204.2327) beta=13.25
Iter 11000 | Total loss: 128.0055 (MSE:0.0163, Reg:127.9892) beta=12.12
Iter 12000 | Total loss: 72.0162 (MSE:0.0162, Reg:72.0000) beta=11.00
Iter 13000 | Total loss: 23.7242 (MSE:0.0161, Reg:23.7081) beta=9.88
Iter 14000 | Total loss: 11.9434 (MSE:0.0166, Reg:11.9268) beta=8.75
Iter 15000 | Total loss: 0.0160 (MSE:0.0160, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0165 (MSE:0.0165, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0164 (MSE:0.0164, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0162 (MSE:0.0162, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0158 (MSE:0.0158, Reg:0.0000) beta=2.00

[4/21] BRECQ computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 65077.6719 (MSE:0.0052, Reg:65077.6680) beta=20.00
Iter  5000 | Total loss: 7025.2163 (MSE:0.0055, Reg:7025.2109) beta=18.88
Iter  6000 | Total loss: 4542.6733 (MSE:0.0055, Reg:4542.6680) beta=17.75
Iter  7000 | Total loss: 3250.9785 (MSE:0.0058, Reg:3250.9727) beta=16.62
Iter  8000 | Total loss: 2359.0681 (MSE:0.0055, Reg:2359.0625) beta=15.50
Iter  9000 | Total loss: 1637.7905 (MSE:0.0059, Reg:1637.7847) beta=14.38
Iter 10000 | Total loss: 1083.9565 (MSE:0.0057, Reg:1083.9508) beta=13.25
Iter 11000 | Total loss: 688.7848 (MSE:0.0057, Reg:688.7791) beta=12.12
Iter 12000 | Total loss: 363.4723 (MSE:0.0060, Reg:363.4663) beta=11.00
Iter 13000 | Total loss: 160.4112 (MSE:0.0057, Reg:160.4055) beta=9.88
Iter 14000 | Total loss: 44.0057 (MSE:0.0057, Reg:44.0000) beta=8.75
Iter 15000 | Total loss: 3.0059 (MSE:0.0059, Reg:3.0000) beta=7.62
Iter 16000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=2.00

[5/21] BRECQ computing: 1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 75014.0156 (MSE:0.0065, Reg:75014.0078) beta=20.00
Iter  5000 | Total loss: 10280.3721 (MSE:0.0069, Reg:10280.3652) beta=18.88
Iter  6000 | Total loss: 6788.9019 (MSE:0.0073, Reg:6788.8945) beta=17.75
Iter  7000 | Total loss: 4819.7134 (MSE:0.0074, Reg:4819.7061) beta=16.62
Iter  8000 | Total loss: 3532.3157 (MSE:0.0069, Reg:3532.3088) beta=15.50
Iter  9000 | Total loss: 2474.1978 (MSE:0.0069, Reg:2474.1909) beta=14.38
Iter 10000 | Total loss: 1556.8538 (MSE:0.0069, Reg:1556.8468) beta=13.25
Iter 11000 | Total loss: 867.9825 (MSE:0.0065, Reg:867.9760) beta=12.12
Iter 12000 | Total loss: 406.2978 (MSE:0.0070, Reg:406.2907) beta=11.00
Iter 13000 | Total loss: 144.0030 (MSE:0.0070, Reg:143.9960) beta=9.88
Iter 14000 | Total loss: 29.0070 (MSE:0.0070, Reg:29.0000) beta=8.75
Iter 15000 | Total loss: 2.0076 (MSE:0.0076, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 0.0069 (MSE:0.0069, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0076 (MSE:0.0076, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0073 (MSE:0.0073, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=2.00

[6/21] BRECQ computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 205269.6875 (MSE:0.0029, Reg:205269.6875) beta=20.00
Iter  5000 | Total loss: 19517.0820 (MSE:0.0032, Reg:19517.0781) beta=18.88
Iter  6000 | Total loss: 12073.7627 (MSE:0.0031, Reg:12073.7598) beta=17.75
Iter  7000 | Total loss: 8358.4102 (MSE:0.0029, Reg:8358.4072) beta=16.62
Iter  8000 | Total loss: 6019.3135 (MSE:0.0027, Reg:6019.3105) beta=15.50
Iter  9000 | Total loss: 4304.8252 (MSE:0.0029, Reg:4304.8223) beta=14.38
Iter 10000 | Total loss: 2896.3037 (MSE:0.0028, Reg:2896.3010) beta=13.25
Iter 11000 | Total loss: 1745.4481 (MSE:0.0030, Reg:1745.4451) beta=12.12
Iter 12000 | Total loss: 891.4338 (MSE:0.0033, Reg:891.4305) beta=11.00
Iter 13000 | Total loss: 372.6913 (MSE:0.0028, Reg:372.6885) beta=9.88
Iter 14000 | Total loss: 79.9183 (MSE:0.0030, Reg:79.9153) beta=8.75
Iter 15000 | Total loss: 5.0031 (MSE:0.0031, Reg:5.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[7/21] BRECQ computing: 1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 227196.0781 (MSE:0.0032, Reg:227196.0781) beta=20.00
Iter  5000 | Total loss: 18652.1367 (MSE:0.0030, Reg:18652.1328) beta=18.88
Iter  6000 | Total loss: 10262.0615 (MSE:0.0027, Reg:10262.0586) beta=17.75
Iter  7000 | Total loss: 6790.4990 (MSE:0.0030, Reg:6790.4961) beta=16.62
Iter  8000 | Total loss: 4741.5073 (MSE:0.0029, Reg:4741.5044) beta=15.50
Iter  9000 | Total loss: 3323.7473 (MSE:0.0032, Reg:3323.7441) beta=14.38
Iter 10000 | Total loss: 2203.6572 (MSE:0.0030, Reg:2203.6543) beta=13.25
Iter 11000 | Total loss: 1384.1505 (MSE:0.0030, Reg:1384.1475) beta=12.12
Iter 12000 | Total loss: 758.1072 (MSE:0.0029, Reg:758.1044) beta=11.00
Iter 13000 | Total loss: 335.8322 (MSE:0.0030, Reg:335.8292) beta=9.88
Iter 14000 | Total loss: 98.8690 (MSE:0.0029, Reg:98.8661) beta=8.75
Iter 15000 | Total loss: 13.0029 (MSE:0.0029, Reg:13.0000) beta=7.62
Iter 16000 | Total loss: 1.0031 (MSE:0.0031, Reg:1.0000) beta=6.50
Iter 17000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[8/21] BRECQ computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 602273.5000 (MSE:0.0029, Reg:602273.5000) beta=20.00
Iter  5000 | Total loss: 47277.9609 (MSE:0.0031, Reg:47277.9570) beta=18.88
Iter  6000 | Total loss: 22682.4883 (MSE:0.0031, Reg:22682.4844) beta=17.75
Iter  7000 | Total loss: 13622.5615 (MSE:0.0031, Reg:13622.5586) beta=16.62
Iter  8000 | Total loss: 8994.9668 (MSE:0.0029, Reg:8994.9639) beta=15.50
Iter  9000 | Total loss: 5979.6724 (MSE:0.0038, Reg:5979.6685) beta=14.38
Iter 10000 | Total loss: 3950.3904 (MSE:0.0031, Reg:3950.3872) beta=13.25
Iter 11000 | Total loss: 2406.7021 (MSE:0.0031, Reg:2406.6990) beta=12.12
Iter 12000 | Total loss: 1174.2327 (MSE:0.0030, Reg:1174.2297) beta=11.00
Iter 13000 | Total loss: 458.5180 (MSE:0.0031, Reg:458.5149) beta=9.88
Iter 14000 | Total loss: 106.2179 (MSE:0.0037, Reg:106.2142) beta=8.75
Iter 15000 | Total loss: 7.5447 (MSE:0.0031, Reg:7.5416) beta=7.62
Iter 16000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0033 (MSE:0.0033, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=2.00

[9/21] BRECQ computing: 1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2755 (MSE:0.2755, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2057 (MSE:0.2057, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2242 (MSE:0.2242, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2085 (MSE:0.2085, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 514514.2500 (MSE:0.1983, Reg:514514.0625) beta=20.00
Iter  5000 | Total loss: 89448.3906 (MSE:0.2211, Reg:89448.1719) beta=18.88
Iter  6000 | Total loss: 55708.1133 (MSE:0.2170, Reg:55707.8945) beta=17.75
Iter  7000 | Total loss: 34835.9180 (MSE:0.2074, Reg:34835.7109) beta=16.62
Iter  8000 | Total loss: 20638.7637 (MSE:0.1883, Reg:20638.5762) beta=15.50
Iter  9000 | Total loss: 11418.7119 (MSE:0.1918, Reg:11418.5205) beta=14.38
Iter 10000 | Total loss: 5568.4614 (MSE:0.2051, Reg:5568.2563) beta=13.25
Iter 11000 | Total loss: 2096.4224 (MSE:0.2088, Reg:2096.2136) beta=12.12
Iter 12000 | Total loss: 588.2223 (MSE:0.1976, Reg:588.0247) beta=11.00
Iter 13000 | Total loss: 81.9432 (MSE:0.2058, Reg:81.7374) beta=9.88
Iter 14000 | Total loss: 6.1520 (MSE:0.1954, Reg:5.9565) beta=8.75
Iter 15000 | Total loss: 0.1821 (MSE:0.1821, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.1924 (MSE:0.1924, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2088 (MSE:0.2088, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.1976 (MSE:0.1976, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2083 (MSE:0.2083, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.1988 (MSE:0.1988, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
    2D search with INT4
Activation quantizer initialized from QuantLayer
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.9105 (MSE:0.9105, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4128 (MSE:0.4128, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2745 (MSE:0.2745, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.3037 (MSE:0.3037, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 37100.8320 (MSE:0.2628, Reg:37100.5703) beta=20.00
Iter  5000 | Total loss: 6795.7666 (MSE:0.2821, Reg:6795.4844) beta=18.88
Iter  6000 | Total loss: 4473.7837 (MSE:0.2761, Reg:4473.5078) beta=17.75
Iter  7000 | Total loss: 3088.9216 (MSE:0.2556, Reg:3088.6660) beta=16.62
Iter  8000 | Total loss: 2043.0018 (MSE:0.2962, Reg:2042.7057) beta=15.50
Iter  9000 | Total loss: 1249.2266 (MSE:0.2856, Reg:1248.9410) beta=14.38
Iter 10000 | Total loss: 700.3438 (MSE:0.2819, Reg:700.0619) beta=13.25
Iter 11000 | Total loss: 368.1223 (MSE:0.2762, Reg:367.8461) beta=12.12
Iter 12000 | Total loss: 148.6403 (MSE:0.3141, Reg:148.3261) beta=11.00
Iter 13000 | Total loss: 53.2783 (MSE:0.2785, Reg:52.9998) beta=9.88
Iter 14000 | Total loss: 12.7062 (MSE:0.2570, Reg:12.4492) beta=8.75
Iter 15000 | Total loss: 0.2958 (MSE:0.2958, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.2640 (MSE:0.2640, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2682 (MSE:0.2682, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2632 (MSE:0.2632, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2499 (MSE:0.2499, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.2666 (MSE:0.2666, Reg:0.0000) beta=2.00
BRECQ values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 47.006%
Total time: 994.06 sec
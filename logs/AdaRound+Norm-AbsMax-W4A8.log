Case: [ resnet18_AdaRoundQuantizer_CH_W8A8 ]
    - {'arch': 'resnet18', 'batch_size': 128, 'num_samples': 1024, 'fold': True, 'batch_size_AdaRound': 32, 'lr': 0.01}
    - weight params: {'scheme': 'AdaRoundQuantizer', 'per_channel': True, 'dstDtype': 'INT8', 'BaseScheme': 'NormQuantizer'}
    - activation params: {'scheme': 'AbsMaxQuantizer', 'dstDtype': 'INT8', 'per_channel': False}

Replace to QuantModule
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
    Conv2d <- BatchNorm2d    BN Folded!
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
p = 2.4
    Parent class is NormQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 20
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0001 (MSE:0.0001, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0036 (MSE:0.0036, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0030 (MSE:0.0030, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0002 (MSE:0.0002, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0023 (MSE:0.0023, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0027 (MSE:0.0027, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0025 (MSE:0.0025, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0026 (MSE:0.0026, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0075 (MSE:0.0075, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0066 (MSE:0.0066, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0072 (MSE:0.0072, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0071 (MSE:0.0071, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0065 (MSE:0.0065, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0062 (MSE:0.0062, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0070 (MSE:0.0070, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0053 (MSE:0.0053, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0055 (MSE:0.0055, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0051 (MSE:0.0051, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.2262 (MSE:0.2262, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.3030 (MSE:0.3030, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.3036 (MSE:0.3036, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2810 (MSE:0.2810, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.3219 (MSE:0.3219, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.3129 (MSE:0.3129, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.3233 (MSE:0.3233, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.3132 (MSE:0.3132, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.2879 (MSE:0.2879, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.3023 (MSE:0.3023, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.2937 (MSE:0.2937, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.2840 (MSE:0.2840, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.2997 (MSE:0.2997, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.3182 (MSE:0.3182, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.2660 (MSE:0.2660, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.2946 (MSE:0.2946, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.3170 (MSE:0.3170, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.2856 (MSE:0.2856, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.2976 (MSE:0.2976, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.3138 (MSE:0.3138, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3054 (MSE:0.3054, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
) 20000
Iter     1 | Total loss: 0.1910 (MSE:0.1910, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2340 (MSE:0.2340, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.3374 (MSE:0.3374, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4036 (MSE:0.4036, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 0.5098 (MSE:0.5098, Reg:0.0000) beta=20.00
Iter  5000 | Total loss: 0.5520 (MSE:0.5520, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.5305 (MSE:0.5305, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.5453 (MSE:0.5453, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.5999 (MSE:0.5999, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.5721 (MSE:0.5721, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.5382 (MSE:0.5382, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.5365 (MSE:0.5365, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.5607 (MSE:0.5607, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.5615 (MSE:0.5615, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.5506 (MSE:0.5506, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.5923 (MSE:0.5923, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.5559 (MSE:0.5559, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.5533 (MSE:0.5533, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.5644 (MSE:0.5644, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.5781 (MSE:0.5781, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.5834 (MSE:0.5834, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,    Quantized model Evaluation accuracy on 50000 images, 65.212%
Total time: 1196.20 sec

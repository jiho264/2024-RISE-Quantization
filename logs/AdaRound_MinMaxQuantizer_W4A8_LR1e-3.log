
Case: [ resnet18_AdaRound_MinMaxQuantizer_CH_W4A8_AdaRoundLR0.001 ]
    - arch: resnet18
    - batch_size: 128
    - num_samples: 1024
    - folding: False
    - batch_size_AdaRound: 32
    - lr: 0.001

- weight params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT4
    - per_channel: True
    - AdaRound: True

- activation params:
    - scheme: MinMaxQuantizer
    - dstDtype: INT8
    - per_channel: False

Replace to QuantModule
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
    ReLU merged
    Parent class is MinMaxQuantizer
    Initiated the V
    Parent class is MinMaxQuantizer
    Initiated the V
Qparams computing done!
    QuantModule: conv1, torch.Size([64, 3, 7, 7])
    QuantModule: layer1.0.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.0.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv1, torch.Size([64, 64, 3, 3])
    QuantModule: layer1.1.conv2, torch.Size([64, 64, 3, 3])
    QuantModule: layer2.0.conv1, torch.Size([128, 64, 3, 3])
    QuantModule: layer2.0.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.0.downsample.0, torch.Size([128, 64, 1, 1])
    QuantModule: layer2.1.conv1, torch.Size([128, 128, 3, 3])
    QuantModule: layer2.1.conv2, torch.Size([128, 128, 3, 3])
    QuantModule: layer3.0.conv1, torch.Size([256, 128, 3, 3])
    QuantModule: layer3.0.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.0.downsample.0, torch.Size([256, 128, 1, 1])
    QuantModule: layer3.1.conv1, torch.Size([256, 256, 3, 3])
    QuantModule: layer3.1.conv2, torch.Size([256, 256, 3, 3])
    QuantModule: layer4.0.conv1, torch.Size([512, 256, 3, 3])
    QuantModule: layer4.0.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.0.downsample.0, torch.Size([512, 256, 1, 1])
    QuantModule: layer4.1.conv1, torch.Size([512, 512, 3, 3])
    QuantModule: layer4.1.conv2, torch.Size([512, 512, 3, 3])
    QuantModule: fc, torch.Size([1000, 512])
Total QuantModule: 21, Folded BN layers : 0
........ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 112, 112])
................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
........................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 64, 56, 56])
................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 128, 28, 28])
........................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
........................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 256, 14, 14])
................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 512, 7, 7])
........................................................................................................................................................................ <- Dots indicate the Original FP inference.
   FP_OUTPUTS shape torch.Size([1024, 1000])

[1/21] AdaRound computing: conv1
 <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0029 (MSE:0.0029, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 2026.0894 (MSE:0.0008, Reg:2026.0886) beta=20.00
Iter  5000 | Total loss: 9.6903 (MSE:0.0017, Reg:9.6886) beta=18.88
Iter  6000 | Total loss: 5.0023 (MSE:0.0023, Reg:5.0000) beta=17.75
Iter  7000 | Total loss: 5.0026 (MSE:0.0026, Reg:5.0000) beta=16.62
Iter  8000 | Total loss: 4.0018 (MSE:0.0018, Reg:4.0000) beta=15.50
Iter  9000 | Total loss: 2.0017 (MSE:0.0017, Reg:2.0000) beta=14.38
Iter 10000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0034 (MSE:0.0034, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0016 (MSE:0.0016, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0020 (MSE:0.0020, Reg:0.0000) beta=2.00

[2/21] AdaRound computing: conv1
,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0003 (MSE:0.0003, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 9797.2832 (MSE:0.0003, Reg:9797.2832) beta=20.00
Iter  5000 | Total loss: 540.2131 (MSE:0.0008, Reg:540.2123) beta=18.88
Iter  6000 | Total loss: 236.2995 (MSE:0.0010, Reg:236.2985) beta=17.75
Iter  7000 | Total loss: 150.6678 (MSE:0.0010, Reg:150.6669) beta=16.62
Iter  8000 | Total loss: 117.4289 (MSE:0.0008, Reg:117.4282) beta=15.50
Iter  9000 | Total loss: 89.2235 (MSE:0.0007, Reg:89.2228) beta=14.38
Iter 10000 | Total loss: 72.4205 (MSE:0.0009, Reg:72.4197) beta=13.25
Iter 11000 | Total loss: 34.4416 (MSE:0.0011, Reg:34.4405) beta=12.12
Iter 12000 | Total loss: 23.7699 (MSE:0.0007, Reg:23.7692) beta=11.00
Iter 13000 | Total loss: 14.5494 (MSE:0.0008, Reg:14.5486) beta=9.88
Iter 14000 | Total loss: 8.1289 (MSE:0.0012, Reg:8.1277) beta=8.75
Iter 15000 | Total loss: 4.3261 (MSE:0.0013, Reg:4.3248) beta=7.62
Iter 16000 | Total loss: 1.9081 (MSE:0.0009, Reg:1.9073) beta=6.50
Iter 17000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=2.00

[3/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0039 (MSE:0.0039, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0031 (MSE:0.0031, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0022 (MSE:0.0022, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 13521.5645 (MSE:0.0015, Reg:13521.5625) beta=20.00
Iter  5000 | Total loss: 2628.3997 (MSE:0.0029, Reg:2628.3967) beta=18.88
Iter  6000 | Total loss: 1611.7404 (MSE:0.0021, Reg:1611.7383) beta=17.75
Iter  7000 | Total loss: 1163.6565 (MSE:0.0019, Reg:1163.6545) beta=16.62
Iter  8000 | Total loss: 841.9818 (MSE:0.0018, Reg:841.9800) beta=15.50
Iter  9000 | Total loss: 628.8499 (MSE:0.0019, Reg:628.8481) beta=14.38
Iter 10000 | Total loss: 472.5756 (MSE:0.0026, Reg:472.5730) beta=13.25
Iter 11000 | Total loss: 337.3268 (MSE:0.0021, Reg:337.3247) beta=12.12
Iter 12000 | Total loss: 229.5013 (MSE:0.0018, Reg:229.4995) beta=11.00
Iter 13000 | Total loss: 143.7323 (MSE:0.0018, Reg:143.7304) beta=9.88
Iter 14000 | Total loss: 78.8070 (MSE:0.0019, Reg:78.8050) beta=8.75
Iter 15000 | Total loss: 37.5030 (MSE:0.0026, Reg:37.5004) beta=7.62
Iter 16000 | Total loss: 13.2275 (MSE:0.0032, Reg:13.2243) beta=6.50
Iter 17000 | Total loss: 3.8089 (MSE:0.0023, Reg:3.8066) beta=5.38
Iter 18000 | Total loss: 0.0045 (MSE:0.0045, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=2.00

[4/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 14767.9814 (MSE:0.0008, Reg:14767.9805) beta=20.00
Iter  5000 | Total loss: 1416.3145 (MSE:0.0011, Reg:1416.3134) beta=18.88
Iter  6000 | Total loss: 625.8881 (MSE:0.0009, Reg:625.8872) beta=17.75
Iter  7000 | Total loss: 398.6459 (MSE:0.0009, Reg:398.6450) beta=16.62
Iter  8000 | Total loss: 292.3921 (MSE:0.0010, Reg:292.3911) beta=15.50
Iter  9000 | Total loss: 233.4231 (MSE:0.0008, Reg:233.4223) beta=14.38
Iter 10000 | Total loss: 194.8532 (MSE:0.0008, Reg:194.8524) beta=13.25
Iter 11000 | Total loss: 148.9959 (MSE:0.0008, Reg:148.9951) beta=12.12
Iter 12000 | Total loss: 117.7879 (MSE:0.0010, Reg:117.7870) beta=11.00
Iter 13000 | Total loss: 90.9395 (MSE:0.0009, Reg:90.9386) beta=9.88
Iter 14000 | Total loss: 57.9708 (MSE:0.0009, Reg:57.9699) beta=8.75
Iter 15000 | Total loss: 28.2730 (MSE:0.0008, Reg:28.2722) beta=7.62
Iter 16000 | Total loss: 15.6885 (MSE:0.0008, Reg:15.6877) beta=6.50
Iter 17000 | Total loss: 4.8654 (MSE:0.0008, Reg:4.8646) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=2.00

[5/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0085 (MSE:0.0085, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0081 (MSE:0.0081, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0054 (MSE:0.0054, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 17306.9023 (MSE:0.0049, Reg:17306.8984) beta=20.00
Iter  5000 | Total loss: 4363.6948 (MSE:0.0063, Reg:4363.6885) beta=18.88
Iter  6000 | Total loss: 2840.9531 (MSE:0.0059, Reg:2840.9473) beta=17.75
Iter  7000 | Total loss: 2076.8665 (MSE:0.0062, Reg:2076.8604) beta=16.62
Iter  8000 | Total loss: 1597.9012 (MSE:0.0057, Reg:1597.8955) beta=15.50
Iter  9000 | Total loss: 1194.1709 (MSE:0.0055, Reg:1194.1654) beta=14.38
Iter 10000 | Total loss: 900.5884 (MSE:0.0056, Reg:900.5828) beta=13.25
Iter 11000 | Total loss: 638.3710 (MSE:0.0059, Reg:638.3651) beta=12.12
Iter 12000 | Total loss: 428.7646 (MSE:0.0061, Reg:428.7585) beta=11.00
Iter 13000 | Total loss: 270.9735 (MSE:0.0055, Reg:270.9680) beta=9.88
Iter 14000 | Total loss: 149.2798 (MSE:0.0065, Reg:149.2733) beta=8.75
Iter 15000 | Total loss: 75.2688 (MSE:0.0060, Reg:75.2628) beta=7.62
Iter 16000 | Total loss: 29.5870 (MSE:0.0059, Reg:29.5810) beta=6.50
Iter 17000 | Total loss: 6.8268 (MSE:0.0059, Reg:6.8209) beta=5.38
Iter 18000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0060 (MSE:0.0060, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0061 (MSE:0.0061, Reg:0.0000) beta=2.00

[6/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0012 (MSE:0.0012, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 34635.9062 (MSE:0.0009, Reg:34635.9062) beta=20.00
Iter  5000 | Total loss: 2158.0200 (MSE:0.0011, Reg:2158.0190) beta=18.88
Iter  6000 | Total loss: 961.6369 (MSE:0.0011, Reg:961.6359) beta=17.75
Iter  7000 | Total loss: 588.8231 (MSE:0.0010, Reg:588.8220) beta=16.62
Iter  8000 | Total loss: 427.9052 (MSE:0.0011, Reg:427.9042) beta=15.50
Iter  9000 | Total loss: 338.3553 (MSE:0.0010, Reg:338.3543) beta=14.38
Iter 10000 | Total loss: 262.9920 (MSE:0.0010, Reg:262.9910) beta=13.25
Iter 11000 | Total loss: 197.9157 (MSE:0.0011, Reg:197.9146) beta=12.12
Iter 12000 | Total loss: 153.9813 (MSE:0.0012, Reg:153.9801) beta=11.00
Iter 13000 | Total loss: 115.5647 (MSE:0.0012, Reg:115.5635) beta=9.88
Iter 14000 | Total loss: 72.2851 (MSE:0.0010, Reg:72.2841) beta=8.75
Iter 15000 | Total loss: 44.2565 (MSE:0.0011, Reg:44.2554) beta=7.62
Iter 16000 | Total loss: 19.5615 (MSE:0.0011, Reg:19.5604) beta=6.50
Iter 17000 | Total loss: 2.6844 (MSE:0.0010, Reg:2.6833) beta=5.38
Iter 18000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=2.00

[7/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0035 (MSE:0.0035, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0032 (MSE:0.0032, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 78660.0078 (MSE:0.0034, Reg:78660.0078) beta=20.00
Iter  5000 | Total loss: 8518.1885 (MSE:0.0041, Reg:8518.1846) beta=18.88
Iter  6000 | Total loss: 4464.2012 (MSE:0.0049, Reg:4464.1963) beta=17.75
Iter  7000 | Total loss: 3099.3789 (MSE:0.0051, Reg:3099.3738) beta=16.62
Iter  8000 | Total loss: 2358.2083 (MSE:0.0046, Reg:2358.2036) beta=15.50
Iter  9000 | Total loss: 1906.9384 (MSE:0.0056, Reg:1906.9327) beta=14.38
Iter 10000 | Total loss: 1544.9415 (MSE:0.0056, Reg:1544.9359) beta=13.25
Iter 11000 | Total loss: 1226.9902 (MSE:0.0044, Reg:1226.9858) beta=12.12
Iter 12000 | Total loss: 918.7366 (MSE:0.0043, Reg:918.7322) beta=11.00
Iter 13000 | Total loss: 611.3560 (MSE:0.0048, Reg:611.3512) beta=9.88
Iter 14000 | Total loss: 383.1297 (MSE:0.0043, Reg:383.1254) beta=8.75
Iter 15000 | Total loss: 191.7276 (MSE:0.0040, Reg:191.7236) beta=7.62
Iter 16000 | Total loss: 77.7892 (MSE:0.0041, Reg:77.7851) beta=6.50
Iter 17000 | Total loss: 17.2319 (MSE:0.0048, Reg:17.2271) beta=5.38
Iter 18000 | Total loss: 0.1550 (MSE:0.0050, Reg:0.1500) beta=4.25
Iter 19000 | Total loss: 0.0041 (MSE:0.0041, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=2.00

[8/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0024 (MSE:0.0024, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0019 (MSE:0.0019, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0018 (MSE:0.0018, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 5333.7241 (MSE:0.0013, Reg:5333.7227) beta=20.00
Iter  5000 | Total loss: 1440.2595 (MSE:0.0017, Reg:1440.2578) beta=18.88
Iter  6000 | Total loss: 1076.4580 (MSE:0.0019, Reg:1076.4561) beta=17.75
Iter  7000 | Total loss: 862.7480 (MSE:0.0016, Reg:862.7465) beta=16.62
Iter  8000 | Total loss: 739.8708 (MSE:0.0016, Reg:739.8691) beta=15.50
Iter  9000 | Total loss: 603.6983 (MSE:0.0017, Reg:603.6967) beta=14.38
Iter 10000 | Total loss: 484.6381 (MSE:0.0025, Reg:484.6356) beta=13.25
Iter 11000 | Total loss: 382.0633 (MSE:0.0025, Reg:382.0607) beta=12.12
Iter 12000 | Total loss: 280.7036 (MSE:0.0017, Reg:280.7019) beta=11.00
Iter 13000 | Total loss: 200.3083 (MSE:0.0017, Reg:200.3066) beta=9.88
Iter 14000 | Total loss: 125.7628 (MSE:0.0018, Reg:125.7610) beta=8.75
Iter 15000 | Total loss: 63.5433 (MSE:0.0018, Reg:63.5416) beta=7.62
Iter 16000 | Total loss: 19.5048 (MSE:0.0018, Reg:19.5030) beta=6.50
Iter 17000 | Total loss: 3.5966 (MSE:0.0018, Reg:3.5947) beta=5.38
Iter 18000 | Total loss: 0.0017 (MSE:0.0017, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0021 (MSE:0.0021, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0028 (MSE:0.0028, Reg:0.0000) beta=2.00

[9/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 60558.0547 (MSE:0.0007, Reg:60558.0547) beta=20.00
Iter  5000 | Total loss: 1124.1621 (MSE:0.0010, Reg:1124.1611) beta=18.88
Iter  6000 | Total loss: 406.0035 (MSE:0.0009, Reg:406.0025) beta=17.75
Iter  7000 | Total loss: 228.8029 (MSE:0.0010, Reg:228.8019) beta=16.62
Iter  8000 | Total loss: 159.0514 (MSE:0.0010, Reg:159.0503) beta=15.50
Iter  9000 | Total loss: 118.9506 (MSE:0.0009, Reg:118.9497) beta=14.38
Iter 10000 | Total loss: 91.6433 (MSE:0.0010, Reg:91.6423) beta=13.25
Iter 11000 | Total loss: 70.1370 (MSE:0.0010, Reg:70.1361) beta=12.12
Iter 12000 | Total loss: 54.3604 (MSE:0.0010, Reg:54.3594) beta=11.00
Iter 13000 | Total loss: 39.8515 (MSE:0.0009, Reg:39.8505) beta=9.88
Iter 14000 | Total loss: 27.2044 (MSE:0.0009, Reg:27.2034) beta=8.75
Iter 15000 | Total loss: 20.4877 (MSE:0.0009, Reg:20.4867) beta=7.62
Iter 16000 | Total loss: 14.2978 (MSE:0.0010, Reg:14.2967) beta=6.50
Iter 17000 | Total loss: 3.9753 (MSE:0.0009, Reg:3.9744) beta=5.38
Iter 18000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=2.00

[10/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0058 (MSE:0.0058, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0050 (MSE:0.0050, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0043 (MSE:0.0043, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 71445.2812 (MSE:0.0041, Reg:71445.2734) beta=20.00
Iter  5000 | Total loss: 7563.9795 (MSE:0.0043, Reg:7563.9751) beta=18.88
Iter  6000 | Total loss: 4351.0283 (MSE:0.0044, Reg:4351.0239) beta=17.75
Iter  7000 | Total loss: 3015.1624 (MSE:0.0045, Reg:3015.1580) beta=16.62
Iter  8000 | Total loss: 2377.3726 (MSE:0.0043, Reg:2377.3682) beta=15.50
Iter  9000 | Total loss: 1891.0405 (MSE:0.0046, Reg:1891.0359) beta=14.38
Iter 10000 | Total loss: 1480.1329 (MSE:0.0044, Reg:1480.1285) beta=13.25
Iter 11000 | Total loss: 1140.1289 (MSE:0.0047, Reg:1140.1241) beta=12.12
Iter 12000 | Total loss: 877.8523 (MSE:0.0045, Reg:877.8478) beta=11.00
Iter 13000 | Total loss: 616.3288 (MSE:0.0046, Reg:616.3242) beta=9.88
Iter 14000 | Total loss: 379.7339 (MSE:0.0045, Reg:379.7294) beta=8.75
Iter 15000 | Total loss: 187.5705 (MSE:0.0047, Reg:187.5657) beta=7.62
Iter 16000 | Total loss: 73.4371 (MSE:0.0049, Reg:73.4323) beta=6.50
Iter 17000 | Total loss: 18.6802 (MSE:0.0046, Reg:18.6756) beta=5.38
Iter 18000 | Total loss: 0.3475 (MSE:0.0047, Reg:0.3428) beta=4.25
Iter 19000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=2.00

[11/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0013 (MSE:0.0013, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 107941.7500 (MSE:0.0013, Reg:107941.7500) beta=20.00
Iter  5000 | Total loss: 347.8155 (MSE:0.0014, Reg:347.8141) beta=18.88
Iter  6000 | Total loss: 89.4907 (MSE:0.0015, Reg:89.4892) beta=17.75
Iter  7000 | Total loss: 25.8129 (MSE:0.0017, Reg:25.8112) beta=16.62
Iter  8000 | Total loss: 15.0014 (MSE:0.0014, Reg:15.0000) beta=15.50
Iter  9000 | Total loss: 11.6367 (MSE:0.0015, Reg:11.6353) beta=14.38
Iter 10000 | Total loss: 7.7380 (MSE:0.0014, Reg:7.7366) beta=13.25
Iter 11000 | Total loss: 6.2605 (MSE:0.0014, Reg:6.2591) beta=12.12
Iter 12000 | Total loss: 5.0015 (MSE:0.0015, Reg:5.0000) beta=11.00
Iter 13000 | Total loss: 3.7138 (MSE:0.0015, Reg:3.7123) beta=9.88
Iter 14000 | Total loss: 2.0013 (MSE:0.0013, Reg:2.0000) beta=8.75
Iter 15000 | Total loss: 2.0014 (MSE:0.0014, Reg:2.0000) beta=7.62
Iter 16000 | Total loss: 2.0014 (MSE:0.0014, Reg:2.0000) beta=6.50
Iter 17000 | Total loss: 0.5754 (MSE:0.0014, Reg:0.5740) beta=5.38
Iter 18000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0014 (MSE:0.0014, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0015 (MSE:0.0015, Reg:0.0000) beta=2.00

[12/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0063 (MSE:0.0063, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0048 (MSE:0.0048, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 230420.0312 (MSE:0.0041, Reg:230420.0312) beta=20.00
Iter  5000 | Total loss: 6189.1626 (MSE:0.0049, Reg:6189.1577) beta=18.88
Iter  6000 | Total loss: 2765.2173 (MSE:0.0049, Reg:2765.2124) beta=17.75
Iter  7000 | Total loss: 1710.2144 (MSE:0.0051, Reg:1710.2094) beta=16.62
Iter  8000 | Total loss: 1255.6964 (MSE:0.0050, Reg:1255.6914) beta=15.50
Iter  9000 | Total loss: 962.8554 (MSE:0.0056, Reg:962.8498) beta=14.38
Iter 10000 | Total loss: 745.1357 (MSE:0.0054, Reg:745.1302) beta=13.25
Iter 11000 | Total loss: 593.3735 (MSE:0.0051, Reg:593.3684) beta=12.12
Iter 12000 | Total loss: 481.2010 (MSE:0.0052, Reg:481.1958) beta=11.00
Iter 13000 | Total loss: 353.8767 (MSE:0.0050, Reg:353.8717) beta=9.88
Iter 14000 | Total loss: 243.2231 (MSE:0.0049, Reg:243.2182) beta=8.75
Iter 15000 | Total loss: 148.0503 (MSE:0.0052, Reg:148.0452) beta=7.62
Iter 16000 | Total loss: 71.3855 (MSE:0.0052, Reg:71.3802) beta=6.50
Iter 17000 | Total loss: 17.4079 (MSE:0.0059, Reg:17.4020) beta=5.38
Iter 18000 | Total loss: 0.3718 (MSE:0.0051, Reg:0.3667) beta=4.25
Iter 19000 | Total loss: 0.0057 (MSE:0.0057, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0056 (MSE:0.0056, Reg:0.0000) beta=2.00

[13/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 17348.8418 (MSE:0.0004, Reg:17348.8418) beta=20.00
Iter  5000 | Total loss: 727.0974 (MSE:0.0004, Reg:727.0969) beta=18.88
Iter  6000 | Total loss: 531.8651 (MSE:0.0004, Reg:531.8646) beta=17.75
Iter  7000 | Total loss: 346.9061 (MSE:0.0004, Reg:346.9056) beta=16.62
Iter  8000 | Total loss: 278.7088 (MSE:0.0004, Reg:278.7084) beta=15.50
Iter  9000 | Total loss: 229.1761 (MSE:0.0004, Reg:229.1757) beta=14.38
Iter 10000 | Total loss: 184.0079 (MSE:0.0005, Reg:184.0074) beta=13.25
Iter 11000 | Total loss: 146.6300 (MSE:0.0004, Reg:146.6296) beta=12.12
Iter 12000 | Total loss: 108.3542 (MSE:0.0004, Reg:108.3537) beta=11.00
Iter 13000 | Total loss: 77.0518 (MSE:0.0005, Reg:77.0513) beta=9.88
Iter 14000 | Total loss: 41.7555 (MSE:0.0004, Reg:41.7551) beta=8.75
Iter 15000 | Total loss: 25.5793 (MSE:0.0005, Reg:25.5788) beta=7.62
Iter 16000 | Total loss: 11.5769 (MSE:0.0004, Reg:11.5765) beta=6.50
Iter 17000 | Total loss: 3.0004 (MSE:0.0004, Reg:3.0000) beta=5.38
Iter 18000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0004 (MSE:0.0004, Reg:0.0000) beta=2.00

[14/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0005 (MSE:0.0005, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 119974.2422 (MSE:0.0005, Reg:119974.2422) beta=20.00
Iter  5000 | Total loss: 19.4108 (MSE:0.0006, Reg:19.4102) beta=18.88
Iter  6000 | Total loss: 0.8942 (MSE:0.0006, Reg:0.8936) beta=17.75
Iter  7000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[15/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 187220.5781 (MSE:0.0041, Reg:187220.5781) beta=20.00
Iter  5000 | Total loss: 4097.2505 (MSE:0.0045, Reg:4097.2461) beta=18.88
Iter  6000 | Total loss: 1826.5337 (MSE:0.0046, Reg:1826.5291) beta=17.75
Iter  7000 | Total loss: 1047.1121 (MSE:0.0044, Reg:1047.1077) beta=16.62
Iter  8000 | Total loss: 750.0391 (MSE:0.0046, Reg:750.0345) beta=15.50
Iter  9000 | Total loss: 587.6794 (MSE:0.0045, Reg:587.6749) beta=14.38
Iter 10000 | Total loss: 475.2576 (MSE:0.0045, Reg:475.2531) beta=13.25
Iter 11000 | Total loss: 365.5741 (MSE:0.0047, Reg:365.5694) beta=12.12
Iter 12000 | Total loss: 273.4769 (MSE:0.0044, Reg:273.4725) beta=11.00
Iter 13000 | Total loss: 194.8172 (MSE:0.0046, Reg:194.8126) beta=9.88
Iter 14000 | Total loss: 121.7935 (MSE:0.0048, Reg:121.7886) beta=8.75
Iter 15000 | Total loss: 71.7975 (MSE:0.0048, Reg:71.7927) beta=7.62
Iter 16000 | Total loss: 19.8330 (MSE:0.0046, Reg:19.8284) beta=6.50
Iter 17000 | Total loss: 2.0605 (MSE:0.0047, Reg:2.0558) beta=5.38
Iter 18000 | Total loss: 0.0044 (MSE:0.0044, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0049 (MSE:0.0049, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0046 (MSE:0.0046, Reg:0.0000) beta=2.00

[16/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0006 (MSE:0.0006, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 122078.9219 (MSE:0.0007, Reg:122078.9219) beta=20.00
Iter  5000 | Total loss: 14.2636 (MSE:0.0007, Reg:14.2629) beta=18.88
Iter  6000 | Total loss: 1.9100 (MSE:0.0007, Reg:1.9093) beta=17.75
Iter  7000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0008 (MSE:0.0008, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0007 (MSE:0.0007, Reg:0.0000) beta=2.00

[17/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0118 (MSE:0.0118, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0131 (MSE:0.0131, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0115 (MSE:0.0115, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 677109.0000 (MSE:0.0112, Reg:677109.0000) beta=20.00
Iter  5000 | Total loss: 4496.5327 (MSE:0.0133, Reg:4496.5195) beta=18.88
Iter  6000 | Total loss: 1538.1809 (MSE:0.0123, Reg:1538.1687) beta=17.75
Iter  7000 | Total loss: 624.3457 (MSE:0.0134, Reg:624.3323) beta=16.62
Iter  8000 | Total loss: 399.6848 (MSE:0.0148, Reg:399.6700) beta=15.50
Iter  9000 | Total loss: 286.6308 (MSE:0.0144, Reg:286.6164) beta=14.38
Iter 10000 | Total loss: 213.3660 (MSE:0.0129, Reg:213.3531) beta=13.25
Iter 11000 | Total loss: 160.0102 (MSE:0.0140, Reg:159.9962) beta=12.12
Iter 12000 | Total loss: 108.3894 (MSE:0.0127, Reg:108.3766) beta=11.00
Iter 13000 | Total loss: 73.6834 (MSE:0.0131, Reg:73.6703) beta=9.88
Iter 14000 | Total loss: 49.0661 (MSE:0.0134, Reg:49.0527) beta=8.75
Iter 15000 | Total loss: 29.6044 (MSE:0.0131, Reg:29.5914) beta=7.62
Iter 16000 | Total loss: 10.6054 (MSE:0.0140, Reg:10.5914) beta=6.50
Iter 17000 | Total loss: 3.0908 (MSE:0.0124, Reg:3.0784) beta=5.38
Iter 18000 | Total loss: 0.0126 (MSE:0.0126, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0135 (MSE:0.0135, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0123 (MSE:0.0123, Reg:0.0000) beta=2.00

[18/21] AdaRound computing: 0
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0042 (MSE:0.0042, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0037 (MSE:0.0037, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0038 (MSE:0.0038, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 63294.9727 (MSE:0.0032, Reg:63294.9688) beta=20.00
Iter  5000 | Total loss: 6612.3584 (MSE:0.0040, Reg:6612.3545) beta=18.88
Iter  6000 | Total loss: 4632.6309 (MSE:0.0038, Reg:4632.6270) beta=17.75
Iter  7000 | Total loss: 3264.1489 (MSE:0.0047, Reg:3264.1443) beta=16.62
Iter  8000 | Total loss: 2541.3591 (MSE:0.0040, Reg:2541.3552) beta=15.50
Iter  9000 | Total loss: 2078.9946 (MSE:0.0040, Reg:2078.9907) beta=14.38
Iter 10000 | Total loss: 1730.7386 (MSE:0.0045, Reg:1730.7341) beta=13.25
Iter 11000 | Total loss: 1391.2357 (MSE:0.0039, Reg:1391.2318) beta=12.12
Iter 12000 | Total loss: 1086.9360 (MSE:0.0041, Reg:1086.9319) beta=11.00
Iter 13000 | Total loss: 778.3714 (MSE:0.0041, Reg:778.3673) beta=9.88
Iter 14000 | Total loss: 495.3007 (MSE:0.0040, Reg:495.2967) beta=8.75
Iter 15000 | Total loss: 266.9683 (MSE:0.0041, Reg:266.9642) beta=7.62
Iter 16000 | Total loss: 107.9548 (MSE:0.0041, Reg:107.9507) beta=6.50
Iter 17000 | Total loss: 20.4732 (MSE:0.0040, Reg:20.4692) beta=5.38
Iter 18000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0040 (MSE:0.0040, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0052 (MSE:0.0052, Reg:0.0000) beta=2.00

[19/21] AdaRound computing: conv1
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 147269.0312 (MSE:0.0010, Reg:147269.0312) beta=20.00
Iter  5000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=18.88
Iter  6000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=17.75
Iter  7000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=16.62
Iter  8000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=15.50
Iter  9000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=14.38
Iter 10000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=13.25
Iter 11000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=12.12
Iter 12000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=11.00
Iter 13000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=9.88
Iter 14000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=8.75
Iter 15000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=7.62
Iter 16000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=6.50
Iter 17000 | Total loss: 0.0011 (MSE:0.0011, Reg:0.0000) beta=5.38
Iter 18000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.0009 (MSE:0.0009, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.0010 (MSE:0.0010, Reg:0.0000) beta=2.00

[20/21] AdaRound computing: conv2
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.4230 (MSE:0.4230, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.4425 (MSE:0.4425, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.4193 (MSE:0.4193, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.4024 (MSE:0.4024, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 550624.9375 (MSE:0.4135, Reg:550624.5000) beta=20.00
Iter  5000 | Total loss: 56264.9688 (MSE:0.4313, Reg:56264.5391) beta=18.88
Iter  6000 | Total loss: 42471.0859 (MSE:0.4377, Reg:42470.6484) beta=17.75
Iter  7000 | Total loss: 24512.9648 (MSE:0.4453, Reg:24512.5195) beta=16.62
Iter  8000 | Total loss: 14932.8252 (MSE:0.4326, Reg:14932.3926) beta=15.50
Iter  9000 | Total loss: 10360.9629 (MSE:0.4156, Reg:10360.5469) beta=14.38
Iter 10000 | Total loss: 7690.9609 (MSE:0.4053, Reg:7690.5557) beta=13.25
Iter 11000 | Total loss: 5910.3682 (MSE:0.4122, Reg:5909.9561) beta=12.12
Iter 12000 | Total loss: 4466.8389 (MSE:0.4171, Reg:4466.4219) beta=11.00
Iter 13000 | Total loss: 3245.6858 (MSE:0.4144, Reg:3245.2715) beta=9.88
Iter 14000 | Total loss: 2238.1079 (MSE:0.4045, Reg:2237.7034) beta=8.75
Iter 15000 | Total loss: 1374.2059 (MSE:0.4229, Reg:1373.7830) beta=7.62
Iter 16000 | Total loss: 688.3654 (MSE:0.4499, Reg:687.9154) beta=6.50
Iter 17000 | Total loss: 203.1809 (MSE:0.4083, Reg:202.7727) beta=5.38
Iter 18000 | Total loss: 5.9387 (MSE:0.3954, Reg:5.5433) beta=4.25
Iter 19000 | Total loss: 0.4006 (MSE:0.4006, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.4080 (MSE:0.4080, Reg:0.0000) beta=2.00

[21/21] AdaRound computing: fc
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, <- Commas indicate the INT inference.
Activation quantizer initialized
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
) 20000
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Iter     1 | Total loss: 0.3529 (MSE:0.3529, Reg:0.0000) beta=0.00
Iter  1000 | Total loss: 0.2410 (MSE:0.2410, Reg:0.0000) beta=0.00
Iter  2000 | Total loss: 0.2253 (MSE:0.2253, Reg:0.0000) beta=0.00
Iter  3000 | Total loss: 0.2207 (MSE:0.2207, Reg:0.0000) beta=0.00
Iter  4000 | Total loss: 108827.9688 (MSE:0.2326, Reg:108827.7344) beta=20.00
Iter  5000 | Total loss: 1371.9338 (MSE:0.2907, Reg:1371.6431) beta=18.88
Iter  6000 | Total loss: 472.9541 (MSE:0.3024, Reg:472.6517) beta=17.75
Iter  7000 | Total loss: 326.4185 (MSE:0.3084, Reg:326.1101) beta=16.62
Iter  8000 | Total loss: 232.7961 (MSE:0.3006, Reg:232.4955) beta=15.50
Iter  9000 | Total loss: 177.6613 (MSE:0.3121, Reg:177.3492) beta=14.38
Iter 10000 | Total loss: 149.3187 (MSE:0.3191, Reg:148.9996) beta=13.25
Iter 11000 | Total loss: 127.7054 (MSE:0.2833, Reg:127.4220) beta=12.12
Iter 12000 | Total loss: 103.0927 (MSE:0.2906, Reg:102.8021) beta=11.00
Iter 13000 | Total loss: 88.1704 (MSE:0.3073, Reg:87.8631) beta=9.88
Iter 14000 | Total loss: 57.8584 (MSE:0.2919, Reg:57.5665) beta=8.75
Iter 15000 | Total loss: 38.1325 (MSE:0.3045, Reg:37.8280) beta=7.62
Iter 16000 | Total loss: 18.9243 (MSE:0.3084, Reg:18.6158) beta=6.50
Iter 17000 | Total loss: 6.3067 (MSE:0.3309, Reg:5.9759) beta=5.38
Iter 18000 | Total loss: 0.3048 (MSE:0.3048, Reg:0.0000) beta=4.25
Iter 19000 | Total loss: 0.2965 (MSE:0.2965, Reg:0.0000) beta=3.12
Iter 20000 | Total loss: 0.3141 (MSE:0.3141, Reg:0.0000) beta=2.00
AdaRound values computing done!
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    Quantized model Evaluation accuracy on 50000 images, 68.438%
Total time: 1301.60 sec

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.utils import *\n",
    "from src.override_resnet import *\n",
    "\n",
    "\n",
    "class Args:\n",
    "    arch = 50\n",
    "    dataset = \"ImageNet\"\n",
    "    # dataset = \"CIFAR100\"\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    batch = 16\n",
    "    epochs = 10\n",
    "    save_every = 1\n",
    "    quan = \"qat\"\n",
    "    only_eval = True\n",
    "    verbose = True\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, img_loader):\n",
    "    elapsed = 0\n",
    "    model.eval()\n",
    "    num_batches = 1\n",
    "    # 이미지 배치들 이용하여 스크립트된 모델 실행\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end - start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print(\"Elapsed time: %3.0f ms\" % (elapsed / num_images * 1000))\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_model(model) -> nn.Module:\n",
    "    flag = False\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__ == ResNet_quan.__name__:\n",
    "            if flag == True:\n",
    "                raise ValueError(\"ResNet_quan is already fused\")\n",
    "            flag = True\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\"conv1\", \"bn1\", \"relu\"],\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        if type(m) == BottleNeck_quan:\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\n",
    "                    [\"conv1\", \"bn1\", \"relu1\"],\n",
    "                    [\"conv2\", \"bn2\", \"relu2\"],\n",
    "                    [\"conv3\", \"bn3\"],\n",
    "                ],\n",
    "                inplace=True,\n",
    "            )\n",
    "            if m.downsample is not None:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    m.downsample,\n",
    "                    [\"0\", \"1\"],\n",
    "                    inplace=True,\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Quantization Aware Training enabled\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# %% my code\n",
    "\n",
    "args = Args()\n",
    "# %% Load the ResNet-50 model\n",
    "if args.quan == \"fp32\":\n",
    "    # case 0 : no quantization case\n",
    "    print(\"----------No quantization enabled\")\n",
    "    device = str(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model = layers_mapping[args.arch](\n",
    "        weights=pretrained_weights_mapping[args.arch]\n",
    "    ).to(device)\n",
    "\n",
    "elif args.quan == \"dynamic\":\n",
    "    # case 1 : Dynamic Quantization\n",
    "    print(\"----------Dynamic Quantization enabled\")\n",
    "    device = \"cuda\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    model = quantized_model\n",
    "\n",
    "elif args.quan == \"static\":\n",
    "    # case 2 : Static Quantization\n",
    "    print(\"----------Static Quantization enabled\")\n",
    "    device = \"cpu\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "\n",
    "elif args.quan == \"qat\":\n",
    "    # case 3 : Quantization Aware Training\n",
    "    print(\"----------Quantization Aware Training enabled\")\n",
    "    device = \"cpu\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "else:\n",
    "    raise ValueError(\"Invalid quantization method\")\n",
    "\n",
    "_folder_path = f\"resnet{args.arch}_{args.dataset}\" + \"_\" + args.quan\n",
    "_file_name = (\n",
    "    f\"resnet{args.arch}_{args.dataset}_epoch\"  # resnet18_cifar10_epoch{epoch}.pth\n",
    ")\n",
    "\n",
    "# %%Set up training and evaluation processes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_loader, test_loader = GetDataset(\n",
    "    dataset_name=args.dataset,\n",
    "    device=device,\n",
    "    root=\"data\",\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순히 양자화 설정 방법을 변경하는 것만으로도 정확도가 67.3%를 넘을 정도로 향상이 되었습니다! 그럼에도 이 수치는 위에서 구한 기준값 71.9%에서 4퍼센트나 낮은 수치입니다. 이제 양자화 자각 학습을 시도해 봅시다.\n",
    "\n",
    "# 5. 양자화 자각 학습(Quantization-aware training)\n",
    "- 양자화 자각 학습(QAT)은 일반적으로 가장 높은 정확도를 제공하는 양자화 방법입니다. 모든 가중치화 활성값은 QAT로 인해 학습 도중에 순전파와 역전파를 도중 《가짜 양자화》됩니다. \n",
    "- 이는 float값이 int8 값으로 반올림하는 것처럼 흉내를 내지만, 모든 계산은 여전히 부동소수점 숫자로 계산을 합니다. 그래서 결국 훈련 동안의 모든 가중치 조정은 모델이 양자화될 것이라는 사실을 《자각》한 채로 이루어지게 됩니다. \n",
    "- 그래서 QAT는 양자화가 이루어지고 나면 동적 양자화나 학습 전 정적 양자화보다 대체로 더 높은 정확도를 보여줍니다.\n",
    "\n",
    "- 실제로 QAT가 이루어지는 전체 흐름은 이전과 매우 유사합니다:\n",
    "\n",
    "  - 이전과 같은 모델을 사용할 수 있습니다. 양자화 자각 학습을 위한 추가적인 준비는 필요 없습니다.\n",
    "\n",
    "  - 가중치와 활성값 뒤에 어떤 종류의 가짜 양자화를 사용할 것인지 명시하는 qconfig 의 사용이 필요합니다. Observer를 명시하는 것 대신에 말이죠.\n",
    "\n",
    "먼저 학습 함수부터 정의합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_quan(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu): Identity()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fuse_model(model)\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): BottleNeck_quan(\n",
      "    (conv1): ConvReLU2d(\n",
      "      64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn1): Identity()\n",
      "    (conv2): ConvReLU2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn2): Identity()\n",
      "    (conv3): Conv2d(\n",
      "      64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn3): Identity()\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): ReLU()\n",
      "    (add): FloatFunctional(\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (1): BottleNeck_quan(\n",
      "    (conv1): ConvReLU2d(\n",
      "      256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn1): Identity()\n",
      "    (conv2): ConvReLU2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn2): Identity()\n",
      "    (conv3): Conv2d(\n",
      "      64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn3): Identity()\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): ReLU()\n",
      "    (add): FloatFunctional(\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): BottleNeck_quan(\n",
      "    (conv1): ConvReLU2d(\n",
      "      256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn1): Identity()\n",
      "    (conv2): ConvReLU2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn2): Identity()\n",
      "    (conv3): Conv2d(\n",
      "      64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn3): Identity()\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): ReLU()\n",
      "    (add): FloatFunctional(\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/miniconda3/envs/py312/lib/python3.12/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "print(\n",
    "    \"Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n\",\n",
    "    model.layer1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:14:22<00:00,  5.71s/it]\n",
      "100%|██████████| 782/782 [09:30<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : eval_loss 1.3583892058685918, eval_acc 80.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:13:28<00:00,  5.64s/it]\n",
      "100%|██████████| 782/782 [09:31<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : eval_loss 1.2956677321582803, eval_acc 80.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:14:20<00:00,  5.70s/it]\n",
      "100%|██████████| 782/782 [09:34<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 : eval_loss 1.253429756826147, eval_acc 80.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:15:16<00:00,  5.78s/it]\n",
      "100%|██████████| 782/782 [09:37<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 : eval_loss 1.2062482112432684, eval_acc 80.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:14:43<00:00,  5.73s/it]\n",
      "100%|██████████| 782/782 [09:32<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 : eval_loss 1.164172125861163, eval_acc 80.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:13:06<00:00,  5.61s/it]\n",
      "100%|██████████| 782/782 [09:32<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 : eval_loss 1.1234003631278986, eval_acc 81.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [1:14:47<00:00,  5.74s/it]\n",
      "100%|██████████| 782/782 [09:46<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 : eval_loss 1.0897839167310148, eval_acc 81.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 456/782 [43:44<31:16,  5.75s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# _, _ = SingleEpochTrain(model, train_loader, criterion, optimizer, device, verb=False)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mSingleEpochTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Freeze quantizer parameters\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         model\u001b[38;5;241m.\u001b[39mapply(torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mdisable_observer)\n",
      "File \u001b[0;32m~/Desktop/2024-RISE-Quantization/src/utils.py:272\u001b[0m, in \u001b[0;36mSingleEpochTrain\u001b[0;34m(model, trainloader, criterion, optimizer, device, verb)\u001b[0m\n\u001b[1;32m    269\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(8):\n",
    "    # _, _ = SingleEpochTrain(model, train_loader, criterion, optimizer, device, verb=False)\n",
    "    _, _ = SingleEpochTrain(model, test_loader, criterion, optimizer, device, verb=False)\n",
    "    if epoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        model.apply(torch.quantization.disable_observer)\n",
    "    if epoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    tmp_model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "    eval_loss, eval_acc = SingleEpochEval(tmp_model, test_loader, criterion, device)\n",
    "    print(f\"epoch {epoch} : eval_loss {eval_loss}, eval_acc {eval_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_benchmark(model, test_loader)\n",
    "print_size_of_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

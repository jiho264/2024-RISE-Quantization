{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.utils import *\n",
    "from src.override_resnet import *\n",
    "\n",
    "\n",
    "class Args:\n",
    "    arch = 50\n",
    "    dataset = \"ImageNet\"\n",
    "    # dataset = \"CIFAR100\"\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    batch = 16\n",
    "    epochs = 10\n",
    "    save_every = 1\n",
    "    quan = \"static\"\n",
    "    only_eval = True\n",
    "    verbose = True\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, img_loader):\n",
    "    elapsed = 0\n",
    "    model.eval()\n",
    "    num_batches = 1\n",
    "    # 이미지 배치들 이용하여 스크립트된 모델 실행\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end - start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print(\"Elapsed time: %3.0f ms\" % (elapsed / num_images * 1000))\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_model(model) -> nn.Module:\n",
    "    flag = False\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__ == ResNet_quan.__name__:\n",
    "            if flag == True:\n",
    "                raise ValueError(\"ResNet_quan is already fused\")\n",
    "            flag = True\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\"conv1\", \"bn1\", \"relu\"],\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        if type(m) == BottleNeck_quan:\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\n",
    "                    [\"conv1\", \"bn1\", \"relu1\"],\n",
    "                    [\"conv2\", \"bn2\", \"relu2\"],\n",
    "                    [\"conv3\", \"bn3\"],\n",
    "                ],\n",
    "                inplace=True,\n",
    "            )\n",
    "            if m.downsample is not None:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    m.downsample,\n",
    "                    [\"0\", \"1\"],\n",
    "                    inplace=True,\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Static Quantization enabled\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# %% my code\n",
    "\n",
    "args = Args()\n",
    "# %% Load the ResNet-50 model\n",
    "if args.quan == \"fp32\":\n",
    "    # case 0 : no quantization case\n",
    "    print(\"----------No quantization enabled\")\n",
    "    device = str(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model = layers_mapping[args.arch](\n",
    "        weights=pretrained_weights_mapping[args.arch]\n",
    "    ).to(device)\n",
    "\n",
    "elif args.quan == \"dynamic\":\n",
    "    # case 1 : Dynamic Quantization\n",
    "    print(\"----------Dynamic Quantization enabled\")\n",
    "    device = \"cuda\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    model = quantized_model\n",
    "\n",
    "elif args.quan == \"static\":\n",
    "    # case 2 : Static Quantization\n",
    "    print(\"----------Static Quantization enabled\")\n",
    "    device = \"cpu\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "\n",
    "elif args.quan == \"qat\":\n",
    "    # case 3 : Quantization Aware Training\n",
    "    print(\"----------Quantization Aware Training enabled\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid quantization method\")\n",
    "\n",
    "_folder_path = f\"resnet{args.arch}_{args.dataset}\" + \"_\" + args.quan\n",
    "_file_name = (\n",
    "    f\"resnet{args.arch}_{args.dataset}_epoch\"  # resnet18_cifar10_epoch{epoch}.pth\n",
    ")\n",
    "\n",
    "# %%Set up training and evaluation processes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_loader, test_loader = GetDataset(\n",
    "    dataset_name=args.dataset,\n",
    "    device=device,\n",
    "    root=\"data\",\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. REF (acc@1 : 80.35%, 시간 및 크기 측정만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  43 ms\n",
      "Size (MB): 102.52663\n",
      "BottleNeck_quan(\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (add): FloatFunctional(\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_ = run_benchmark(model, test_loader)\n",
    "print_size_of_model(model)\n",
    "print(model.layer1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuse 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 102.158986\n",
      "None\n",
      "BottleNeck_quan(\n",
      "  (conv1): ConvReLU2d(\n",
      "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (conv2): ConvReLU2d(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (bn2): Identity()\n",
      "  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bn3): Identity()\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (relu1): Identity()\n",
      "  (relu2): Identity()\n",
      "  (relu3): ReLU()\n",
      "  (add): FloatFunctional(\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = fuse_model(model)\n",
    "print(print_size_of_model(model))\n",
    "print(model.layer1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibration (training set 한 바퀴 돌림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/miniconda3/envs/py312/lib/python3.12/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.qconfig = torch.quantization.get_default_qconfig(\"x86\")\n",
    "print(model.qconfig)\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "print(\"Post Training Quantization Prepare: Inserting Observers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [37:28<00:00,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# _, _ = SingleEpochEval(model, train_loader, criterion, device)\n",
    "_, _ = SingleEpochEval(model, test_loader, criterion, device)\n",
    "print(\"Post Training Quantization: Calibration done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.convert(model, inplace=True)\n",
    "print(\"Post Training Quantization: Convert done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static quantization 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  20 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [09:36<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 26.151272\n",
      "Eval Loss: 1.4316, Eval Acc: 79.81%\n",
      "Post Training Quantization: Eval done\n"
     ]
    }
   ],
   "source": [
    "_ = run_benchmark(model, test_loader)\n",
    "\n",
    "eval_loss, eval_acc = SingleEpochEval(model, test_loader, criterion, device)\n",
    "print_size_of_model(model)\n",
    "print(f\"Eval Loss: {eval_loss:.4f}, Eval Acc: {eval_acc:.2f}%\")\n",
    "print(\"Post Training Quantization: Eval done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

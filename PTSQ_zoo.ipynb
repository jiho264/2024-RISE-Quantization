{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.utils import *\n",
    "from src.override_resnet import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fuse_model(model) -> nn.Module:\n",
    "    flag = False\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__ == ResNet_quan.__name__:\n",
    "            if flag == True:\n",
    "                raise ValueError(\"ResNet_quan is already fused\")\n",
    "            flag = True\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\"conv1\", \"bn1\", \"relu\"],\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        if type(m) == BottleNeck_quan:\n",
    "\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\n",
    "                    [\"conv1\", \"bn1\", \"relu1\"],\n",
    "                    [\"conv2\", \"bn2\", \"relu2\"],\n",
    "                    [\"conv3\", \"bn3\"],\n",
    "                ],\n",
    "                inplace=True,\n",
    "            )\n",
    "            if m.downsample is not None:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    m.downsample,\n",
    "                    [\"0\", \"1\"],\n",
    "                    inplace=True,\n",
    "                )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "model = resnet50_quan(weights=pretrained_weights_mapping[50]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(_weights, _name):\n",
    "    if type(_weights) == torch.Tensor:\n",
    "        _tmp = copy.deepcopy(_weights)\n",
    "        _tmp = _tmp.flatten().numpy()\n",
    "\n",
    "    else:\n",
    "        _tmp = copy.deepcopy(_weights)\n",
    "        _tmp = _tmp.weight().int_repr().numpy()\n",
    "        _tmp = _tmp.astype(float)\n",
    "        _tmp = _tmp.flatten()\n",
    "\n",
    "    plt.hist(_tmp.flatten(), bins=256)\n",
    "    plt.xlabel(\"Weight Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of {_name}\")\n",
    "    plt.show()\n",
    "    print(_tmp.min(), _tmp.max(), _tmp.mean(), _tmp.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Acc of Reference Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the origin network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BottleNeck_quan(\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (add): FloatFunctional(\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      ")\n",
      "Post Training Quantization: Eval done\n"
     ]
    }
   ],
   "source": [
    "print(model.layer1[0])\n",
    "# check_accuracy(model=model, device=\"cpu\", batch_size=25)\n",
    "print(\"Post Training Quantization: Eval done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABI4UlEQVR4nO3de1wUZf//8fciB/HAEiqs3J4wzUNqpqai5aFQVPLWtNIyQ6MsQ/Ncet+lVhZmpZZ3RUe1u7P3bXWnqSGeKgnPZaSmZqIiaCmsogLC/P7wx35bAQVcWBhfz8djHrnXXDPzuXaBfXftzI7FMAxDAAAAJuXh7gIAAADKEmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHMKFGjRppxIgR7i7D9F588UU1btxYVapUUdu2bd1dDoAiEHaACm7RokWyWCzasmVLoet79OihVq1aXfFxvv76a82cOfOK93O1+Oabb/T444+ra9euWrhwoZ5//nl3l1Rqn376qe677z41bdpUFotFPXr0cHdJgEt5ursAAK63Z88eeXiU7P9lvv76a7322msEnmJas2aNPDw89O6778rb29vd5VyRN954Q1u3btVNN92kP//8093lAC5H2AFMyMfHx90llFhmZqaqV6/u7jKK7dixY/L19a30QUeS/v3vf+tvf/ubPDw8XDJLCFQ0fIwFmNDF5+zk5OTo6aefVtOmTVW1alXVqlVLN998s+Li4iRJI0aM0GuvvSZJslgsjiVfZmamJk2apPr168vHx0fNmjXTSy+9JMMwnI579uxZPfbYY6pdu7Zq1qypv//97zpy5IgsFovTjNHMmTNlsVj0yy+/6N5779U111yjm2++WZL0008/acSIEWrcuLGqVq0qm82mBx54oMCMQ/4+fv31V913332yWq2qU6eOnnrqKRmGoUOHDmnAgAHy8/OTzWbTyy+/XKzn7vz583r22Wd17bXXysfHR40aNdI//vEPZWVlOfpYLBYtXLhQmZmZjudq0aJFl9xvYmKi+vXrp2uuuUbVq1dXmzZt9Morrzj1WbNmjW655RZVr15d/v7+GjBggHbt2lXouPft26cRI0bI399fVqtVI0eO1JkzZxz9WrVqpZ49exaoIy8vT3/729905513Otrq169f4plAoDJhZgeoJDIyMvTHH38UaM/JybnstjNnzlRMTIwefPBBdezYUXa7XVu2bNG2bdvUq1cvPfzww0pJSVFcXJz+/e9/O21rGIb+/ve/a+3atYqKilLbtm21atUqTZkyRUeOHNG8efMcfUeMGKHPPvtMw4cPV+fOnbV+/XpFREQUWdddd92lpk2b6vnnn3cEp7i4OP32228aOXKkbDabkpKS9NZbbykpKUk//PCDUwiTpCFDhqhFixaaPXu2li9frlmzZikgIEBvvvmmbr31Vr3wwgv68MMPNXnyZN10003q1q3bJZ+rBx98UIsXL9add96pSZMmKTExUTExMdq1a5c+//xzSRdmQt566y1t2rRJ77zzjiSpS5cuRe4zLi5Ot99+u+rWratx48bJZrNp165dWrZsmcaNGydJWr16tfr27avGjRtr5syZOnv2rBYsWKCuXbtq27ZtatSokdM+7777boWEhCgmJkbbtm3TO++8o8DAQL3wwguO52XmzJlKTU2VzWZzbPfdd98pJSVFQ4cOveTzAJiKAaBCW7hwoSHpksv111/vtE3Dhg2NyMhIx+MbbrjBiIiIuORxoqOjjcL+JHzxxReGJGPWrFlO7XfeeadhsViMffv2GYZhGFu3bjUkGePHj3fqN2LECEOSMWPGDEfbjBkzDEnGPffcU+B4Z86cKdD28ccfG5KMDRs2FNjHqFGjHG3nz5836tWrZ1gsFmP27NmO9pMnTxq+vr5Oz0lhduzYYUgyHnzwQaf2yZMnG5KMNWvWONoiIyON6tWrX3J/+TWFhIQYDRs2NE6ePOm0Li8vz/Hvtm3bGoGBgcaff/7paPvxxx8NDw8P4/777y8w7gceeMBpX3fccYdRq1Ytx+M9e/YYkowFCxY49Xv00UeNGjVqFPo8G4ZhXH/99Ub37t0vOy6gMmHeEqgkXnvtNcXFxRVY2rRpc9lt/f39lZSUpL1795b4uF9//bWqVKmixx57zKl90qRJMgxDK1askCStXLlSkvToo4869Rs7dmyR+37kkUcKtPn6+jr+fe7cOf3xxx/q3LmzJGnbtm0F+j/44IOOf1epUkUdOnSQYRiKiopytPv7+6tZs2b67bffiqxFujBWSZo4caJT+6RJkyRJy5cvv+T2hdm+fbsOHDig8ePHy9/f32ld/izV0aNHtWPHDo0YMUIBAQGO9W3atFGvXr0cdf3Vxc/dLbfcoj///FN2u12SdN1116lt27b69NNPHX1yc3P1n//8R/3793d6ngGzI+wAlUTHjh0VFhZWYLnmmmsuu+0zzzyj9PR0XXfddWrdurWmTJmin376qVjHPXjwoIKDg1WzZk2n9hYtWjjW5//Xw8NDISEhTv2aNGlS5L4v7itJJ06c0Lhx4xQUFCRfX1/VqVPH0S8jI6NA/wYNGjg9tlqtqlq1qmrXrl2g/eTJk0XW8tcxXFyzzWaTv7+/Y6wlsX//fkm65Im/+ftt1qxZgXUtWrTQH3/8oczMTKf2i8ed/3Pw1zEOGTJE33//vY4cOSJJWrdunY4dO6YhQ4aUeBxAZUbYAa4C3bp10/79+/Xee++pVatWeuedd9SuXTvH+SbuUtjswt133623335bjzzyiJYuXapvvvnGMWuUl5dXoH+VKlWK1SapwAnVRbn4vKCKqDhjHDJkiAzD0JIlSyRJn332maxWq/r06VMuNQIVBWEHuEoEBARo5MiR+vjjj3Xo0CG1adPG6Qqpot7gGzZsqJSUFJ06dcqpfffu3Y71+f/Ny8vTgQMHnPrt27ev2DWePHlS8fHxmjp1qp5++mndcccd6tWrlxo3blzsfVyJ/DFc/HFfWlqa0tPTHWMtiWuvvVaS9PPPP1/yuNKF70e62O7du1W7du1SXZYfEhKijh076tNPP9X58+e1dOlSDRw4sFJ+NQFwJQg7wFXg4su2a9SooSZNmjhdTp3/Zpqenu7Ut1+/fsrNzdW//vUvp/Z58+bJYrGob9++kqTw8HBJ0uuvv+7Ub8GCBcWuM3+24uIZmPnz5xd7H1eiX79+hR5v7ty5knTJK8uK0q5dO4WEhGj+/PkFntv8cdatW1dt27bV4sWLnfr8/PPP+uabbxx1lcaQIUP0ww8/6L333tMff/zBR1i4KnHpOXAVaNmypXr06KH27dsrICBAW7Zs0X/+8x+NGTPG0ad9+/aSpMcee0zh4eGqUqWKhg4dqv79+6tnz5765z//qd9//1033HCDvvnmG3355ZcaP368Y+aiffv2Gjx4sObPn68///zTcen5r7/+Kql4Hw35+fmpW7dumjNnjnJycvS3v/1N33zzTYHZorJyww03KDIyUm+99ZbS09PVvXt3bdq0SYsXL9bAgQML/d6ay/Hw8NAbb7yh/v37q23btho5cqTq1q2r3bt3KykpSatWrZJ04T5bffv2VWhoqKKiohyXnlut1iv6Vuu7775bkydP1uTJkxUQEKCwsLACfTZs2KANGzZIko4fP67MzEzNmjVL0oWPQC93uT5Q4bnxSjAAxZB/6fnmzZsLXd+9e/fLXno+a9Yso2PHjoa/v7/h6+trNG/e3HjuueeM7OxsR5/z588bY8eONerUqWNYLBany9BPnTplTJgwwQgODja8vLyMpk2bGi+++KLTpdOGYRiZmZlGdHS0ERAQYNSoUcMYOHCg4xLov14Knn/59PHjxwuM5/Dhw8Ydd9xh+Pv7G1ar1bjrrruMlJSUIi9fv3gfRV0SXtjzVJicnBzj6aefNkJCQgwvLy+jfv36xrRp04xz584V6zhF+e6774xevXoZNWvWNKpXr260adOmwGXhq1evNrp27Wr4+voafn5+Rv/+/Y1ffvnFqU9R487/OTlw4ECBY3ft2rXQS+ov3mdhy1+fc6CyshhGMc/YA4BS2LFjh2688UZ98MEHGjZsmLvLAXAV4pwdAC5z9uzZAm3z58+Xh4cHH4UAcBvO2QHgMnPmzNHWrVvVs2dPeXp6asWKFVqxYoVGjRql+vXru7s8AFcpPsYC4DJxcXF6+umn9csvv+j06dNq0KCBhg8frn/+85/y9OT/rQC4B2EHAACYGufsAAAAUyPsAAAAU+NDdF24305KSopq1qxZKe6JAwAALnwL+alTpxQcHCwPj6Lnbwg7klJSUrhSBACASurQoUOqV69ekesJO5Jq1qwp6cKT5efn5+ZqAABAcdjtdtWvX9/xPl4Uwo7+7549fn5+hB0AACqZy52CwgnKAADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AEyj0dTlajR1ubvLAFDBEHYAmA6hB8BfEXYAAICpEXYAAICpuTXs5Obm6qmnnlJISIh8fX117bXX6tlnn5VhGI4+hmFo+vTpqlu3rnx9fRUWFqa9e/c67efEiRMaNmyY/Pz85O/vr6ioKJ0+fbq8hwMAACogt4adF154QW+88Yb+9a9/adeuXXrhhRc0Z84cLViwwNFnzpw5evXVVxUbG6vExERVr15d4eHhOnfunKPPsGHDlJSUpLi4OC1btkwbNmzQqFGj3DEkAABQwViMv06jlLPbb79dQUFBevfddx1tgwcPlq+vrz744AMZhqHg4GBNmjRJkydPliRlZGQoKChIixYt0tChQ7Vr1y61bNlSmzdvVocOHSRJK1euVL9+/XT48GEFBwdftg673S6r1aqMjAz5+fmVzWABlLmLT0r+fXaEmyoBUB6K+/7t1pmdLl26KD4+Xr/++qsk6ccff9R3332nvn37SpIOHDig1NRUhYWFObaxWq3q1KmTEhISJEkJCQny9/d3BB1JCgsLk4eHhxITEws9blZWlux2u9MCAADMydOdB586darsdruaN2+uKlWqKDc3V88995yGDRsmSUpNTZUkBQUFOW0XFBTkWJeamqrAwECn9Z6engoICHD0uVhMTIyefvppVw8HAABUQG6d2fnss8/04Ycf6qOPPtK2bdu0ePFivfTSS1q8eHGZHnfatGnKyMhwLIcOHSrT4wEAAPdx68zOlClTNHXqVA0dOlSS1Lp1ax08eFAxMTGKjIyUzWaTJKWlpalu3bqO7dLS0tS2bVtJks1m07Fjx5z2e/78eZ04ccKx/cV8fHzk4+NTBiMCAAAVjVtnds6cOSMPD+cSqlSpory8PElSSEiIbDab4uPjHevtdrsSExMVGhoqSQoNDVV6erq2bt3q6LNmzRrl5eWpU6dO5TAKAABQkbl1Zqd///567rnn1KBBA11//fXavn275s6dqwceeECSZLFYNH78eM2aNUtNmzZVSEiInnrqKQUHB2vgwIGSpBYtWqhPnz566KGHFBsbq5ycHI0ZM0ZDhw4t1pVYAADA3NwadhYsWKCnnnpKjz76qI4dO6bg4GA9/PDDmj59uqPP448/rszMTI0aNUrp6em6+eabtXLlSlWtWtXR58MPP9SYMWN02223ycPDQ4MHD9arr77qjiEBAIAKxq3fs1NR8D07gDnwPTvA1aVSfM8OAABAWSPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAU3Nr2GnUqJEsFkuBJTo6WpJ07tw5RUdHq1atWqpRo4YGDx6stLQ0p30kJycrIiJC1apVU2BgoKZMmaLz58+7YzgAAKACcmvY2bx5s44ePepY4uLiJEl33XWXJGnChAn66quvtGTJEq1fv14pKSkaNGiQY/vc3FxFREQoOztbGzdu1OLFi7Vo0SJNnz7dLeMBAAAVj8UwDMPdReQbP368li1bpr1798put6tOnTr66KOPdOedd0qSdu/erRYtWighIUGdO3fWihUrdPvttyslJUVBQUGSpNjYWD3xxBM6fvy4vL29i3Vcu90uq9WqjIwM+fn5ldn4AJStRlOXOz3+fXaEmyoBUB6K+/5dYc7Zyc7O1gcffKAHHnhAFotFW7duVU5OjsLCwhx9mjdvrgYNGighIUGSlJCQoNatWzuCjiSFh4fLbrcrKSmpyGNlZWXJbrc7LQAAwJwqTNj54osvlJ6erhEjRkiSUlNT5e3tLX9/f6d+QUFBSk1NdfT5a9DJX5+/rigxMTGyWq2OpX79+q4bCAAAqFAqTNh599131bdvXwUHB5f5saZNm6aMjAzHcujQoTI/JgAAcA9PdxcgSQcPHtTq1au1dOlSR5vNZlN2drbS09OdZnfS0tJks9kcfTZt2uS0r/yrtfL7FMbHx0c+Pj4uHAEAAKioKsTMzsKFCxUYGKiIiP87mbB9+/by8vJSfHy8o23Pnj1KTk5WaGioJCk0NFQ7d+7UsWPHHH3i4uLk5+enli1blt8AAABAheX2mZ28vDwtXLhQkZGR8vT8v3KsVquioqI0ceJEBQQEyM/PT2PHjlVoaKg6d+4sSerdu7datmyp4cOHa86cOUpNTdWTTz6p6OhoZm4AAICkChB2Vq9ereTkZD3wwAMF1s2bN08eHh4aPHiwsrKyFB4ertdff92xvkqVKlq2bJlGjx6t0NBQVa9eXZGRkXrmmWfKcwgAAKACq1Dfs+MufM8OYA58zw5wdal037MDAABQFgg7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AEzh4u/YAYB8hB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AptVo6nI1mrrc3WUAcDPCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDW3h50jR47ovvvuU61ateTr66vWrVtry5YtjvWGYWj69OmqW7eufH19FRYWpr179zrt48SJExo2bJj8/Pzk7++vqKgonT59uryHAgAAKiC3hp2TJ0+qa9eu8vLy0ooVK/TLL7/o5Zdf1jXXXOPoM2fOHL366quKjY1VYmKiqlevrvDwcJ07d87RZ9iwYUpKSlJcXJyWLVumDRs2aNSoUe4YEgAAqGAshmEY7jr41KlT9f333+vbb78tdL1hGAoODtakSZM0efJkSVJGRoaCgoK0aNEiDR06VLt27VLLli21efNmdejQQZK0cuVK9evXT4cPH1ZwcPBl67Db7bJarcrIyJCfn5/rBgig3Fzqm5J/nx1RjpUAKC/Fff9268zO//73P3Xo0EF33XWXAgMDdeONN+rtt992rD9w4IBSU1MVFhbmaLNarerUqZMSEhIkSQkJCfL393cEHUkKCwuTh4eHEhMTCz1uVlaW7Ha70wIAAMzJrWHnt99+0xtvvKGmTZtq1apVGj16tB577DEtXrxYkpSamipJCgoKctouKCjIsS41NVWBgYFO6z09PRUQEODoc7GYmBhZrVbHUr9+fVcPDQAAVBBuDTt5eXlq166dnn/+ed14440aNWqUHnroIcXGxpbpcadNm6aMjAzHcujQoTI9HgAAcB+3hp26deuqZcuWTm0tWrRQcnKyJMlms0mS0tLSnPqkpaU51tlsNh07dsxp/fnz53XixAlHn4v5+PjIz8/PaQEAAObk1rDTtWtX7dmzx6nt119/VcOGDSVJISEhstlsio+Pd6y32+1KTExUaGioJCk0NFTp6enaunWro8+aNWuUl5enTp06lcMoAABARebpzoNPmDBBXbp00fPPP6+7775bmzZt0ltvvaW33npLkmSxWDR+/HjNmjVLTZs2VUhIiJ566ikFBwdr4MCBki7MBPXp08fx8VdOTo7GjBmjoUOHFutKLAAAYG5uDTs33XSTPv/8c02bNk3PPPOMQkJCNH/+fA0bNszR5/HHH1dmZqZGjRql9PR03XzzzVq5cqWqVq3q6PPhhx9qzJgxuu222+Th4aHBgwfr1VdfdceQAABABePW79mpKPieHaDy43t2gKtPpfieHQAAgLJG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKbm1rAzc+ZMWSwWp6V58+aO9efOnVN0dLRq1aqlGjVqaPDgwUpLS3PaR3JysiIiIlStWjUFBgZqypQpOn/+fHkPBQAAVFClCju//fabywq4/vrrdfToUcfy3XffOdZNmDBBX331lZYsWaL169crJSVFgwYNcqzPzc1VRESEsrOztXHjRi1evFiLFi3S9OnTXVYfAACo3EoVdpo0aaKePXvqgw8+0Llz566oAE9PT9lsNsdSu3ZtSVJGRobeffddzZ07V7feeqvat2+vhQsXauPGjfrhhx8kSd98841++eUXffDBB2rbtq369u2rZ599Vq+99pqys7OvqC4AAGAOpQo727ZtU5s2bTRx4kTZbDY9/PDD2rRpU6kK2Lt3r4KDg9W4cWMNGzZMycnJkqStW7cqJydHYWFhjr7NmzdXgwYNlJCQIElKSEhQ69atFRQU5OgTHh4uu92upKSkIo+ZlZUlu93utAAAAHMqVdhp27atXnnlFaWkpOi9997T0aNHdfPNN6tVq1aaO3eujh8/Xqz9dOrUSYsWLdLKlSv1xhtv6MCBA7rlllt06tQppaamytvbW/7+/k7bBAUFKTU1VZKUmprqFHTy1+evK0pMTIysVqtjqV+/fglGDwAAKpMrOkHZ09NTgwYN0pIlS/TCCy9o3759mjx5surXr6/7779fR48eveT2ffv21V133aU2bdooPDxcX3/9tdLT0/XZZ59dSVmXNW3aNGVkZDiWQ4cOlenxAACA+1xR2NmyZYseffRR1a1bV3PnztXkyZO1f/9+xcXFKSUlRQMGDCjR/vz9/XXddddp3759stlsys7OVnp6ulOftLQ02Ww2SZLNZitwdVb+4/w+hfHx8ZGfn5/TAgAAzKlUYWfu3Llq3bq1unTpopSUFL3//vs6ePCgZs2apZCQEN1yyy1atGiRtm3bVqL9nj59Wvv371fdunXVvn17eXl5KT4+3rF+z549Sk5OVmhoqCQpNDRUO3fu1LFjxxx94uLi5Ofnp5YtW5ZmaAAAwGQ8S7PRG2+8oQceeEAjRoxQ3bp1C+0TGBiod99995L7mTx5svr376+GDRsqJSVFM2bMUJUqVXTPPffIarUqKipKEydOVEBAgPz8/DR27FiFhoaqc+fOkqTevXurZcuWGj58uObMmaPU1FQ9+eSTio6Olo+PT2mGBgAATKZUYWfv3r2X7ePt7a3IyMhL9jl8+LDuuece/fnnn6pTp45uvvlm/fDDD6pTp44kad68efLw8NDgwYOVlZWl8PBwvf76647tq1SpomXLlmn06NEKDQ1V9erVFRkZqWeeeaY0wwIAACZkMQzDKOlGCxcuVI0aNXTXXXc5tS9ZskRnzpy5bMipaOx2u6xWqzIyMjh/B6ikGk1dXuS632dHlGMlAMpLcd+/S3XOTkxMjOPL//4qMDBQzz//fGl2CQAAUCZKFXaSk5MVEhJSoL1hw4aOLwUEAACoCEoVdgIDA/XTTz8VaP/xxx9Vq1atKy4KAADAVUoVdu655x499thjWrt2rXJzc5Wbm6s1a9Zo3LhxGjp0qKtrBAAAKLVSXY317LPP6vfff9dtt90mT88Lu8jLy9P999/POTsAAKBCKVXY8fb21qeffqpnn31WP/74o3x9fdW6dWs1bNjQ1fUBAABckVKFnXzXXXedrrvuOlfVAgAA4HKlCju5ublatGiR4uPjdezYMeXl5TmtX7NmjUuKAwAAuFKlCjvjxo3TokWLFBERoVatWslisbi6LgAAAJcoVdj55JNP9Nlnn6lfv36urgcAAMClSnXpube3t5o0aeLqWgAAAFyuVGFn0qRJeuWVV1SK22oBAACUq1J9jPXdd99p7dq1WrFiha6//np5eXk5rV+6dKlLigMAALhSpQo7/v7+uuOOO1xdCwAAgMuVKuwsXLjQ1XUAAACUiVKdsyNJ58+f1+rVq/Xmm2/q1KlTkqSUlBSdPn3aZcUBAABcqVLN7Bw8eFB9+vRRcnKysrKy1KtXL9WsWVMvvPCCsrKyFBsb6+o6AQAASqVUMzvjxo1Thw4ddPLkSfn6+jra77jjDsXHx7usOABwhUZTl7u7BABuVKqZnW+//VYbN26Ut7e3U3ujRo105MgRlxQGAMVBkAFwOaWa2cnLy1Nubm6B9sOHD6tmzZpXXBQAAICrlCrs9O7dW/Pnz3c8tlgsOn36tGbMmMEtJAAAQIVSqo+xXn75ZYWHh6tly5Y6d+6c7r33Xu3du1e1a9fWxx9/7OoaAQAASq1UYadevXr68ccf9cknn+inn37S6dOnFRUVpWHDhjmdsAwAAOBupQo7kuTp6an77rvPlbUAAAC4XKnCzvvvv3/J9ffff3+pigEAAHC1UoWdcePGOT3OycnRmTNn5O3trWrVqhF2AABAhVGqq7FOnjzptJw+fVp79uzRzTffzAnKAACgQin1vbEu1rRpU82ePbvArA8AAIA7uSzsSBdOWk5JSXHlLgEAAK5Iqc7Z+d///uf02DAMHT16VP/617/UtWtXlxQGAADgCqUKOwMHDnR6bLFYVKdOHd166616+eWXXVEXAACAS5Qq7OTl5bm6DgAAgDLh0nN2rsTs2bNlsVg0fvx4R9u5c+cUHR2tWrVqqUaNGho8eLDS0tKctktOTlZERISqVaumwMBATZkyRefPny/n6gEAQEVVqpmdiRMnFrvv3LlzL9tn8+bNevPNN9WmTRun9gkTJmj58uVasmSJrFarxowZo0GDBun777+XJOXm5ioiIkI2m00bN27U0aNHdf/998vLy0vPP/98yQYFAABMqVRhZ/v27dq+fbtycnLUrFkzSdKvv/6qKlWqqF27do5+Fovlsvs6ffq0hg0bprfffluzZs1ytGdkZOjdd9/VRx99pFtvvVWStHDhQrVo0UI//PCDOnfurG+++Ua//PKLVq9eraCgILVt21bPPvusnnjiCc2cOVPe3t6lGR4AADCRUn2M1b9/f3Xr1k2HDx/Wtm3btG3bNh06dEg9e/bU7bffrrVr12rt2rVas2bNZfcVHR2tiIgIhYWFObVv3bpVOTk5Tu3NmzdXgwYNlJCQIElKSEhQ69atFRQU5OgTHh4uu92upKSkIo+ZlZUlu93utAAAAHMqVdh5+eWXFRMTo2uuucbRds0112jWrFkluhrrk08+0bZt2xQTE1NgXWpqqry9veXv7+/UHhQUpNTUVEefvwad/PX564oSExMjq9XqWOrXr1/smgEAQOVSqrBjt9t1/PjxAu3Hjx/XqVOnirWPQ4cOady4cfrwww9VtWrV0pRRatOmTVNGRoZjOXToULkeHwAAlJ9ShZ077rhDI0eO1NKlS3X48GEdPnxY//3vfxUVFaVBgwYVax9bt27VsWPH1K5dO3l6esrT01Pr16/Xq6++Kk9PTwUFBSk7O1vp6elO26Wlpclms0mSbDZbgauz8h/n9ymMj4+P/Pz8nBYAAGBOpQo7sbGx6tu3r+699141bNhQDRs21L333qs+ffro9ddfL9Y+brvtNu3cuVM7duxwLB06dNCwYcMc//by8lJ8fLxjmz179ig5OVmhoaGSpNDQUO3cuVPHjh1z9ImLi5Ofn59atmxZmqEBAACTKdXVWNWqVdPrr7+uF198Ufv375ckXXvttapevXqx91GzZk21atXKqa169eqqVauWoz0qKkoTJ05UQECA/Pz8NHbsWIWGhqpz586SpN69e6tly5YaPny45syZo9TUVD355JOKjo6Wj49PaYYGAABMplRhJ9/Ro0d19OhRdevWTb6+vjIMo1iXmxfXvHnz5OHhocGDBysrK0vh4eFOM0dVqlTRsmXLNHr0aIWGhqp69eqKjIzUM88847IaAABA5WYxDMMo6UZ//vmn7r77bq1du1YWi0V79+5V48aN9cADD+iaa66pdPfHstvtslqtysjI4PwdoJJpNHV5sfr9PjuijCsBUN6K+/5dqnN2JkyYIC8vLyUnJ6tatWqO9iFDhmjlypWl2SUAAECZKNXHWN98841WrVqlevXqObU3bdpUBw8edElhAAAArlCqmZ3MzEynGZ18J06c4MRgAABQoZQq7Nxyyy16//33HY8tFovy8vI0Z84c9ezZ02XFAQAAXKlSfYw1Z84c3XbbbdqyZYuys7P1+OOPKykpSSdOnHDckRwAAKAiKNXMTqtWrfTrr7/q5ptv1oABA5SZmalBgwZp+/btuvbaa11dIwAAQKmVeGYnJydHffr0UWxsrP75z3+WRU0AAAAuU+KZHS8vL/30009lUQsAAIDLlepjrPvuu0/vvvuuq2sBAABwuVKdoHz+/Hm99957Wr16tdq3b1/gnlhz5851SXEAAABXqkRh57ffflOjRo30888/q127dpKkX3/91amPK++NBQAAcKVKFHaaNm2qo0ePau3atZIu3B7i1VdfVVBQUJkUBwAAcKVKdM7OxfcMXbFihTIzM11aEAAAgCuV6gTlfKW4YToAAEC5KtHHWBaLpcA5OZyjA8AdGk1d7u4SAFQSJQo7hmFoxIgRjpt9njt3To888kiBq7GWLl3qugoBAACuQInCTmRkpNPj++67z6XFAAAAuFqJws7ChQvLqg4AAIAycUUnKAMAAFR0hB0AAGBqhB0AV4VGU5dzBRdwlSLsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAU3Nr2HnjjTfUpk0b+fn5yc/PT6GhoVqxYoVj/blz5xQdHa1atWqpRo0aGjx4sNLS0pz2kZycrIiICFWrVk2BgYGaMmWKzp8/X95DAQAAFZRbw069evU0e/Zsbd26VVu2bNGtt96qAQMGKCkpSZI0YcIEffXVV1qyZInWr1+vlJQUDRo0yLF9bm6uIiIilJ2drY0bN2rx4sVatGiRpk+f7q4hAQCACsZiGIbh7iL+KiAgQC+++KLuvPNO1alTRx999JHuvPNOSdLu3bvVokULJSQkqHPnzlqxYoVuv/12paSkKCgoSJIUGxurJ554QsePH5e3t3exjmm322W1WpWRkSE/P78yGxsA1yntHcx/nx3h4koAuEtx378rzDk7ubm5+uSTT5SZmanQ0FBt3bpVOTk5CgsLc/Rp3ry5GjRooISEBElSQkKCWrdu7Qg6khQeHi673e6YHSpMVlaW7Ha70wIAAMzJ7WFn586dqlGjhnx8fPTII4/o888/V8uWLZWamipvb2/5+/s79Q8KClJqaqokKTU11Sno5K/PX1eUmJgYWa1Wx1K/fn3XDgoAAFQYbg87zZo1044dO5SYmKjRo0crMjJSv/zyS5kec9q0acrIyHAshw4dKtPjAQAA9/F0dwHe3t5q0qSJJKl9+/bavHmzXnnlFQ0ZMkTZ2dlKT093mt1JS0uTzWaTJNlsNm3atMlpf/lXa+X3KYyPj498fHxcPBIAAFARuX1m52J5eXnKyspS+/bt5eXlpfj4eMe6PXv2KDk5WaGhoZKk0NBQ7dy5U8eOHXP0iYuLk5+fn1q2bFnutQMAgIrHrTM706ZNU9++fdWgQQOdOnVKH330kdatW6dVq1bJarUqKipKEydOVEBAgPz8/DR27FiFhoaqc+fOkqTevXurZcuWGj58uObMmaPU1FQ9+eSTio6OZuYGAABIcnPYOXbsmO6//34dPXpUVqtVbdq00apVq9SrVy9J0rx58+Th4aHBgwcrKytL4eHhev311x3bV6lSRcuWLdPo0aMVGhqq6tWrKzIyUs8884y7hgQAACqYCvc9O+7A9+wAlQ/fswOg0n3PDgAAQFkg7AC4qjSaurzUs0IAKifCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDW33wgUAEqCy8YBlBQzOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwCuSo2mLlejqcvdXQaAckDYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApubWsBMTE6ObbrpJNWvWVGBgoAYOHKg9e/Y49Tl37pyio6NVq1Yt1ahRQ4MHD1ZaWppTn+TkZEVERKhatWoKDAzUlClTdP78+fIcCgAAqKDcGnbWr1+v6Oho/fDDD4qLi1NOTo569+6tzMxMR58JEyboq6++0pIlS7R+/XqlpKRo0KBBjvW5ubmKiIhQdna2Nm7cqMWLF2vRokWaPn26O4YEAAAqGIthGIa7i8h3/PhxBQYGav369erWrZsyMjJUp04dffTRR7rzzjslSbt371aLFi2UkJCgzp07a8WKFbr99tuVkpKioKAgSVJsbKyeeOIJHT9+XN7e3pc9rt1ul9VqVUZGhvz8/Mp0jACujKtv8fD77AiX7g9A+Snu+3eFOmcnIyNDkhQQECBJ2rp1q3JychQWFubo07x5czVo0EAJCQmSpISEBLVu3doRdCQpPDxcdrtdSUlJhR4nKytLdrvdaQEAAOZUYcJOXl6exo8fr65du6pVq1aSpNTUVHl7e8vf39+pb1BQkFJTUx19/hp08tfnrytMTEyMrFarY6lfv76LRwMAACqKChN2oqOj9fPPP+uTTz4p82NNmzZNGRkZjuXQoUNlfkwAAOAenu4uQJLGjBmjZcuWacOGDapXr56j3WazKTs7W+np6U6zO2lpabLZbI4+mzZtctpf/tVa+X0u5uPjIx8fHxePAgAAVERundkxDENjxozR559/rjVr1igkJMRpffv27eXl5aX4+HhH2549e5ScnKzQ0FBJUmhoqHbu3Kljx445+sTFxcnPz08tW7Ysn4EAAIAKy60zO9HR0froo4/05ZdfqmbNmo5zbKxWq3x9fWW1WhUVFaWJEycqICBAfn5+Gjt2rEJDQ9W5c2dJUu/evdWyZUsNHz5cc+bMUWpqqp588klFR0czewMAANx76bnFYim0feHChRoxYoSkC18qOGnSJH388cfKyspSeHi4Xn/9daePqA4ePKjRo0dr3bp1ql69uiIjIzV79mx5ehYvy3HpOVA5uPqy83xcfg5UTsV9/65Q37PjLoQdoHIg7AD4q0r5PTsAAACuRtgBAACmRtgBAACmRtgBAACmRtgBAACmViG+QRkALqWsrsICcHVgZgcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQfAVa/R1OXcbBQwMe56DqDCIoAAcAVmdgAAgKkRdgAAgKkRdgDg/+PcHcCcCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDU3Bp2NmzYoP79+ys4OFgWi0VffPGF03rDMDR9+nTVrVtXvr6+CgsL0969e536nDhxQsOGDZOfn5/8/f0VFRWl06dPl+MoALga32QMwJXcGnYyMzN1ww036LXXXit0/Zw5c/Tqq68qNjZWiYmJql69usLDw3Xu3DlHn2HDhikpKUlxcXFatmyZNmzYoFGjRpXXEAAAQAXn6c6D9+3bV3379i10nWEYmj9/vp588kkNGDBAkvT+++8rKChIX3zxhYYOHapdu3Zp5cqV2rx5szp06CBJWrBggfr166eXXnpJwcHB5TYWAABQMVXYc3YOHDig1NRUhYWFOdqsVqs6deqkhIQESVJCQoL8/f0dQUeSwsLC5OHhocTExCL3nZWVJbvd7rQAAABzqrBhJzU1VZIUFBTk1B4UFORYl5qaqsDAQKf1np6eCggIcPQpTExMjKxWq2OpX7++i6sHAAAVRYUNO2Vp2rRpysjIcCyHDh1yd0kAKhBOkAbMpcKGHZvNJklKS0tzak9LS3Oss9lsOnbsmNP68+fP68SJE44+hfHx8ZGfn5/TAsC9CBgAykqFDTshISGy2WyKj493tNntdiUmJio0NFSSFBoaqvT0dG3dutXRZ82aNcrLy1OnTp3KvWYAV64iBp6KWBOA4nPr1VinT5/Wvn37HI8PHDigHTt2KCAgQA0aNND48eM1a9YsNW3aVCEhIXrqqacUHBysgQMHSpJatGihPn366KGHHlJsbKxycnI0ZswYDR06lCuxAFwxQg5gDm4NO1u2bFHPnj0djydOnChJioyM1KJFi/T4448rMzNTo0aNUnp6um6++WatXLlSVatWdWzz4YcfasyYMbrtttvk4eGhwYMH69VXXy33sQAoHQIFgLJmMQzDcHcR7ma322W1WpWRkcH5O0A5qyxh5/fZEe4uAcBFivv+XWHP2QEAAHAFwg4AFANXiwGVF2EHAACYGmEHAACYmluvxgJwdfnrx0CV/YTf/LFU9nEAVwNmdgAAgKkxswMAJcBJykDlw8wOAAAwNcIOgDJn5su2zTouwEwIOwAAwNQ4ZweASzWautxxhdKlZj2YEQFQXpjZAQAApkbYAQAApkbYAYArZOYTsAEzIOwAgIsRfoCKhROUAbgcb/QAKhLCDgCXIOAAqKj4GAtAiRFsClfYx1d8pAW4HzM7AIqNN20AlREzOwAKdblgw4zF5f31+WHWB3Afwg5wleMN1/14DYCyxcdYAJzwplu2inMLjfzbbQBwDcIOgMviHlcAKjPCDoAiEWTcj9ke4MoRdoCrEG+gFR9BE3AdTlAGgEqKE5uB4iHsAEAFU1iAIdgApcfHWIDJFPaGmP9xFW+W5sbHk0DhCDtAJVecAFNUH8JP5VPc16yo4NNo6nLCEK46hB0Akgg+VxNmgHC1IewAFcDl/m/7Uv+XDlzJ7N7F64oTgAhLqGxMc4Lya6+9pkaNGqlq1arq1KmTNm3a5O6SYDL5J4iWdcC41DE4SRUlVdqfp4vX8cWSqMxMMbPz6aefauLEiYqNjVWnTp00f/58hYeHa8+ePQoMDHR3ebjKXOr/eouzrqjHQHFdyc+Oq34OS/t7AJQFU4SduXPn6qGHHtLIkSMlSbGxsVq+fLnee+89TZ061c3VobIr6jJgyfmPdWneJK70jQRwJ1f9HJb2Y1yguCyGYRjuLuJKZGdnq1q1avrPf/6jgQMHOtojIyOVnp6uL7/88rL7sNvtslqtysjIkJ+fXxlWe3UrTkBw1R+zv/7xLM4f5ML6crk2UP4u9Xt38brS/r346/YEqcqtuO/flX5m548//lBubq6CgoKc2oOCgrR79+5Ct8nKylJWVpbjcUZGhqQLT5qrtZqxSpL089PhLt+3q7SasapAfYXVfXFb/uO/Kmxdflte1hlJUoMJS4qs5VLrCjtGUXUXd1+X6luS7QG4Rkn+Ptjt9kL/DkkX/k4Uta6w/RX297+ov99//dtTnL+LRdV3uWOV5P2jsL+5V6qwv7HF3S6/jrJ+D8x/3S47b2NUckeOHDEkGRs3bnRqnzJlitGxY8dCt5kxY4YhiYWFhYWFhcUEy6FDhy6ZFSr9zE7t2rVVpUoVpaWlObWnpaXJZrMVus20adM0ceJEx+O8vDydOHFCtWrVksViKdZx7Xa76tevr0OHDpn6o6+rYZxXwxglxmk2jNNcGGfpGIahU6dOKTg4+JL9Kn3Y8fb2Vvv27RUfH+84ZycvL0/x8fEaM2ZModv4+PjIx8fHqc3f379Ux/fz8zP1D2a+q2GcV8MYJcZpNozTXBhnyVmt1sv2qfRhR5ImTpyoyMhIdejQQR07dtT8+fOVmZnpuDoLAABcvUwRdoYMGaLjx49r+vTpSk1NVdu2bbVy5coCJy0DAICrjynCjiSNGTOmyI+tyoKPj49mzJhR4OMws7kaxnk1jFFinGbDOM2FcZatSv89OwAAAJdimntjAQAAFIawAwAATI2wAwAATI2wAwAATI2wUwy///67oqKiFBISIl9fX1177bWaMWOGsrOzL7nduXPnFB0drVq1aqlGjRoaPHhwgW96rmiee+45denSRdWqVSv2Fy2OGDFCFovFaenTp0/ZFnqFSjNOwzA0ffp01a1bV76+vgoLC9PevXvLttArdOLECQ0bNkx+fn7y9/dXVFSUTp8+fcltevToUeD1fOSRR8qp4uJ57bXX1KhRI1WtWlWdOnXSpk2bLtl/yZIlat68uapWrarWrVvr66+/LqdKr0xJxrlo0aICr1vVqlXLsdqS27Bhg/r376/g4GBZLBZ98cUXl91m3bp1ateunXx8fNSkSRMtWrSozOu8UiUd57p16wq8lhaLRampqeVTcCnFxMTopptuUs2aNRUYGKiBAwdqz549l92uPH4/CTvFsHv3buXl5enNN99UUlKS5s2bp9jYWP3jH/+45HYTJkzQV199pSVLlmj9+vVKSUnRoEGDyqnq0snOztZdd92l0aNHl2i7Pn366OjRo47l448/LqMKXaM045wzZ45effVVxcbGKjExUdWrV1d4eLjOnTtXhpVemWHDhikpKUlxcXFatmyZNmzYoFGjRl12u4ceesjp9ZwzZ045VFs8n376qSZOnKgZM2Zo27ZtuuGGGxQeHq5jx44V2n/jxo265557FBUVpe3bt2vgwIEaOHCgfv7553KuvGRKOk7pwrfS/vV1O3jwYDlWXHKZmZm64YYb9NprrxWr/4EDBxQREaGePXtqx44dGj9+vB588EGtWlW8m2+6S0nHmW/Pnj1Or2dgYGAZVega69evV3R0tH744QfFxcUpJydHvXv3VmZmZpHblNvvp0vuxnkVmjNnjhESElLk+vT0dMPLy8tYsmSJo23Xrl2GJCMhIaE8SrwiCxcuNKxWa7H6RkZGGgMGDCjTespKcceZl5dn2Gw248UXX3S0paenGz4+PsbHH39chhWW3i+//GJIMjZv3uxoW7FihWGxWIwjR44UuV337t2NcePGlUOFpdOxY0cjOjra8Tg3N9cIDg42YmJiCu1/9913GxEREU5tnTp1Mh5++OEyrfNKlXScJfmdrYgkGZ9//vkl+zz++OPG9ddf79Q2ZMgQIzw8vAwrc63ijHPt2rWGJOPkyZPlUlNZOXbsmCHJWL9+fZF9yuv3k5mdUsrIyFBAQECR67du3aqcnByFhYU52po3b64GDRooISGhPEosV+vWrVNgYKCaNWum0aNH688//3R3SS514MABpaamOr2eVqtVnTp1qrCvZ0JCgvz9/dWhQwdHW1hYmDw8PJSYmHjJbT/88EPVrl1brVq10rRp03TmzJmyLrdYsrOztXXrVqfXwcPDQ2FhYUW+DgkJCU79JSk8PLzCvm5S6cYpSadPn1bDhg1Vv359DRgwQElJSeVRbrmpjK/llWjbtq3q1q2rXr166fvvv3d3OSWWkZEhSZd8ryyv19Q036Bcnvbt26cFCxbopZdeKrJPamqqvL29C5wPEhQUVOE/dy2pPn36aNCgQQoJCdH+/fv1j3/8Q3379lVCQoKqVKni7vJcIv81u/gWJBX59UxNTS0w7e3p6amAgIBL1nzvvfeqYcOGCg4O1k8//aQnnnhCe/bs0dKlS8u65Mv6448/lJubW+jrsHv37kK3SU1NrVSvm1S6cTZr1kzvvfee2rRpo4yMDL300kvq0qWLkpKSVK9evfIou8wV9Vra7XadPXtWvr6+bqrMterWravY2Fh16NBBWVlZeuedd9SjRw8lJiaqXbt27i6vWPLy8jR+/Hh17dpVrVq1KrJfef1+XtUzO1OnTi30JLC/Lhf/YTly5Ij69Omju+66Sw899JCbKi+Z0oyzJIYOHaq///3vat26tQYOHKhly5Zp8+bNWrdunesGUQxlPc6KoqzHOWrUKIWHh6t169YaNmyY3n//fX3++efav3+/C0cBVwsNDdX999+vtm3bqnv37lq6dKnq1KmjN998092loYSaNWumhx9+WO3bt1eXLl303nvvqUuXLpo3b567Syu26Oho/fzzz/rkk0/cXYqkq3xmZ9KkSRoxYsQl+zRu3Njx75SUFPXs2VNdunTRW2+9dcntbDabsrOzlZ6e7jS7k5aWJpvNdiVll1hJx3mlGjdurNq1a2vfvn267bbbXLbfyynLcea/Zmlpaapbt66jPS0tTW3bti3VPkuruOO02WwFTmY9f/68Tpw4UaKfwU6dOkm6MKN57bXXlrheV6pdu7aqVKlS4KrGS/1e2Wy2EvWvCEozzot5eXnpxhtv1L59+8qiRLco6rX08/MzzaxOUTp27KjvvvvO3WUUy5gxYxwXRFxuVrG8fj+v6rBTp04d1alTp1h9jxw5op49e6p9+/ZauHChPDwuPSnWvn17eXl5KT4+XoMHD5Z04cz65ORkhYaGXnHtJVGScbrC4cOH9eeffzqFgvJQluMMCQmRzWZTfHy8I9zY7XYlJiaW+Mq1K1XccYaGhio9PV1bt25V+/btJUlr1qxRXl6eI8AUx44dOySp3F/Pwnh7e6t9+/aKj4/XwIEDJV2YLo+Pjy/yRsChoaGKj4/X+PHjHW1xcXHl/ntYEqUZ58Vyc3O1c+dO9evXrwwrLV+hoaEFLkuu6K+lq+zYsaNC/A5eimEYGjt2rD7//HOtW7dOISEhl92m3H4/XXq6s0kdPnzYaNKkiXHbbbcZhw8fNo4ePepY/tqnWbNmRmJioqPtkUceMRo0aGCsWbPG2LJlixEaGmqEhoa6YwjFdvDgQWP79u3G008/bdSoUcPYvn27sX37duPUqVOOPs2aNTOWLl1qGIZhnDp1ypg8ebKRkJBgHDhwwFi9erXRrl07o2nTpsa5c+fcNYzLKuk4DcMwZs+ebfj7+xtffvml8dNPPxkDBgwwQkJCjLNnz7pjCMXSp08f48YbbzQSExON7777zmjatKlxzz33ONZf/HO7b98+45lnnjG2bNliHDhwwPjyyy+Nxo0bG926dXPXEAr45JNPDB8fH2PRokXGL7/8YowaNcrw9/c3UlNTDcMwjOHDhxtTp0519P/+++8NT09P46WXXjJ27dplzJgxw/Dy8jJ27tzpriEUS0nH+fTTTxurVq0y9u/fb2zdutUYOnSoUbVqVSMpKcldQ7isU6dOOX73JBlz5841tm/fbhw8eNAwDMOYOnWqMXz4cEf/3377zahWrZoxZcoUY9euXcZrr71mVKlSxVi5cqW7hlAsJR3nvHnzjC+++MLYu3evsXPnTmPcuHGGh4eHsXr1ancNoVhGjx5tWK1WY926dU7vk2fOnHH0cdfvJ2GnGBYuXGhIKnTJd+DAAUOSsXbtWkfb2bNnjUcffdS45pprjGrVqhl33HGHU0CqiCIjIwsd51/HJclYuHChYRiGcebMGaN3795GnTp1DC8vL6Nhw4bGQw895PiDXFGVdJyGceHy86eeesoICgoyfHx8jNtuu83Ys2dP+RdfAn/++adxzz33GDVq1DD8/PyMkSNHOgW6i39uk5OTjW7duhkBAQGGj4+P0aRJE2PKlClGRkaGm0ZQuAULFhgNGjQwvL29jY4dOxo//PCDY1337t2NyMhIp/6fffaZcd111xne3t7G9ddfbyxfvrycKy6dkoxz/Pjxjr5BQUFGv379jG3btrmh6uLLv8T64iV/XJGRkUb37t0LbNO2bVvD29vbaNy4sdPvaEVV0nG+8MILxrXXXmtUrVrVCAgIMHr06GGsWbPGPcWXQFHvk399jdz1+2n5/wUCAACY0lV9NRYAADA/wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4At1u3bp0sFovS09OLvc3MmTPL/b5kl9KoUSPNnz/f3WUAKARhB0CxxcbGqmbNmjp//ryj7fTp0/Ly8lKPHj2c+uYHmOLcLb1Lly46evSorFarS+vt0aOH0z13CtO6dWs98sgjha7797//LR8fH/3xxx8urQtA+SLsACi2nj176vTp09qyZYuj7dtvv5XNZlNiYqLOnTvnaF+7dq0aNGhQrDule3t7y2azyWKxlEndlxIVFaVPPvlEZ8+eLbBu4cKF+vvf/67atWuXe10AXIewA6DYmjVrprp162rdunWOtnXr1mnAgAEKCQnRDz/84NTes2dPSRfu2B0TE6OQkBD5+vrqhhtu0H/+8x+nvhd/jPX222+rfv36qlatmu644w7NnTtX/v7+BWr697//rUaNGslqtWro0KE6deqUJGnEiBFav369XnnlFVksFlksFv3+++8Ftr/vvvt09uxZ/fe//3VqP3DggNatW6eoqCjt379fAwYMUFBQkGrUqKGbbrpJq1evLvJ5+v3332WxWBx3jJek9PR0WSwWp+fu559/Vt++fVWjRg0FBQVp+PDhzCIBZYCwA6BEevbsqbVr1zoer127Vj169FD37t0d7WfPnlViYqIj7MTExOj9999XbGyskpKSNGHCBN13331av359ocf4/vvv9cgjj2jcuHHasWOHevXqpeeee65Av/379+uLL77QsmXLtGzZMq1fv16zZ8+WJL3yyisKDQ3VQw89pKNHj+ro0aOqX79+gX3Url1bAwYM0HvvvefUvmjRItWrV0+9e/fW6dOn1a9fP8XHx2v79u3q06eP+vfvr+Tk5NI9iboQfm699VbdeOON2rJli1auXKm0tDTdfffdpd4ngCK4/NaiAEzt7bffNqpXr27k5OQYdrvd8PT0NI4dO2Z89NFHRrdu3QzDMIz4+HhDknHw4EHj3LlzRrVq1YyNGzc67ScqKsq45557DMP4v7tCnzx50jAMwxgyZIgRERHh1H/YsGGG1Wp1PJ4xY4ZRrVo1w263O9qmTJlidOrUyfG4e/fuxrhx4y47ppUrVxoWi8X47bffDMO4cIf7hg0bGk8++WSR21x//fXGggULHI8bNmxozJs3zzCM/7ub/Pbt2x3rT5486XSH+Weffdbo3bu30z4PHTpkSDL27Nlz2ZoBFB8zOwBKpEePHsrMzNTmzZv17bff6rrrrlOdOnXUvXt3x3k769atU+PGjdWgQQPt27dPZ86cUa9evVSjRg3H8v777xd58vKePXvUsWNHp7aLH0sXroCqWbOm43HdunV17NixEo+pV69eqlevnhYuXChJio+PV3JyskaOHCnpwknYkydPVosWLeTv768aNWpo165dVzSz8+OPP2rt2rVOz0nz5s0lqVgndQMoPk93FwCgcmnSpInq1auntWvX6uTJk+revbskKTg4WPXr19fGjRu1du1a3XrrrZIuBAVJWr58uf72t7857cvHx+eKavHy8nJ6bLFYlJeXV+L9eHh4aMSIEVq8eLFmzpyphQsXqmfPnmrcuLEkafLkyYqLi9NLL72kJk2ayNfXV3feeaeys7OL3J8kGYbhaMvJyXHqc/r0afXv318vvPBCge3r1q1b4jEAKBphB0CJ9ezZU+vWrdPJkyc1ZcoUR3u3bt20YsUKbdq0SaNHj5YktWzZUj4+PkpOTnYEo8tp1qyZNm/e7NR28ePi8Pb2Vm5ubrH6jhw5UrNmzdLSpUv1+eef65133nGs+/777zVixAjdcccdki4ElcJOds5Xp04dSdLRo0d14403SpLTycqS1K5dO/33v/9Vo0aN5OnJn2KgLPExFoAS69mzp7777jvt2LHDKcB0795db775prKzsx0nJ9esWVOTJ0/WhAkTtHjxYu3fv1/btm3TggULtHjx4kL3P3bsWH399deaO3eu9u7dqzfffFMrVqwo8aXpjRo1UmJion7//Xf98ccfl5z1CQkJ0a233qpRo0bJx8dHgwYNcqxr2rSpli5dqh07dujHH3/Uvffee8l9+fr6qnPnzpo9e7Z27dql9evX68knn3TqEx0drRMnTuiee+7R5s2btX//fq1atUojR44sdkADUDyEHQAl1rNnT509e1ZNmjRRUFCQo7179+46deqU4xL1fM8++6yeeuopxcTEqEWLFurTp4+WL1+ukJCQQvfftWtXxcbGau7cubrhhhu0cuVKTZgwQVWrVi1RnZMnT1aVKlXUsmVL1alT57Ln2ERFRenkyZO69957nY41d+5cXXPNNerSpYv69++v8PBwtWvX7pL7eu+993T+/Hm1b99e48eP16xZs5zWBwcH6/vvv1dubq569+6t1q1ba/z48fL393d8DAbANSzGXz9UBoAK6qGHHtLu3bv17bffursUAJUMHxQDqJBeeukl9erVS9WrV9eKFSu0ePFivf766+4uC0AlxMwOgArp7rvv1rp163Tq1Ck1btxYY8eOLfIeVgBwKYQdAABgapwFBwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATO3/Aa3RP+W3O1QdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9180801 1.9789636 -0.0006088594 0.26923782\n"
     ]
    }
   ],
   "source": [
    "__conv1 = model.conv1.weight.data\n",
    "show_plot(__conv1, \"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5129721 1.0011052 -0.015311795 0.16769269\n"
     ]
    }
   ],
   "source": [
    "__layer1_0_conv1 = model.layer1[0].conv1.weight.data\n",
    "show_plot(__layer1_0_conv1, \"layer1_0_conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.43231362 1.2790387 9.732936e-07 0.040513273\n"
     ]
    }
   ],
   "source": [
    "__fc = model.fc.weight.data\n",
    "show_plot(__fc, \"fc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the fused network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 102.159626\n",
      "102.159626\n",
      "BottleNeck_quan(\n",
      "  (conv1): ConvReLU2d(\n",
      "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (conv2): ConvReLU2d(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (bn2): Identity()\n",
      "  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (bn3): Identity()\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (relu1): Identity()\n",
      "  (relu2): Identity()\n",
      "  (relu3): ReLU()\n",
      "  (add): FloatFunctional(\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model = fuse_model(model)\n",
    "print(print_size_of_model(model))\n",
    "print(model.layer1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of ResNet_quan(\n",
      "  (conv1): ConvReLU2d(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): Identity()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (1): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (2): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (1): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (2): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (3): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (1): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (2): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (3): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (4): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (5): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (1): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "    (2): BottleNeck_quan(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): ConvReLU2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn3): Identity()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (relu1): Identity()\n",
      "      (relu2): Identity()\n",
      "      (relu3): ReLU()\n",
      "      (add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (quant): QuantStub()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calibration for Post-Training Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization Prepare: Inserting Observers\n"
     ]
    }
   ],
   "source": [
    "# model.conv1.qconfig = torch.quantization.QConfig(\n",
    "#     activation=torch.quantization.PlaceholderObserver.with_args(dtype=torch.qint8),\n",
    "#     weight=torch.quantization.PlaceholderObserver.with_args(dtype=torch.qint8),\n",
    "# )\n",
    "\n",
    "# model.conv1.qconfig = torch.quantization.QConfig(\n",
    "#     activation=torch.quantization.NoopObserver,\n",
    "#     weight=torch.quantization.NoopObserver.with_args(dtype=torch.qint8),\n",
    "# )\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"x86\")\n",
    "print(\"Post Training Quantization Prepare: Inserting Observers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization Prepare: Inserting Observers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/miniconda3/envs/py312/lib/python3.12/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "print(\"Post Training Quantization Prepare: Inserting Observers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module  has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f9999e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f9999e0>})\n",
      "Module conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418fbf9440>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418fbf9440>})\n",
      "Module conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc720>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc720>})\n",
      "Module conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc7c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc7c0>})\n",
      "Module conv1.activation_post_process does not have a qconfig\n",
      "Module bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc860>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc860>})\n",
      "Module relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc900>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc900>})\n",
      "Module maxpool has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc9a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fc9a0>})\n",
      "Module layer1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fca40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fca40>})\n",
      "Module layer1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcae0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcae0>})\n",
      "Module layer1.0.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcb80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcb80>})\n",
      "Module layer1.0.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcc20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcc20>})\n",
      "Module layer1.0.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fccc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fccc0>})\n",
      "Module layer1.0.conv1.activation_post_process does not have a qconfig\n",
      "Module layer1.0.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcd60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcd60>})\n",
      "Module layer1.0.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fce00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fce00>})\n",
      "Module layer1.0.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcea0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcea0>})\n",
      "Module layer1.0.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcf40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcf40>})\n",
      "Module layer1.0.conv2.activation_post_process does not have a qconfig\n",
      "Module layer1.0.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcfe0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fcfe0>})\n",
      "Module layer1.0.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd080>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd080>})\n",
      "Module layer1.0.conv3.activation_post_process does not have a qconfig\n",
      "Module layer1.0.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd120>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd120>})\n",
      "Module layer1.0.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd1c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd1c0>})\n",
      "Module layer1.0.downsample has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd260>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd260>})\n",
      "Module layer1.0.downsample.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd300>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd300>})\n",
      "Module layer1.0.downsample.0.activation_post_process does not have a qconfig\n",
      "Module layer1.0.downsample.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd3a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd3a0>})\n",
      "Module layer1.0.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd440>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd440>})\n",
      "Module layer1.0.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd4e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd4e0>})\n",
      "Module layer1.0.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd580>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd580>})\n",
      "Module layer1.0.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd620>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd620>})\n",
      "Module layer1.0.add.activation_post_process does not have a qconfig\n",
      "Module layer1.0.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd760>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd760>})\n",
      "Module layer1.0.quant.activation_post_process does not have a qconfig\n",
      "Module layer1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd800>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd800>})\n",
      "Module layer1.1.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd8a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd8a0>})\n",
      "Module layer1.1.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd940>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd940>})\n",
      "Module layer1.1.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd9e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fd9e0>})\n",
      "Module layer1.1.conv1.activation_post_process does not have a qconfig\n",
      "Module layer1.1.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fda80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fda80>})\n",
      "Module layer1.1.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdb20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdb20>})\n",
      "Module layer1.1.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdbc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdbc0>})\n",
      "Module layer1.1.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdc60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdc60>})\n",
      "Module layer1.1.conv2.activation_post_process does not have a qconfig\n",
      "Module layer1.1.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdd00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdd00>})\n",
      "Module layer1.1.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdda0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdda0>})\n",
      "Module layer1.1.conv3.activation_post_process does not have a qconfig\n",
      "Module layer1.1.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fde40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fde40>})\n",
      "Module layer1.1.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdee0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdee0>})\n",
      "Module layer1.1.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdf80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fdf80>})\n",
      "Module layer1.1.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe020>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe020>})\n",
      "Module layer1.1.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe0c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe0c0>})\n",
      "Module layer1.1.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe160>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe160>})\n",
      "Module layer1.1.add.activation_post_process does not have a qconfig\n",
      "Module layer1.1.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe2a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe2a0>})\n",
      "Module layer1.1.quant.activation_post_process does not have a qconfig\n",
      "Module layer1.2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe340>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe340>})\n",
      "Module layer1.2.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe3e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe3e0>})\n",
      "Module layer1.2.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe480>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe480>})\n",
      "Module layer1.2.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe520>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe520>})\n",
      "Module layer1.2.conv1.activation_post_process does not have a qconfig\n",
      "Module layer1.2.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe5c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe5c0>})\n",
      "Module layer1.2.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe660>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe660>})\n",
      "Module layer1.2.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe700>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe700>})\n",
      "Module layer1.2.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe7a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe7a0>})\n",
      "Module layer1.2.conv2.activation_post_process does not have a qconfig\n",
      "Module layer1.2.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe840>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe840>})\n",
      "Module layer1.2.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe8e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe8e0>})\n",
      "Module layer1.2.conv3.activation_post_process does not have a qconfig\n",
      "Module layer1.2.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe980>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fe980>})\n",
      "Module layer1.2.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fea20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fea20>})\n",
      "Module layer1.2.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7feac0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7feac0>})\n",
      "Module layer1.2.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7feb60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7feb60>})\n",
      "Module layer1.2.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fec00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fec00>})\n",
      "Module layer1.2.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7feca0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7feca0>})\n",
      "Module layer1.2.add.activation_post_process does not have a qconfig\n",
      "Module layer1.2.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fede0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fede0>})\n",
      "Module layer1.2.quant.activation_post_process does not have a qconfig\n",
      "Module layer2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fee80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fee80>})\n",
      "Module layer2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fef20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fef20>})\n",
      "Module layer2.0.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fefc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fefc0>})\n",
      "Module layer2.0.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff060>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff060>})\n",
      "Module layer2.0.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff100>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff100>})\n",
      "Module layer2.0.conv1.activation_post_process does not have a qconfig\n",
      "Module layer2.0.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff1a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff1a0>})\n",
      "Module layer2.0.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff240>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff240>})\n",
      "Module layer2.0.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff2e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff2e0>})\n",
      "Module layer2.0.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff380>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff380>})\n",
      "Module layer2.0.conv2.activation_post_process does not have a qconfig\n",
      "Module layer2.0.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff420>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff420>})\n",
      "Module layer2.0.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff4c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff4c0>})\n",
      "Module layer2.0.conv3.activation_post_process does not have a qconfig\n",
      "Module layer2.0.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff560>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff560>})\n",
      "Module layer2.0.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff600>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff600>})\n",
      "Module layer2.0.downsample has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff6a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff6a0>})\n",
      "Module layer2.0.downsample.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff740>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff740>})\n",
      "Module layer2.0.downsample.0.activation_post_process does not have a qconfig\n",
      "Module layer2.0.downsample.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff7e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff7e0>})\n",
      "Module layer2.0.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff880>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff880>})\n",
      "Module layer2.0.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff920>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff920>})\n",
      "Module layer2.0.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff9c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ff9c0>})\n",
      "Module layer2.0.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffa60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffa60>})\n",
      "Module layer2.0.add.activation_post_process does not have a qconfig\n",
      "Module layer2.0.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffba0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffba0>})\n",
      "Module layer2.0.quant.activation_post_process does not have a qconfig\n",
      "Module layer2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffc40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffc40>})\n",
      "Module layer2.1.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffce0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffce0>})\n",
      "Module layer2.1.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffd80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffd80>})\n",
      "Module layer2.1.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffe20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffe20>})\n",
      "Module layer2.1.conv1.activation_post_process does not have a qconfig\n",
      "Module layer2.1.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffec0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7ffec0>})\n",
      "Module layer2.1.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fff60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x75418f7fff60>})\n",
      "Module layer2.1.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98040>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98040>})\n",
      "Module layer2.1.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b980e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b980e0>})\n",
      "Module layer2.1.conv2.activation_post_process does not have a qconfig\n",
      "Module layer2.1.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98180>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98180>})\n",
      "Module layer2.1.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98220>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98220>})\n",
      "Module layer2.1.conv3.activation_post_process does not have a qconfig\n",
      "Module layer2.1.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b982c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b982c0>})\n",
      "Module layer2.1.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98360>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98360>})\n",
      "Module layer2.1.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98400>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98400>})\n",
      "Module layer2.1.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b984a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b984a0>})\n",
      "Module layer2.1.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98540>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98540>})\n",
      "Module layer2.1.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b985e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b985e0>})\n",
      "Module layer2.1.add.activation_post_process does not have a qconfig\n",
      "Module layer2.1.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98720>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98720>})\n",
      "Module layer2.1.quant.activation_post_process does not have a qconfig\n",
      "Module layer2.2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b987c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b987c0>})\n",
      "Module layer2.2.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98860>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98860>})\n",
      "Module layer2.2.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98900>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98900>})\n",
      "Module layer2.2.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b989a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b989a0>})\n",
      "Module layer2.2.conv1.activation_post_process does not have a qconfig\n",
      "Module layer2.2.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98a40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98a40>})\n",
      "Module layer2.2.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98ae0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98ae0>})\n",
      "Module layer2.2.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98b80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98b80>})\n",
      "Module layer2.2.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98c20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98c20>})\n",
      "Module layer2.2.conv2.activation_post_process does not have a qconfig\n",
      "Module layer2.2.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98cc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98cc0>})\n",
      "Module layer2.2.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98d60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98d60>})\n",
      "Module layer2.2.conv3.activation_post_process does not have a qconfig\n",
      "Module layer2.2.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98e00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98e00>})\n",
      "Module layer2.2.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98ea0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98ea0>})\n",
      "Module layer2.2.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98f40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98f40>})\n",
      "Module layer2.2.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98fe0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b98fe0>})\n",
      "Module layer2.2.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99080>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99080>})\n",
      "Module layer2.2.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99120>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99120>})\n",
      "Module layer2.2.add.activation_post_process does not have a qconfig\n",
      "Module layer2.2.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99260>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99260>})\n",
      "Module layer2.2.quant.activation_post_process does not have a qconfig\n",
      "Module layer2.3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99300>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99300>})\n",
      "Module layer2.3.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b993a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b993a0>})\n",
      "Module layer2.3.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99440>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99440>})\n",
      "Module layer2.3.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b994e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b994e0>})\n",
      "Module layer2.3.conv1.activation_post_process does not have a qconfig\n",
      "Module layer2.3.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99580>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99580>})\n",
      "Module layer2.3.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99620>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99620>})\n",
      "Module layer2.3.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b996c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b996c0>})\n",
      "Module layer2.3.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99760>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99760>})\n",
      "Module layer2.3.conv2.activation_post_process does not have a qconfig\n",
      "Module layer2.3.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99800>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99800>})\n",
      "Module layer2.3.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b998a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b998a0>})\n",
      "Module layer2.3.conv3.activation_post_process does not have a qconfig\n",
      "Module layer2.3.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99940>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99940>})\n",
      "Module layer2.3.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b999e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b999e0>})\n",
      "Module layer2.3.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99a80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99a80>})\n",
      "Module layer2.3.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99b20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99b20>})\n",
      "Module layer2.3.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99bc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99bc0>})\n",
      "Module layer2.3.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99c60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99c60>})\n",
      "Module layer2.3.add.activation_post_process does not have a qconfig\n",
      "Module layer2.3.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99da0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99da0>})\n",
      "Module layer2.3.quant.activation_post_process does not have a qconfig\n",
      "Module layer3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99e40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99e40>})\n",
      "Module layer3.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99ee0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99ee0>})\n",
      "Module layer3.0.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99f80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b99f80>})\n",
      "Module layer3.0.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a020>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a020>})\n",
      "Module layer3.0.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a0c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a0c0>})\n",
      "Module layer3.0.conv1.activation_post_process does not have a qconfig\n",
      "Module layer3.0.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a160>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a160>})\n",
      "Module layer3.0.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a200>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a200>})\n",
      "Module layer3.0.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a2a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a2a0>})\n",
      "Module layer3.0.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a340>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a340>})\n",
      "Module layer3.0.conv2.activation_post_process does not have a qconfig\n",
      "Module layer3.0.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a3e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a3e0>})\n",
      "Module layer3.0.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a480>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a480>})\n",
      "Module layer3.0.conv3.activation_post_process does not have a qconfig\n",
      "Module layer3.0.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a520>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a520>})\n",
      "Module layer3.0.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a5c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a5c0>})\n",
      "Module layer3.0.downsample has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a660>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a660>})\n",
      "Module layer3.0.downsample.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a700>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a700>})\n",
      "Module layer3.0.downsample.0.activation_post_process does not have a qconfig\n",
      "Module layer3.0.downsample.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a7a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a7a0>})\n",
      "Module layer3.0.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a840>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a840>})\n",
      "Module layer3.0.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a8e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a8e0>})\n",
      "Module layer3.0.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a980>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9a980>})\n",
      "Module layer3.0.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9aa20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9aa20>})\n",
      "Module layer3.0.add.activation_post_process does not have a qconfig\n",
      "Module layer3.0.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ab60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ab60>})\n",
      "Module layer3.0.quant.activation_post_process does not have a qconfig\n",
      "Module layer3.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ac00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ac00>})\n",
      "Module layer3.1.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9aca0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9aca0>})\n",
      "Module layer3.1.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ad40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ad40>})\n",
      "Module layer3.1.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ade0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ade0>})\n",
      "Module layer3.1.conv1.activation_post_process does not have a qconfig\n",
      "Module layer3.1.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ae80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ae80>})\n",
      "Module layer3.1.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9af20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9af20>})\n",
      "Module layer3.1.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9afc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9afc0>})\n",
      "Module layer3.1.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b060>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b060>})\n",
      "Module layer3.1.conv2.activation_post_process does not have a qconfig\n",
      "Module layer3.1.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b100>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b100>})\n",
      "Module layer3.1.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b1a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b1a0>})\n",
      "Module layer3.1.conv3.activation_post_process does not have a qconfig\n",
      "Module layer3.1.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b240>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b240>})\n",
      "Module layer3.1.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b2e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b2e0>})\n",
      "Module layer3.1.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b380>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b380>})\n",
      "Module layer3.1.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b420>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b420>})\n",
      "Module layer3.1.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b4c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b4c0>})\n",
      "Module layer3.1.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b560>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b560>})\n",
      "Module layer3.1.add.activation_post_process does not have a qconfig\n",
      "Module layer3.1.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b6a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b6a0>})\n",
      "Module layer3.1.quant.activation_post_process does not have a qconfig\n",
      "Module layer3.2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b740>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b740>})\n",
      "Module layer3.2.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b7e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b7e0>})\n",
      "Module layer3.2.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b880>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b880>})\n",
      "Module layer3.2.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b920>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b920>})\n",
      "Module layer3.2.conv1.activation_post_process does not have a qconfig\n",
      "Module layer3.2.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b9c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9b9c0>})\n",
      "Module layer3.2.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ba60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9ba60>})\n",
      "Module layer3.2.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bb00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bb00>})\n",
      "Module layer3.2.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bba0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bba0>})\n",
      "Module layer3.2.conv2.activation_post_process does not have a qconfig\n",
      "Module layer3.2.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bc40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bc40>})\n",
      "Module layer3.2.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bce0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bce0>})\n",
      "Module layer3.2.conv3.activation_post_process does not have a qconfig\n",
      "Module layer3.2.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bd80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bd80>})\n",
      "Module layer3.2.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9be20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9be20>})\n",
      "Module layer3.2.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bec0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bec0>})\n",
      "Module layer3.2.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bf60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194b9bf60>})\n",
      "Module layer3.2.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac040>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac040>})\n",
      "Module layer3.2.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac0e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac0e0>})\n",
      "Module layer3.2.add.activation_post_process does not have a qconfig\n",
      "Module layer3.2.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac220>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac220>})\n",
      "Module layer3.2.quant.activation_post_process does not have a qconfig\n",
      "Module layer3.3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac2c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac2c0>})\n",
      "Module layer3.3.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac360>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac360>})\n",
      "Module layer3.3.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac400>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac400>})\n",
      "Module layer3.3.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac4a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac4a0>})\n",
      "Module layer3.3.conv1.activation_post_process does not have a qconfig\n",
      "Module layer3.3.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac540>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac540>})\n",
      "Module layer3.3.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac5e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac5e0>})\n",
      "Module layer3.3.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac680>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac680>})\n",
      "Module layer3.3.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac720>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac720>})\n",
      "Module layer3.3.conv2.activation_post_process does not have a qconfig\n",
      "Module layer3.3.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac7c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac7c0>})\n",
      "Module layer3.3.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac860>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac860>})\n",
      "Module layer3.3.conv3.activation_post_process does not have a qconfig\n",
      "Module layer3.3.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac900>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac900>})\n",
      "Module layer3.3.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac9a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bac9a0>})\n",
      "Module layer3.3.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baca40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baca40>})\n",
      "Module layer3.3.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacae0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacae0>})\n",
      "Module layer3.3.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacb80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacb80>})\n",
      "Module layer3.3.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacc20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacc20>})\n",
      "Module layer3.3.add.activation_post_process does not have a qconfig\n",
      "Module layer3.3.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacd60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacd60>})\n",
      "Module layer3.3.quant.activation_post_process does not have a qconfig\n",
      "Module layer3.4 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bace00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bace00>})\n",
      "Module layer3.4.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacea0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacea0>})\n",
      "Module layer3.4.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacf40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacf40>})\n",
      "Module layer3.4.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacfe0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bacfe0>})\n",
      "Module layer3.4.conv1.activation_post_process does not have a qconfig\n",
      "Module layer3.4.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad080>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad080>})\n",
      "Module layer3.4.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad120>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad120>})\n",
      "Module layer3.4.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad1c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad1c0>})\n",
      "Module layer3.4.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad260>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad260>})\n",
      "Module layer3.4.conv2.activation_post_process does not have a qconfig\n",
      "Module layer3.4.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad300>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad300>})\n",
      "Module layer3.4.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad3a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad3a0>})\n",
      "Module layer3.4.conv3.activation_post_process does not have a qconfig\n",
      "Module layer3.4.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad440>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad440>})\n",
      "Module layer3.4.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad4e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad4e0>})\n",
      "Module layer3.4.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad580>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad580>})\n",
      "Module layer3.4.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad620>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad620>})\n",
      "Module layer3.4.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad6c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad6c0>})\n",
      "Module layer3.4.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad760>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad760>})\n",
      "Module layer3.4.add.activation_post_process does not have a qconfig\n",
      "Module layer3.4.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad8a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad8a0>})\n",
      "Module layer3.4.quant.activation_post_process does not have a qconfig\n",
      "Module layer3.5 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad940>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad940>})\n",
      "Module layer3.5.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad9e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bad9e0>})\n",
      "Module layer3.5.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bada80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bada80>})\n",
      "Module layer3.5.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badb20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badb20>})\n",
      "Module layer3.5.conv1.activation_post_process does not have a qconfig\n",
      "Module layer3.5.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badbc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badbc0>})\n",
      "Module layer3.5.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badc60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badc60>})\n",
      "Module layer3.5.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badd00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badd00>})\n",
      "Module layer3.5.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badda0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badda0>})\n",
      "Module layer3.5.conv2.activation_post_process does not have a qconfig\n",
      "Module layer3.5.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bade40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bade40>})\n",
      "Module layer3.5.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badee0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badee0>})\n",
      "Module layer3.5.conv3.activation_post_process does not have a qconfig\n",
      "Module layer3.5.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badf80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194badf80>})\n",
      "Module layer3.5.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae020>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae020>})\n",
      "Module layer3.5.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae0c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae0c0>})\n",
      "Module layer3.5.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae160>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae160>})\n",
      "Module layer3.5.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae200>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae200>})\n",
      "Module layer3.5.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae2a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae2a0>})\n",
      "Module layer3.5.add.activation_post_process does not have a qconfig\n",
      "Module layer3.5.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae3e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae3e0>})\n",
      "Module layer3.5.quant.activation_post_process does not have a qconfig\n",
      "Module layer4 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae480>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae480>})\n",
      "Module layer4.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae520>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae520>})\n",
      "Module layer4.0.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae5c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae5c0>})\n",
      "Module layer4.0.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae660>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae660>})\n",
      "Module layer4.0.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae700>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae700>})\n",
      "Module layer4.0.conv1.activation_post_process does not have a qconfig\n",
      "Module layer4.0.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae7a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae7a0>})\n",
      "Module layer4.0.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae840>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae840>})\n",
      "Module layer4.0.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae8e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae8e0>})\n",
      "Module layer4.0.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae980>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bae980>})\n",
      "Module layer4.0.conv2.activation_post_process does not have a qconfig\n",
      "Module layer4.0.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baea20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baea20>})\n",
      "Module layer4.0.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baeac0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baeac0>})\n",
      "Module layer4.0.conv3.activation_post_process does not have a qconfig\n",
      "Module layer4.0.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baeb60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baeb60>})\n",
      "Module layer4.0.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baec00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baec00>})\n",
      "Module layer4.0.downsample has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baeca0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baeca0>})\n",
      "Module layer4.0.downsample.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baed40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baed40>})\n",
      "Module layer4.0.downsample.0.activation_post_process does not have a qconfig\n",
      "Module layer4.0.downsample.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baede0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baede0>})\n",
      "Module layer4.0.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baee80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baee80>})\n",
      "Module layer4.0.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baef20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baef20>})\n",
      "Module layer4.0.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baefc0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baefc0>})\n",
      "Module layer4.0.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf060>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf060>})\n",
      "Module layer4.0.add.activation_post_process does not have a qconfig\n",
      "Module layer4.0.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf1a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf1a0>})\n",
      "Module layer4.0.quant.activation_post_process does not have a qconfig\n",
      "Module layer4.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf240>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf240>})\n",
      "Module layer4.1.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf2e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf2e0>})\n",
      "Module layer4.1.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf380>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf380>})\n",
      "Module layer4.1.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf420>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf420>})\n",
      "Module layer4.1.conv1.activation_post_process does not have a qconfig\n",
      "Module layer4.1.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf4c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf4c0>})\n",
      "Module layer4.1.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf560>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf560>})\n",
      "Module layer4.1.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf600>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf600>})\n",
      "Module layer4.1.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf6a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf6a0>})\n",
      "Module layer4.1.conv2.activation_post_process does not have a qconfig\n",
      "Module layer4.1.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf740>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf740>})\n",
      "Module layer4.1.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf7e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf7e0>})\n",
      "Module layer4.1.conv3.activation_post_process does not have a qconfig\n",
      "Module layer4.1.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf880>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf880>})\n",
      "Module layer4.1.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf920>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf920>})\n",
      "Module layer4.1.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf9c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baf9c0>})\n",
      "Module layer4.1.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafa60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafa60>})\n",
      "Module layer4.1.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafb00>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafb00>})\n",
      "Module layer4.1.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafba0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafba0>})\n",
      "Module layer4.1.add.activation_post_process does not have a qconfig\n",
      "Module layer4.1.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafce0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafce0>})\n",
      "Module layer4.1.quant.activation_post_process does not have a qconfig\n",
      "Module layer4.2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafd80>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafd80>})\n",
      "Module layer4.2.conv1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafe20>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafe20>})\n",
      "Module layer4.2.conv1.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafec0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bafec0>})\n",
      "Module layer4.2.conv1.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baff60>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194baff60>})\n",
      "Module layer4.2.conv1.activation_post_process does not have a qconfig\n",
      "Module layer4.2.bn1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4040>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4040>})\n",
      "Module layer4.2.conv2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc40e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc40e0>})\n",
      "Module layer4.2.conv2.0 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4180>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4180>})\n",
      "Module layer4.2.conv2.1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4220>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4220>})\n",
      "Module layer4.2.conv2.activation_post_process does not have a qconfig\n",
      "Module layer4.2.bn2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc42c0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc42c0>})\n",
      "Module layer4.2.conv3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4360>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4360>})\n",
      "Module layer4.2.conv3.activation_post_process does not have a qconfig\n",
      "Module layer4.2.bn3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4400>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4400>})\n",
      "Module layer4.2.relu has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc44a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc44a0>})\n",
      "Module layer4.2.relu1 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4540>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4540>})\n",
      "Module layer4.2.relu2 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc45e0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc45e0>})\n",
      "Module layer4.2.relu3 has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4680>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4680>})\n",
      "Module layer4.2.add has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4720>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4720>})\n",
      "Module layer4.2.add.activation_post_process does not have a qconfig\n",
      "Module layer4.2.quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4860>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4860>})\n",
      "Module layer4.2.quant.activation_post_process does not have a qconfig\n",
      "Module avgpool has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4900>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4900>})\n",
      "Module fc has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc49a0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc49a0>})\n",
      "Module fc.activation_post_process does not have a qconfig\n",
      "Module quant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4a40>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4a40>})\n",
      "Module quant.activation_post_process does not have a qconfig\n",
      "Module dequant has qconfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4ae0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x754194bc4ae0>})\n"
     ]
    }
   ],
   "source": [
    "#    .\n",
    "for name, module in model.named_modules():\n",
    "    #  qconfig  .\n",
    "    if hasattr(module, \"qconfig\") and module.qconfig is not None:\n",
    "        print(f\"Module {name} has qconfig: {module.qconfig}\")\n",
    "    else:\n",
    "        print(f\"Module {name} does not have a qconfig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inference with the representative dataset (calculate the quantization parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5005 [00:03<5:10:09,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader, test_loader = GetDataset(\n",
    "    dataset_name=\"ImageNet\",\n",
    "    device=device,\n",
    "    root=\"data\",\n",
    "    batch_size=256,\n",
    "    num_workers=8,\n",
    ")\n",
    "_, _ = SingleEpochEval(model, train_loader, criterion, \"cuda\", 2)\n",
    "print(\"Post Training Quantization: Calibration done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=19.868104934692383)\n",
      ")\n",
      "layer1.0.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=25.477624893188477)\n",
      ")\n",
      "layer1.1.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=31.43402099609375)\n",
      ")\n",
      "layer1.1.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=26.45480728149414)\n",
      ")\n",
      "layer1.2.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=20.847946166992188)\n",
      ")\n",
      "layer1.2.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=47.31219482421875)\n",
      ")\n",
      "layer2.0.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=49.666500091552734)\n",
      ")\n",
      "layer2.0.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=60.40828323364258)\n",
      ")\n",
      "layer2.1.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=98.18000030517578)\n",
      ")\n",
      "layer2.1.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=45.52012634277344)\n",
      ")\n",
      "layer2.2.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=89.75450897216797)\n",
      ")\n",
      "layer2.2.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=41.67042922973633)\n",
      ")\n",
      "layer2.3.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=89.65528869628906)\n",
      ")\n",
      "layer2.3.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=39.797550201416016)\n",
      ")\n",
      "layer3.0.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=52.10879135131836)\n",
      ")\n",
      "layer3.0.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=19.441930770874023)\n",
      ")\n",
      "layer3.1.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=49.36713409423828)\n",
      ")\n",
      "layer3.1.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=52.54195022583008)\n",
      ")\n",
      "layer3.2.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=27.662155151367188)\n",
      ")\n",
      "layer3.2.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=29.94948387145996)\n",
      ")\n",
      "layer3.3.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=40.89009475708008)\n",
      ")\n",
      "layer3.3.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=47.3126106262207)\n",
      ")\n",
      "layer3.4.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=64.1731185913086)\n",
      ")\n",
      "layer3.4.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=36.01228713989258)\n",
      ")\n",
      "layer3.5.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=24.64007568359375)\n",
      ")\n",
      "layer3.5.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=31.509828567504883)\n",
      ")\n",
      "layer4.0.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=26.78187370300293)\n",
      ")\n",
      "layer4.0.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=21.950286865234375)\n",
      ")\n",
      "layer4.1.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=35.36379623413086)\n",
      ")\n",
      "layer4.1.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=20.39574432373047)\n",
      ")\n",
      "layer4.2.conv1 ConvReLU2d(\n",
      "  (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=34.059661865234375)\n",
      ")\n",
      "layer4.2.conv2 ConvReLU2d(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): HistogramObserver(min_val=0.0, max_val=26.80963706970215)\n",
      ")\n",
      "Post Training Quantization: Convert done\n",
      "Size (MB): 55.88915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55.88915"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "# torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# quantization   \n",
    "no_quant_layers = [model.conv1, model.bn1]\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if (\n",
    "        isinstance(\n",
    "            module,\n",
    "            (nn.intrinsic.ConvBn2d, nn.intrinsic.ConvReLU2d),\n",
    "        )\n",
    "        and module not in no_quant_layers\n",
    "    ):\n",
    "        print(name, module)\n",
    "        module = torch.quantization.convert(module, inplace=True)\n",
    "\n",
    "print(\"Post Training Quantization: Convert done\")\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/quantized/cpu/qconv.cpp:1912 [kernel]\nQuantizedCUDA: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/quantized/cudnn/Conv.cpp:388 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost Training Quantization: Eval done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/2024-RISE-Quantization/src/utils.py:416\u001b[0m, in \u001b[0;36mcheck_accuracy\u001b[0;34m(model, device, dataset_name, batch_size, num_iter)\u001b[0m\n\u001b[1;32m    407\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    409\u001b[0m _, test_loader \u001b[38;5;241m=\u001b[39m GetDataset(\n\u001b[1;32m    410\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m    411\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    415\u001b[0m )\n\u001b[0;32m--> 416\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m print_size_of_model(model)\n\u001b[1;32m    418\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/Desktop/2024-RISE-Quantization/src/utils.py:376\u001b[0m, in \u001b[0;36mrun_benchmark\u001b[0;34m(model, img_loader, device)\u001b[0m\n\u001b[1;32m    374\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    375\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 376\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    378\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m elapsed \u001b[38;5;241m+\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/2024-RISE-Quantization/src/override_resnet.py:115\u001b[0m, in \u001b[0;36mResNet_quan.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[0;32m--> 115\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/2024-RISE-Quantization/src/override_resnet.py:62\u001b[0m, in \u001b[0;36mBottleNeck_quan.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[0;32m---> 62\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     64\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/conv.py:468\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    465\u001b[0m     _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice,\n\u001b[1;32m    467\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py312/lib/python3.12/site-packages/torch/_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/quantized/cpu/qconv.cpp:1912 [kernel]\nQuantizedCUDA: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/quantized/cudnn/Conv.cpp:388 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(model=model, device=\"cpu\", batch_size=25)\n",
    "print(\"Post Training Quantization: Eval done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  .\n",
    "__q__conv1 = model.conv1.weight().int_repr()\n",
    "show_plot(__q__conv1, \"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  .\n",
    "__q__layer1_0_conv1 = model.layer1[0].conv1.weight().int_repr()\n",
    "show_plot(__q__layer1_0_conv1, \"layer1_0_conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  .\n",
    "__q__fc = model.fc.weight().int_repr()\n",
    "show_plot(__q__fc, \"fc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

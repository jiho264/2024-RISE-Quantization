{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.utils import *\n",
    "from src.override_resnet import *\n",
    "\n",
    "\n",
    "class Args:\n",
    "    arch = 50\n",
    "    dataset = \"ImageNet\"\n",
    "    # dataset = \"CIFAR100\"\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    batch = 16\n",
    "    epochs = 10\n",
    "    save_every = 1\n",
    "    quan = \"static\"\n",
    "    only_eval = True\n",
    "    verbose = True\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, img_loader):\n",
    "    elapsed = 0\n",
    "    model.eval()\n",
    "    num_batches = 1\n",
    "    # 이미지 배치들 이용하여 스크립트된 모델 실행\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end - start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print(\"Elapsed time: %3.0f ms\" % (elapsed / num_images * 1000))\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_model(model) -> nn.Module:\n",
    "    flag = False\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__ == ResNet_quan.__name__:\n",
    "            if flag == True:\n",
    "                raise ValueError(\"ResNet_quan is already fused\")\n",
    "            flag = True\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\"conv1\", \"bn1\", \"relu\"],\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        if type(m) == BottleNeck_quan:\n",
    "            torch.quantization.fuse_modules(\n",
    "                m,\n",
    "                [\n",
    "                    [\"conv1\", \"bn1\", \"relu1\"],\n",
    "                    [\"conv2\", \"bn2\", \"relu2\"],\n",
    "                    [\"conv3\", \"bn3\"],\n",
    "                ],\n",
    "                inplace=True,\n",
    "            )\n",
    "            if m.downsample is not None:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    m.downsample,\n",
    "                    [\"0\", \"1\"],\n",
    "                    inplace=True,\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Static Quantization enabled\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# %% my code\n",
    "\n",
    "args = Args()\n",
    "# %% Load the ResNet-50 model\n",
    "if args.quan == \"fp32\":\n",
    "    # case 0 : no quantization case\n",
    "    print(\"----------No quantization enabled\")\n",
    "    device = str(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model = layers_mapping[args.arch](\n",
    "        weights=pretrained_weights_mapping[args.arch]\n",
    "    ).to(device)\n",
    "\n",
    "elif args.quan == \"dynamic\":\n",
    "    # case 1 : Dynamic Quantization\n",
    "    print(\"----------Dynamic Quantization enabled\")\n",
    "    device = \"cuda\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    model = quantized_model\n",
    "\n",
    "elif args.quan == \"static\":\n",
    "    # case 2 : Static Quantization\n",
    "    print(\"----------Static Quantization enabled\")\n",
    "    device = \"cpu\"\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "\n",
    "elif args.quan == \"qat\":\n",
    "    # case 3 : Quantization Aware Training\n",
    "    print(\"----------Quantization Aware Training enabled\")\n",
    "    device = str(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model = resnet50_quan(weights=pretrained_weights_mapping[args.arch]).to(device)\n",
    "else:\n",
    "    raise ValueError(\"Invalid quantization method\")\n",
    "\n",
    "_folder_path = f\"resnet{args.arch}_{args.dataset}\" + \"_\" + args.quan\n",
    "_file_name = (\n",
    "    f\"resnet{args.arch}_{args.dataset}_epoch\"  # resnet18_cifar10_epoch{epoch}.pth\n",
    ")\n",
    "\n",
    "# %%Set up training and evaluation processes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_loader, test_loader = GetDataset(\n",
    "    dataset_name=args.dataset,\n",
    "    device=device,\n",
    "    root=\"data\",\n",
    "    batch_size=256,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순히 양자화 설정 방법을 변경하는 것만으로도 정확도가 67.3%를 넘을 정도로 향상이 되었습니다! 그럼에도 이 수치는 위에서 구한 기준값 71.9%에서 4퍼센트나 낮은 수치입니다. 이제 양자화 자각 학습을 시도해 봅시다.\n",
    "\n",
    "# 5. 양자화 자각 학습(Quantization-aware training)\n",
    "- 양자화 자각 학습(QAT)은 일반적으로 가장 높은 정확도를 제공하는 양자화 방법입니다. 모든 가중치화 활성값은 QAT로 인해 학습 도중에 순전파와 역전파를 도중 《가짜 양자화》됩니다. \n",
    "- 이는 float값이 int8 값으로 반올림하는 것처럼 흉내를 내지만, 모든 계산은 여전히 부동소수점 숫자로 계산을 합니다. 그래서 결국 훈련 동안의 모든 가중치 조정은 모델이 양자화될 것이라는 사실을 《자각》한 채로 이루어지게 됩니다. \n",
    "- 그래서 QAT는 양자화가 이루어지고 나면 동적 양자화나 학습 전 정적 양자화보다 대체로 더 높은 정확도를 보여줍니다.\n",
    "\n",
    "- 실제로 QAT가 이루어지는 전체 흐름은 이전과 매우 유사합니다:\n",
    "\n",
    "  - 이전과 같은 모델을 사용할 수 있습니다. 양자화 자각 학습을 위한 추가적인 준비는 필요 없습니다.\n",
    "\n",
    "  - 가중치와 활성값 뒤에 어떤 종류의 가짜 양자화를 사용할 것인지 명시하는 qconfig 의 사용이 필요합니다. Observer를 명시하는 것 대신에 말이죠.\n",
    "\n",
    "먼저 학습 함수부터 정의합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_quan(\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (bn1): Identity()\n",
       "  (relu): Identity()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): BottleNeck_quan(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn1): Identity()\n",
       "      (conv2): ConvReLU2d(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (bn2): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn3): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (relu1): Identity()\n",
       "      (relu2): Identity()\n",
       "      (relu3): ReLU()\n",
       "      (add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fuse_model(model)\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"x86\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): BottleNeck_quan(\n",
      "    (conv1): ConvReLU2d(\n",
      "      64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn1): Identity()\n",
      "    (conv2): ConvReLU2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn2): Identity()\n",
      "    (conv3): Conv2d(\n",
      "      64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn3): Identity()\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "      (1): Identity()\n",
      "    )\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): ReLU()\n",
      "    (add): FloatFunctional(\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (1): BottleNeck_quan(\n",
      "    (conv1): ConvReLU2d(\n",
      "      256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn1): Identity()\n",
      "    (conv2): ConvReLU2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn2): Identity()\n",
      "    (conv3): Conv2d(\n",
      "      64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn3): Identity()\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): ReLU()\n",
      "    (add): FloatFunctional(\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): BottleNeck_quan(\n",
      "    (conv1): ConvReLU2d(\n",
      "      256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn1): Identity()\n",
      "    (conv2): ConvReLU2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn2): Identity()\n",
      "    (conv3): Conv2d(\n",
      "      64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (bn3): Identity()\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (relu1): Identity()\n",
      "    (relu2): Identity()\n",
      "    (relu3): ReLU()\n",
      "    (add): FloatFunctional(\n",
      "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/miniconda3/envs/py312/lib/python3.12/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "print(\n",
    "    \"Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n\",\n",
    "    model.layer1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(8):\n",
    "    _, _ = SingleEpochTrain(model, train_loader, criterion, optimizer, epoch, device)\n",
    "    if epoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        model.apply(torch.quantization.disable_observer)\n",
    "    if epoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "        \n",
    "        \n",
    "    model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = SingleEpochEval(model, test_loader, criterion, device)\n",
    "    print(f\"epoch {epoch} : eval_loss {eval_loss}, eval_acc {eval_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  48 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.383083820343018"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = run_benchmark(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
